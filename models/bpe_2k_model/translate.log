2025-05-27 17:30:52,985 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-27 17:30:53,022 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-27 17:30:53,061 - INFO - joeynmt.model - Enc-dec model built.
2025-05-27 17:30:53,096 - INFO - joeynmt.helpers - Load model from /Users/merilinsilva/Documents/SemestersUZH/4thSem/MT/exercises/msousa_spareek_mt_exercise_04/mt-exercise-4/models/bpe_2k_model/31000.ckpt.
2025-05-27 17:30:53,103 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-27 17:30:53,103 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-27 17:30:53,106 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 17:30:53,106 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 17:35:47,151 - INFO - joeynmt.prediction - Generation took 294.0215[sec]. (No references given)
2025-05-27 18:32:08,462 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-27 18:32:08,498 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-27 18:32:08,562 - INFO - joeynmt.model - Enc-dec model built.
2025-05-27 18:32:08,601 - INFO - joeynmt.helpers - Load model from /Users/merilinsilva/Documents/SemestersUZH/4thSem/MT/exercises/msousa_spareek_mt_exercise_04/mt-exercise-4/models/bpe_2k_model/31000.ckpt.
2025-05-27 18:32:08,609 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-27 18:32:08,609 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-27 18:32:08,613 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 18:32:08,613 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 18:36:59,605 - INFO - joeynmt.prediction - Generation took 290.9694[sec]. (No references given)
2025-05-27 18:39:18,031 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-27 18:39:18,067 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-27 18:39:18,107 - INFO - joeynmt.model - Enc-dec model built.
2025-05-27 18:39:18,146 - INFO - joeynmt.helpers - Load model from /Users/merilinsilva/Documents/SemestersUZH/4thSem/MT/exercises/msousa_spareek_mt_exercise_04/mt-exercise-4/models/bpe_2k_model/31000.ckpt.
2025-05-27 18:39:18,152 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-27 18:39:18,153 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-27 18:39:18,156 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 18:39:18,156 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 18:44:12,240 - INFO - joeynmt.prediction - Generation took 294.0621[sec]. (No references given)
2025-05-27 18:45:20,743 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-27 18:45:20,780 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-27 18:45:20,820 - INFO - joeynmt.model - Enc-dec model built.
2025-05-27 18:45:20,858 - INFO - joeynmt.helpers - Load model from /Users/merilinsilva/Documents/SemestersUZH/4thSem/MT/exercises/msousa_spareek_mt_exercise_04/mt-exercise-4/models/bpe_2k_model/31000.ckpt.
2025-05-27 18:45:20,865 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-27 18:45:20,865 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-27 18:45:20,868 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 18:45:20,868 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 18:50:16,332 - INFO - joeynmt.prediction - Generation took 295.4412[sec]. (No references given)
