2025-05-27 10:10:14,026 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-27 10:10:14,026 - INFO - joeynmt.helpers -                           cfg.name : bpe_4k_model
2025-05-27 10:10:14,026 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2025-05-27 10:10:14,026 - INFO - joeynmt.helpers -                     cfg.data.train : data/my_lang_pair_bpe_4000/train
2025-05-27 10:10:14,026 - INFO - joeynmt.helpers -                       cfg.data.dev : data/my_lang_pair_bpe_4000/dev
2025-05-27 10:10:14,026 - INFO - joeynmt.helpers -                      cfg.data.test : data/my_lang_pair_bpe_4000/test
2025-05-27 10:10:14,026 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2025-05-27 10:10:14,026 - INFO - joeynmt.helpers -                  cfg.data.src.lang : nl
2025-05-27 10:10:14,026 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2025-05-27 10:10:14,026 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : data/bpe_4000/joint_vocab.txt
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 4000
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenization : None
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : data/bpe_4000/bpe.codes
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : de
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : data/bpe_4000/joint_vocab.txt
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 4000
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenization : None
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : data/bpe_4000/bpe.codes
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -            cfg.training.batch_size : 32
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -            cfg.training.batch_type : sentence
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2025-05-27 10:10:14,027 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/bpe_4k_model
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -             cfg.training.overwrite : True
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2025-05-27 10:10:14,028 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2025-05-27 10:10:14,030 - INFO - joeynmt.data - Building tokenizer...
2025-05-27 10:10:14,037 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-27 10:10:14,037 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-27 10:10:14,037 - INFO - joeynmt.data - Loading train set...
2025-05-27 10:10:14,167 - INFO - joeynmt.data - Building vocabulary...
2025-05-27 10:10:14,311 - INFO - joeynmt.data - Loading dev set...
2025-05-27 10:10:14,312 - INFO - joeynmt.data - Loading test set...
2025-05-27 10:10:14,315 - INFO - joeynmt.data - Data loaded.
2025-05-27 10:10:14,315 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=nl, trg_lang=de, has_trg=True, random_subset=-1)
2025-05-27 10:10:14,315 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=1001, src_lang=nl, trg_lang=de, has_trg=True, random_subset=-1)
2025-05-27 10:10:14,315 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1779, src_lang=nl, trg_lang=de, has_trg=True, random_subset=-1)
2025-05-27 10:10:14,315 - INFO - joeynmt.data - First training example:
	[SRC] Al G@@ @@@ @ ore over het af@@ @@@ @ wen@@ @@@ @ den van de klim@@ @@@ @ aat@@ @@@ @ c@@ @@@ @ ris@@ @@@ @ is
	[TRG] Al G@@ @@@ @ ore : Die Ab@@ @@@ @ wen@@ @@@ @ dung der Klima@@ @@@ @ k@@ @@@ @ at@@ @@@ @ ast@@ @@@ @ rop@@ @@@ @ he
2025-05-27 10:10:14,315 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4)  (5) ! (6) « (7) » (8) # (9) $
2025-05-27 10:10:14,315 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4)  (5) ! (6) « (7) » (8) # (9) $
2025-05-27 10:10:14,315 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 3989
2025-05-27 10:10:14,315 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 3989
2025-05-27 10:10:14,317 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-27 10:10:14,368 - INFO - joeynmt.model - Enc-dec model built.
2025-05-27 10:10:14,370 - INFO - joeynmt.model - Total params: 3920384
2025-05-27 10:10:14,370 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2025-05-27 10:10:14,370 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=3989),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=3989),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2025-05-27 10:10:14,371 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2025-05-27 10:10:14,371 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2025-05-27 10:10:14,371 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 32
	effective batch size (w. parallel & accumulation): 32
2025-05-27 10:10:14,371 - INFO - joeynmt.training - EPOCH 1
2025-05-27 10:10:56,237 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     2.840543, Batch Acc: 0.322061, Tokens per Sec:     3662, Lr: 0.000300
2025-05-27 10:11:37,583 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     2.439234, Batch Acc: 0.428712, Tokens per Sec:     3696, Lr: 0.000300
2025-05-27 10:12:20,403 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     2.445703, Batch Acc: 0.459212, Tokens per Sec:     3653, Lr: 0.000300
2025-05-27 10:13:00,102 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     2.330166, Batch Acc: 0.467383, Tokens per Sec:     3839, Lr: 0.000300
2025-05-27 10:13:40,036 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     2.332865, Batch Acc: 0.472133, Tokens per Sec:     3820, Lr: 0.000300
2025-05-27 10:13:40,036 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 10:13:40,036 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 10:18:26,963 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.25, ppl:   9.52, acc:   0.48, generation: 286.9095[sec], evaluation: 0.0000[sec]
2025-05-27 10:18:26,968 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 10:18:27,106 - INFO - joeynmt.training - Example #0
2025-05-27 10:18:27,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 10:18:27,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 10:18:27,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'die', 'der', 'Z@@', '<unk>', '@', 'te', ',', 'die', 'der', 'Welt', ',', 'die', 'Z@@', '<unk>', '@', 'ung', ',', 'die', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', ',', 'die', 'der', 'Welt', ',', 'die', 'der', 'der', 'der', 'Welt', ',', 'die', 'der', 'Welt', ',', 'die', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'Welt', '.', '</s>']
2025-05-27 10:18:27,107 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 10:18:27,107 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 10:18:27,107 - INFO - joeynmt.training - 	Hypothesis: Und die der Z<unk> @ te , die der Welt , die Z<unk> @ ung , die der der der der der der der der der der der der der der der der der der der der der der der der der der der , die der Welt , die der der der Welt , die der Welt , die der der der der der der der der der der der der der der der der der der der der der der der der der Welt .
2025-05-27 10:18:27,107 - INFO - joeynmt.training - Example #1
2025-05-27 10:18:27,107 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 10:18:27,107 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 10:18:27,107 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', ',', 'dass', 'die', 'Welt', ',', 'die', 'Welt', ',', 'dass', 'die', 'Welt', ',', 'die', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'Welt', '.', '</s>']
2025-05-27 10:18:27,107 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 10:18:27,107 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 10:18:27,107 - INFO - joeynmt.training - 	Hypothesis: Und ich , dass die Welt , die Welt , dass die Welt , die der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der Welt .
2025-05-27 10:18:27,107 - INFO - joeynmt.training - Example #2
2025-05-27 10:18:27,107 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 10:18:27,107 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 10:18:27,107 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ist', 'es', ',', 'die', 'der', 'F@@', '<unk>', '@', 'ung', ',', 'die', 'der', '<unk>', '@', 'ung', ',', 'die', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'F@@', '<unk>', '@', 'ung', ',', 'die', 'der', 'F@@', '<unk>', '@', 'ung', ',', 'die', 'der', 'F@@', '<unk>', '@', 'ung', '.', '</s>']
2025-05-27 10:18:27,107 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 10:18:27,108 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 10:18:27,108 - INFO - joeynmt.training - 	Hypothesis: Das ist es , die der F<unk> @ ung , die der <unk> @ ung , die der der der der der der der der der der der der der der F<unk> @ ung , die der F<unk> @ ung , die der F<unk> @ ung .
2025-05-27 10:18:27,108 - INFO - joeynmt.training - Example #3
2025-05-27 10:18:27,108 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 10:18:27,108 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 10:18:27,108 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ist', 'ein', 'F@@', '<unk>', '@', 'ung', ',', 'die', 'der', 'der', 'der', '<unk>', '@', 'ung', ',', 'die', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'K@@', '<unk>', '@', 'ung', '.', '</s>']
2025-05-27 10:18:27,108 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 10:18:27,108 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 10:18:27,108 - INFO - joeynmt.training - 	Hypothesis: Das ist ein F<unk> @ ung , die der der der <unk> @ ung , die der der der der der der der der der der der der der der der der der K<unk> @ ung .
2025-05-27 10:18:27,108 - INFO - joeynmt.training - Example #4
2025-05-27 10:18:27,108 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 10:18:27,108 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 10:18:27,108 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', ',', 'dass', 'die', 'Z@@', '<unk>', '@', 'ung', ',', 'die', 'Z@@', '<unk>', '@', 'te', ',', 'dass', 'die', 'Z@@', '<unk>', '@', 'ung', ',', 'dass', 'die', 'Welt', ',', 'dass', 'die', 'die', ',', 'dass', 'die', ',', 'dass', 'die', ',', 'dass', 'die', ',', 'dass', 'die', 'die', 'die', 'die', 'die', ',', 'dass', 'die', 'Welt', '.', '</s>']
2025-05-27 10:18:27,108 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 10:18:27,108 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 10:18:27,108 - INFO - joeynmt.training - 	Hypothesis: Und ich , dass die Z<unk> @ ung , die Z<unk> @ te , dass die Z<unk> @ ung , dass die Welt , dass die die , dass die , dass die , dass die , dass die die die die die , dass die Welt .
2025-05-27 10:19:09,307 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     2.283266, Batch Acc: 0.474421, Tokens per Sec:     3642, Lr: 0.000300
2025-05-27 10:19:49,049 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     2.189898, Batch Acc: 0.477826, Tokens per Sec:     3835, Lr: 0.000300
2025-05-27 10:20:31,878 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     2.210824, Batch Acc: 0.481714, Tokens per Sec:     3629, Lr: 0.000300
2025-05-27 10:21:13,701 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     2.202457, Batch Acc: 0.483099, Tokens per Sec:     3606, Lr: 0.000300
2025-05-27 10:21:55,864 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     2.082074, Batch Acc: 0.484243, Tokens per Sec:     3616, Lr: 0.000300
2025-05-27 10:21:55,865 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 10:21:55,865 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 10:26:23,316 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.17, ppl:   8.77, acc:   0.49, generation: 267.4408[sec], evaluation: 0.0000[sec]
2025-05-27 10:26:23,323 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 10:26:23,442 - INFO - joeynmt.training - Example #0
2025-05-27 10:26:23,443 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 10:26:23,443 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 10:26:23,443 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'die', 'Welt', ',', 'dass', 'die', 'Welt', ',', 'dass', 'die', 'Welt', ',', 'dass', 'die', 'Welt', ',', 'dass', 'die', 'Welt', ',', 'dass', 'die', 'Welt', ',', 'dass', 'die', 'Welt', ',', 'dass', 'die', 'Welt', ',', 'dass', 'die', 'Welt', ',', 'dass', 'die', 'Welt', ',', 'dass', 'die', 'Welt', ',', 'die', 'die', 'die', 'Welt', ',', 'die', 'Welt', ',', 'die', 'Welt', '.', '</s>']
2025-05-27 10:26:23,443 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 10:26:23,443 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 10:26:23,443 - INFO - joeynmt.training - 	Hypothesis: Und die Welt , dass die Welt , dass die Welt , dass die Welt , dass die Welt , dass die Welt , dass die Welt , dass die Welt , dass die Welt , dass die Welt , dass die Welt , die die die Welt , die Welt , die Welt .
2025-05-27 10:26:23,443 - INFO - joeynmt.training - Example #1
2025-05-27 10:26:23,443 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 10:26:23,443 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 10:26:23,443 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'nicht', 'nicht', 'nicht', ',', 'die', 'die', 'Welt', ',', 'die', 'die', 'die', 'die', 'Welt', ',', 'die', 'Welt', '.', '</s>']
2025-05-27 10:26:23,443 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 10:26:23,443 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 10:26:23,443 - INFO - joeynmt.training - 	Hypothesis: Aber das nicht nicht nicht , die die Welt , die die die die Welt , die Welt .
2025-05-27 10:26:23,443 - INFO - joeynmt.training - Example #2
2025-05-27 10:26:23,444 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 10:26:23,444 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 10:26:23,444 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Welt', 'in', 'der', 'Welt', ',', 'die', 'die', 'Welt', ',', 'die', ',', 'die', 'die', 'Welt', ',', 'die', 'Welt', ',', 'die', 'Welt', ',', 'die', 'Welt', ',', 'die', 'K@@', '<unk>', '@', 'ung', ',', 'die', 'die', 'K@@', '<unk>', '@', 'ung', ',', 'die', 'K@@', '<unk>', '@', 'ung', ',', 'die', 'die', 'die', 'die', 'die', 'K@@', '<unk>', '@', 'ung', '.', '</s>']
2025-05-27 10:26:23,444 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 10:26:23,444 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 10:26:23,444 - INFO - joeynmt.training - 	Hypothesis: Die Welt in der Welt , die die Welt , die , die die Welt , die Welt , die Welt , die Welt , die K<unk> @ ung , die die K<unk> @ ung , die K<unk> @ ung , die die die die die K<unk> @ ung .
2025-05-27 10:26:23,444 - INFO - joeynmt.training - Example #3
2025-05-27 10:26:23,444 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 10:26:23,444 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 10:26:23,444 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'die', 'K@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 's@@', '<unk>', '@', 't', '.', '</s>']
2025-05-27 10:26:23,444 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 10:26:23,444 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 10:26:23,444 - INFO - joeynmt.training - 	Hypothesis: Es ist die K<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ s<unk> @ t .
2025-05-27 10:26:23,444 - INFO - joeynmt.training - Example #4
2025-05-27 10:26:23,444 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 10:26:23,444 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 10:26:23,444 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'paar', 'paar', 'paar', ',', 'dass', 'ich', 'ein', 'paar', 'paar', ',', 'dass', 'ich', 'ein', 'paar', 'paar', ',', 'dass', 'ich', 'ein', 'paar', 'paar', 'paar', ',', 'dass', 'ich', 'ein', 'paar', 'paar', ',', '</s>']
2025-05-27 10:26:23,445 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 10:26:23,445 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 10:26:23,445 - INFO - joeynmt.training - 	Hypothesis: Die paar paar paar , dass ich ein paar paar , dass ich ein paar paar , dass ich ein paar paar paar , dass ich ein paar paar ,
2025-05-27 10:27:06,147 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.103892, Batch Acc: 0.487634, Tokens per Sec:     3606, Lr: 0.000300
2025-05-27 10:27:45,657 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     2.091212, Batch Acc: 0.490398, Tokens per Sec:     3846, Lr: 0.000300
2025-05-27 10:28:27,625 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     2.032571, Batch Acc: 0.491579, Tokens per Sec:     3611, Lr: 0.000300
2025-05-27 10:29:06,142 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     2.110110, Batch Acc: 0.493674, Tokens per Sec:     3880, Lr: 0.000300
2025-05-27 10:29:47,392 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.043630, Batch Acc: 0.496391, Tokens per Sec:     3732, Lr: 0.000300
2025-05-27 10:29:47,392 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 10:29:47,392 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 10:35:06,294 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.10, ppl:   8.20, acc:   0.50, generation: 318.8764[sec], evaluation: 0.0000[sec]
2025-05-27 10:35:06,301 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 10:35:06,434 - INFO - joeynmt.training - Example #0
2025-05-27 10:35:06,435 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 10:35:06,435 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 10:35:06,435 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', ',', 'ich', 'in', 'der', 'ich', 'in', 'der', 'P@@', '<unk>', '@', 'i@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ung', ',', 'ich', 'ich', 'in', 'der', 'P@@', '<unk>', '@', 'i@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ung', ',', 'die', 'ich', 'in', 'der', 'Menschen', 'in', 'der', 'Welt', 'in', 'der', 'Welt', 'in', 'der', 'Welt', ',', 'die', 'die', 'die', 'die', 'Menschen', 'in', 'der', 'Welt', 'in', 'der', 'Welt', ',', 'die', 'die', 'die', 'Menschen', 'in', 'der', 'Welt', 'zu', '<unk>', '@', 'ung', '<unk>', '@', 'ten', 'in', 'der', 'Welt', '.', '</s>']
2025-05-27 10:35:06,435 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 10:35:06,435 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 10:35:06,435 - INFO - joeynmt.training - 	Hypothesis: Und ich habe , ich in der ich in der P<unk> @ i<unk> @ i<unk> @ ung , ich ich in der P<unk> @ i<unk> @ i<unk> @ ung , die ich in der Menschen in der Welt in der Welt in der Welt , die die die die Menschen in der Welt in der Welt , die die die Menschen in der Welt zu <unk> @ ung <unk> @ ten in der Welt .
2025-05-27 10:35:06,435 - INFO - joeynmt.training - Example #1
2025-05-27 10:35:06,435 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 10:35:06,435 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 10:35:06,435 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'Welt', ',', 'die', 'wir', 'die', 'Welt', 'der', 'Welt', ',', 'die', 'wir', 'in', 'der', 'Welt', ',', 'die', 'wir', 'nicht', 'der', 'Welt', '.', '</s>']
2025-05-27 10:35:06,435 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 10:35:06,436 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 10:35:06,436 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Welt , die wir die Welt der Welt , die wir in der Welt , die wir nicht der Welt .
2025-05-27 10:35:06,436 - INFO - joeynmt.training - Example #2
2025-05-27 10:35:06,436 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 10:35:06,436 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 10:35:06,436 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ver@@', '<unk>', '@', '-@@', '<unk>', '@', 'y@@', '<unk>', '@', '-@@', '<unk>', '@', 'y@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>']
2025-05-27 10:35:06,436 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 10:35:06,436 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 10:35:06,436 - INFO - joeynmt.training - 	Hypothesis: Die Ver<unk> @ -<unk> @ y<unk> @ -<unk> @ y<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk>
2025-05-27 10:35:06,436 - INFO - joeynmt.training - Example #3
2025-05-27 10:35:06,436 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 10:35:06,436 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 10:35:06,436 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'die', 'B@@', '<unk>', '@', 'y@@', '<unk>', '@', '-@@', '<unk>', '@', 'y@@', '<unk>', '@', '-@@', '<unk>', '@', 'y@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', 'y@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@']
2025-05-27 10:35:06,436 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 10:35:06,436 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 10:35:06,436 - INFO - joeynmt.training - 	Hypothesis: Es ist die B<unk> @ y<unk> @ -<unk> @ y<unk> @ -<unk> @ y<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ y<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @
2025-05-27 10:35:06,436 - INFO - joeynmt.training - Example #4
2025-05-27 10:35:06,436 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 10:35:06,436 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 10:35:06,436 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Art', 'ist', 'ein', 'paar', 'Jahren', ',', 'dass', 'ich', 'ein', 'paar', 'Jahren', ',', 'was', 'ich', ',', 'was', 'ich', ',', 'was', 'ich', ',', 'was', 'ich', ',', 'was', 'ich', ',', 'was', 'ich', 'ist', ',', 'was', 'ich', 'ist', ',', 'was', 'ich', 'ist', '.', '</s>']
2025-05-27 10:35:06,437 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 10:35:06,437 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 10:35:06,437 - INFO - joeynmt.training - 	Hypothesis: Die Art ist ein paar Jahren , dass ich ein paar Jahren , was ich , was ich , was ich , was ich , was ich , was ich ist , was ich ist , was ich ist .
2025-05-27 10:35:50,538 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.069204, Batch Acc: 0.497080, Tokens per Sec:     3484, Lr: 0.000300
2025-05-27 10:36:35,424 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.055396, Batch Acc: 0.498252, Tokens per Sec:     3372, Lr: 0.000300
2025-05-27 10:37:16,617 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     2.087208, Batch Acc: 0.502837, Tokens per Sec:     3769, Lr: 0.000300
2025-05-27 10:37:58,615 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.001066, Batch Acc: 0.504340, Tokens per Sec:     3641, Lr: 0.000300
2025-05-27 10:38:40,874 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.043581, Batch Acc: 0.505451, Tokens per Sec:     3644, Lr: 0.000300
2025-05-27 10:38:40,875 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 10:38:40,875 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 10:43:31,677 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.00, ppl:   7.40, acc:   0.50, generation: 290.7887[sec], evaluation: 0.0000[sec]
2025-05-27 10:43:31,684 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 10:43:31,801 - INFO - joeynmt.training - Example #0
2025-05-27 10:43:31,801 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 10:43:31,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 10:43:31,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'der', 'letzten', 'Jahren', ',', 'ich', 'habe', ',', 'ich', 'ich', 'mit', 'diesem', 'Jahren', ',', 'die', 'ich', 'in', 'der', 'letzten', 'Jahr', ',', 'die', 'ich', 'in', 'der', 'letzten', 'Jahr', ',', 'die', 'ich', 'in', 'der', 'letzten', 'Jahren', ',', 'die', 'ich', 'in', 'der', 'letzten', 'Jahren', ',', 'die', 'ich', 'in', 'der', 'letzten', 'Jahren', ',', 'die', 'letzten', 'Jahren', ',', 'die', 'ich', 'in', 'der', 'letzten', 'Jahren', ',', 'die', 'die', 'letzten', 'Jahren', ',', 'die', 'ich', 'in', 'der', 'letzten', 'Jahren', ',', 'die', 'die', 'letzten', 'Jahren', 'war', ',', 'die', 'ich', 'in', 'der', 'letzten', 'Jahren', ',', 'die', 'ich', 'in', 'der', 'letzten', 'Jahren', ',', 'die', 'ich', 'in', 'der', 'letzten', 'Jahren', ',', 'die', 'ich', 'habe', ',', 'die', 'die', 'letzten', 'Jahren', '.', '</s>']
2025-05-27 10:43:31,801 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 10:43:31,801 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 10:43:31,801 - INFO - joeynmt.training - 	Hypothesis: In der letzten Jahren , ich habe , ich ich mit diesem Jahren , die ich in der letzten Jahr , die ich in der letzten Jahr , die ich in der letzten Jahren , die ich in der letzten Jahren , die ich in der letzten Jahren , die letzten Jahren , die ich in der letzten Jahren , die die letzten Jahren , die ich in der letzten Jahren , die die letzten Jahren war , die ich in der letzten Jahren , die ich in der letzten Jahren , die ich in der letzten Jahren , die ich habe , die die letzten Jahren .
2025-05-27 10:43:31,801 - INFO - joeynmt.training - Example #1
2025-05-27 10:43:31,801 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 10:43:31,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 10:43:31,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'K@@', '<unk>', '@', 'ü@@', '<unk>', '@', 'den', 'K@@', '<unk>', '@', 'ü@@', '<unk>', '@', 'den', 'K@@', '<unk>', '@', 'ü@@', '<unk>', '@', 'ung', 'der', 'der', 'Welt', 'ist', ',', 'weil', 'es', 'nicht', 'der', 'der', 'Welt', '.', '</s>']
2025-05-27 10:43:31,802 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 10:43:31,802 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 10:43:31,802 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die K<unk> @ ü<unk> @ den K<unk> @ ü<unk> @ den K<unk> @ ü<unk> @ ung der der Welt ist , weil es nicht der der Welt .
2025-05-27 10:43:31,802 - INFO - joeynmt.training - Example #2
2025-05-27 10:43:31,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 10:43:31,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 10:43:31,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'H@@', '<unk>', '@', 'am@@', '<unk>', '@', '-@@', '<unk>', '@', 'S@@', '<unk>', '@', 'am@@', '<unk>', '@', '-@@', '<unk>', '@', 'S@@', '<unk>', '@', 'an@@', '<unk>', '@', 'an@@', '<unk>', '@', 'er@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ü@@', '<unk>', '@', 'te', ',', 'die', 'wir', 'haben', ',', 'die', 'wir', 'uns', 'in', 'der', 'Welt', '.', '</s>']
2025-05-27 10:43:31,802 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 10:43:31,802 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 10:43:31,802 - INFO - joeynmt.training - 	Hypothesis: Die H<unk> @ am<unk> @ -<unk> @ S<unk> @ am<unk> @ -<unk> @ S<unk> @ an<unk> @ an<unk> @ er<unk> @ sch<unk> @ ü<unk> @ te , die wir haben , die wir uns in der Welt .
2025-05-27 10:43:31,802 - INFO - joeynmt.training - Example #3
2025-05-27 10:43:31,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 10:43:31,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 10:43:31,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'die', 'K@@', '<unk>', '@', 'ü@@', '<unk>', '@', 'te', 'und', 'die', 'K@@', '<unk>', '@', 'ü@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ü@@', '<unk>', '@', 'te', '.', '</s>']
2025-05-27 10:43:31,802 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 10:43:31,803 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 10:43:31,803 - INFO - joeynmt.training - 	Hypothesis: Es ist die K<unk> @ ü<unk> @ te und die K<unk> @ ü<unk> @ st<unk> @ ü<unk> @ te .
2025-05-27 10:43:31,803 - INFO - joeynmt.training - Example #4
2025-05-27 10:43:31,803 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 10:43:31,803 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 10:43:31,803 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Geschichte', 'ist', ',', 'dass', 'ich', 'ein', 'paar', 'Jahren', ',', 'die', 'ich', 'ein', 'paar', 'Jahren', ',', 'was', 'ich', 'ist', ',', 'was', 'ich', 'ist', ',', 'was', 'ich', 'ist', ',', 'was', 'ich', 'hier', 'ist', ',', 'was', 'ich', 'ist', ',', 'was', 'ich', 'ist', ',', '</s>']
2025-05-27 10:43:31,803 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 10:43:31,803 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 10:43:31,803 - INFO - joeynmt.training - 	Hypothesis: Die Geschichte ist , dass ich ein paar Jahren , die ich ein paar Jahren , was ich ist , was ich ist , was ich ist , was ich hier ist , was ich ist , was ich ist ,
2025-05-27 10:44:17,243 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     1.981449, Batch Acc: 0.509144, Tokens per Sec:     3407, Lr: 0.000300
2025-05-27 10:45:03,039 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     1.901328, Batch Acc: 0.513941, Tokens per Sec:     3389, Lr: 0.000300
2025-05-27 10:45:44,828 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     1.910347, Batch Acc: 0.516498, Tokens per Sec:     3672, Lr: 0.000300
2025-05-27 10:46:25,854 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     1.881891, Batch Acc: 0.520738, Tokens per Sec:     3755, Lr: 0.000300
2025-05-27 10:47:05,643 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     1.926109, Batch Acc: 0.522948, Tokens per Sec:     3869, Lr: 0.000300
2025-05-27 10:47:05,643 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 10:47:05,643 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 10:51:59,360 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.67, acc:   0.52, generation: 293.7051[sec], evaluation: 0.0000[sec]
2025-05-27 10:51:59,366 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 10:51:59,486 - INFO - joeynmt.training - Example #0
2025-05-27 10:51:59,486 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 10:51:59,486 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 10:51:59,486 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', ',', 'dass', 'diese', 'zwei', 'Jahren', ',', 'die', 'ich', 'meine', ',', 'dass', 'diese', 'M@@', '<unk>', '@', 'om@@', '<unk>', '@', 'o', 'zu', 'sehen', ',', 'dass', 'die', 'die', 'meisten', 'von', 'der', 'letzten', 'Jahren', ',', 'die', 'die', 'letzten', 'Jahren', ',', 'die', 'die', 'letzten', 'Jahren', ',', 'die', 'die', 'letzten', 'Jahren', ',', 'die', 'die', 'letzten', 'Jahren', ',', 'die', 'die', 'letzten', 'Jahren', ',', 'die', 'die', 'die', 'letzten', 'der', 'letzten', 'Jahren', ',', 'die', 'die', 'letzten', 'der', 'letzten', 'Jahren', ',', 'die', 'die', 'letzten', 'Jahren', ',', 'die', 'die', 'letzten', 'der', 'letzten', 'Jahren', 'war', ',', 'die', 'die', 'letzten', 'Jahren', 'war', ',', 'die', 'die', 'letzten', 'Jahren', 'war', ',', '</s>']
2025-05-27 10:51:59,486 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 10:51:59,486 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 10:51:59,486 - INFO - joeynmt.training - 	Hypothesis: Und ich habe , dass diese zwei Jahren , die ich meine , dass diese M<unk> @ om<unk> @ o zu sehen , dass die die meisten von der letzten Jahren , die die letzten Jahren , die die letzten Jahren , die die letzten Jahren , die die letzten Jahren , die die letzten Jahren , die die die letzten der letzten Jahren , die die letzten der letzten Jahren , die die letzten Jahren , die die letzten der letzten Jahren war , die die letzten Jahren war , die die letzten Jahren war ,
2025-05-27 10:51:59,486 - INFO - joeynmt.training - Example #1
2025-05-27 10:51:59,486 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 10:51:59,486 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 10:51:59,487 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'Sch@@', '<unk>', '@', 'ei@@', '<unk>', '@', 'ch@@', '<unk>', '@', 't@@', '<unk>', '@', 'ung', ',', 'weil', 'es', 'nicht', 'die', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Problem', 'ist', ',', 'weil', 'es', 'nicht', 'die', 'Problem', '.', '</s>']
2025-05-27 10:51:59,487 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 10:51:59,487 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 10:51:59,487 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Sch<unk> @ ei<unk> @ ch<unk> @ t<unk> @ ung , weil es nicht die Problem , weil es nicht die Problem ist , weil es nicht die Problem .
2025-05-27 10:51:59,487 - INFO - joeynmt.training - Example #2
2025-05-27 10:51:59,487 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 10:51:59,487 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 10:51:59,487 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'B@@', '<unk>', '@', 'or@@', '<unk>', '@', 'p@@', '<unk>', '@', 'o', 'hat', 'die', 'B@@', '<unk>', '@', 'i@@', '<unk>', '@', 'p@@', '<unk>', '@', 'o', 'in', 'der', 'K@@', '<unk>', '@', 'la@@', '<unk>', '@', 'd@@', '<unk>', '@', 'e@@', '<unk>', '@', 'p@@', '<unk>', '@', 'a@@', '<unk>', '@', 'g', 'von', 'uns', 'uns', 'in', 'der', 'Ver@@', '<unk>', '@', 'su@@', '<unk>', '@', 'ch@@', '<unk>', '@', 'ung', '.', '</s>']
2025-05-27 10:51:59,487 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 10:51:59,487 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 10:51:59,487 - INFO - joeynmt.training - 	Hypothesis: Die B<unk> @ or<unk> @ p<unk> @ o hat die B<unk> @ i<unk> @ p<unk> @ o in der K<unk> @ la<unk> @ d<unk> @ e<unk> @ p<unk> @ a<unk> @ g von uns uns in der Ver<unk> @ su<unk> @ ch<unk> @ ung .
2025-05-27 10:51:59,487 - INFO - joeynmt.training - Example #3
2025-05-27 10:51:59,487 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 10:51:59,487 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 10:51:59,487 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'die', 'B@@', '<unk>', '@', 'or@@', '<unk>', '@', 'te', 'und', 'die', 'B@@', '<unk>', '@', 'or@@', '<unk>', '@', 'te', 'in', 'der', 'B@@', '<unk>', '@', 'or@@', '<unk>', '@', 'te', 'in', 'der', 'K@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ü@@', '<unk>', '@', 'gen', '.', '</s>']
2025-05-27 10:51:59,487 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 10:51:59,488 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 10:51:59,488 - INFO - joeynmt.training - 	Hypothesis: Es ist die B<unk> @ or<unk> @ te und die B<unk> @ or<unk> @ te in der B<unk> @ or<unk> @ te in der K<unk> @ r<unk> @ ü<unk> @ gen .
2025-05-27 10:51:59,488 - INFO - joeynmt.training - Example #4
2025-05-27 10:51:59,488 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 10:51:59,488 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 10:51:59,488 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Frage', 'ist', ',', 'die', 'ich', 'hier', ',', 'die', 'ist', 'ein', 'paar', 'Jahren', ',', 'was', 'es', 'ist', ',', 'was', 'es', 'ist', ',', 'was', 'es', 'ist', ',', 'was', 'das', 'ist', ',', 'was', 'es', 'ist', ',', 'was', 'das', 'ist', '.', '</s>']
2025-05-27 10:51:59,488 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 10:51:59,488 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 10:51:59,488 - INFO - joeynmt.training - 	Hypothesis: Die Frage ist , die ich hier , die ist ein paar Jahren , was es ist , was es ist , was es ist , was das ist , was es ist , was das ist .
2025-05-27 10:52:40,015 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     1.855933, Batch Acc: 0.528240, Tokens per Sec:     3743, Lr: 0.000300
2025-05-27 10:53:21,204 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     1.862475, Batch Acc: 0.531752, Tokens per Sec:     3656, Lr: 0.000300
2025-05-27 10:54:03,802 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     1.785837, Batch Acc: 0.539665, Tokens per Sec:     3586, Lr: 0.000300
2025-05-27 10:54:44,286 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     1.738067, Batch Acc: 0.543343, Tokens per Sec:     3790, Lr: 0.000300
2025-05-27 10:55:27,119 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     1.739765, Batch Acc: 0.548208, Tokens per Sec:     3635, Lr: 0.000300
2025-05-27 10:55:27,119 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 10:55:27,119 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:00:06,775 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   6.01, acc:   0.54, generation: 279.6440[sec], evaluation: 0.0000[sec]
2025-05-27 11:00:06,782 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:00:06,892 - INFO - joeynmt.helpers - delete models/bpe_4k_model/500.ckpt
2025-05-27 11:00:06,896 - INFO - joeynmt.training - Example #0
2025-05-27 11:00:06,896 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 11:00:06,896 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 11:00:06,896 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'in', 'der', 'letzten', 'Jahr', 'Jahr', ',', 'dass', 'ich', 'diese', 'M@@', '<unk>', '@', 'eter', 'zeigen', ',', 'dass', 'die', 'die', 'meisten', 'von', 'dieser', 'T@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ie', 'zu', 'zeigen', ',', 'dass', 'die', 'meisten', 'von', 'drei', 'Millionen', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Millionen', 'Millionen', ',', 'die', 'die', 'die', 'die', 'Verein@@', '<unk>', '@', 'igten', 'Millionen', 'Millionen', 'von', 'der', 'USA', ',', 'die', 'die', 'die', 'die', 'USA', 'von', 'der', 'USA', ',', 'die', 'die', 'die', 'USA', 'von', 'der', 'USA', 'war', '.', '</s>']
2025-05-27 11:00:06,896 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 11:00:06,896 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 11:00:06,896 - INFO - joeynmt.training - 	Hypothesis: Und in der letzten Jahr Jahr , dass ich diese M<unk> @ eter zeigen , dass die die meisten von dieser T<unk> @ i<unk> @ ie zu zeigen , dass die meisten von drei Millionen , die die letzten drei Millionen , die die letzten drei Millionen Millionen Millionen , die die die die Verein<unk> @ igten Millionen Millionen von der USA , die die die die USA von der USA , die die die USA von der USA war .
2025-05-27 11:00:06,896 - INFO - joeynmt.training - Example #1
2025-05-27 11:00:06,896 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 11:00:06,896 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 11:00:06,896 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'Ver@@', '<unk>', '@', 'su@@', '<unk>', '@', 'cht', 'der', 'Problem', 'von', 'der', 'Problem', 'von', 'der', 'Problem', 'von', 'der', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Problem', 'ist', ',', 'weil', 'es', 'nicht', 'die', 'Problem', 'ist', '.', '</s>']
2025-05-27 11:00:06,897 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 11:00:06,897 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 11:00:06,897 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Ver<unk> @ su<unk> @ cht der Problem von der Problem von der Problem von der Problem , weil es nicht die Problem ist , weil es nicht die Problem ist .
2025-05-27 11:00:06,897 - INFO - joeynmt.training - Example #2
2025-05-27 11:00:06,897 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 11:00:06,897 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 11:00:06,897 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'B@@', '<unk>', '@', 'an@@', '<unk>', '@', 'y@@', '<unk>', '@', 's', 'auf', 'der', 'K@@', '<unk>', '@', 'am@@', '<unk>', '@', 'i@@', '<unk>', '@', 'p@@', '<unk>', '@', 'al@@', '<unk>', '@', 'te', 'ist', 'das', 'in', 'der', 'K@@', '<unk>', '@', 'ommun@@', '<unk>', '@', 'ation', 'von', 'uns', 'in', 'der', 'Ver@@', '<unk>', '@', 'su@@', '<unk>', '@', 'ch', 'der', 'Ver@@', '<unk>', '@', 'su@@', '<unk>', '@', 'ch', '.', '</s>']
2025-05-27 11:00:06,897 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 11:00:06,897 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 11:00:06,897 - INFO - joeynmt.training - 	Hypothesis: Die B<unk> @ an<unk> @ y<unk> @ s auf der K<unk> @ am<unk> @ i<unk> @ p<unk> @ al<unk> @ te ist das in der K<unk> @ ommun<unk> @ ation von uns in der Ver<unk> @ su<unk> @ ch der Ver<unk> @ su<unk> @ ch .
2025-05-27 11:00:06,897 - INFO - joeynmt.training - Example #3
2025-05-27 11:00:06,897 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 11:00:06,897 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 11:00:06,897 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'den', 'K@@', '<unk>', '@', 'ü@@', '<unk>', '@', 't@@', '<unk>', '@', 'm@@', '<unk>', '@', 'el', 'und', 'in', 'der', 'K@@', '<unk>', '@', 'ü@@', '<unk>', '@', 'ch@@', '<unk>', '@', 'm@@', '<unk>', '@', 'el', '.', '</s>']
2025-05-27 11:00:06,897 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 11:00:06,897 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 11:00:06,897 - INFO - joeynmt.training - 	Hypothesis: Es ist in den K<unk> @ ü<unk> @ t<unk> @ m<unk> @ el und in der K<unk> @ ü<unk> @ ch<unk> @ m<unk> @ el .
2025-05-27 11:00:06,897 - INFO - joeynmt.training - Example #4
2025-05-27 11:00:06,898 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 11:00:06,898 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 11:00:06,898 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'letzten', 'Jahren', 'ist', 'ein', 'bisschen', ',', 'dass', 'ich', 'eine', 'Z@@', '<unk>', '@', 'ei@@', '<unk>', '@', 'm@@', '<unk>', '@', 'ung', 'von', 'der', 'letzten', 'Jahr', ',', 'was', 'die', 'letzten', 'Jahre', 'Jahre', 'ist', '.', '</s>']
2025-05-27 11:00:06,898 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 11:00:06,898 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 11:00:06,898 - INFO - joeynmt.training - 	Hypothesis: Die letzten Jahren ist ein bisschen , dass ich eine Z<unk> @ ei<unk> @ m<unk> @ ung von der letzten Jahr , was die letzten Jahre Jahre ist .
2025-05-27 11:00:51,331 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     1.705995, Batch Acc: 0.551228, Tokens per Sec:     3473, Lr: 0.000300
2025-05-27 11:01:01,700 - INFO - joeynmt.training - Epoch   1: total training loss 6570.40
2025-05-27 11:01:01,700 - INFO - joeynmt.training - EPOCH 2
2025-05-27 11:01:33,894 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     1.692173, Batch Acc: 0.561185, Tokens per Sec:     3621, Lr: 0.000300
2025-05-27 11:02:16,909 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     1.701851, Batch Acc: 0.564561, Tokens per Sec:     3554, Lr: 0.000300
2025-05-27 11:03:01,005 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     1.607948, Batch Acc: 0.568599, Tokens per Sec:     3489, Lr: 0.000300
2025-05-27 11:03:39,631 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     1.622526, Batch Acc: 0.572960, Tokens per Sec:     3943, Lr: 0.000300
2025-05-27 11:03:39,631 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 11:03:39,631 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:07:58,135 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.45, acc:   0.56, generation: 258.4910[sec], evaluation: 0.0000[sec]
2025-05-27 11:07:58,140 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:07:58,247 - INFO - joeynmt.helpers - delete models/bpe_4k_model/1000.ckpt
2025-05-27 11:07:58,251 - INFO - joeynmt.training - Example #0
2025-05-27 11:07:58,251 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 11:07:58,252 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 11:07:58,252 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'or@@', '<unk>', '@', 't', 'Jahr', ',', 'ich', 'habe', 'diese', 'zwei', 'Jahren', 'zu', 'sehen', ',', 'um', 'die', 'zwei', 'Millionen', ',', 'um', 'die', 'letzten', 'drei', 'Millionen', ',', 'die', 'die', 'drei', 'Millionen', ',', 'die', 'die', 'drei', 'Millionen', ',', 'die', 'drei', 'Millionen', ',', 'die', 'die', 'drei', 'Millionen', ',', 'die', 'die', 'drei', 'Millionen', ',', 'die', 'die', 'drei', 'Millionen', ',', 'die', 'die', 'die', 'drei', 'Millionen', ',', 'die', 'die', 'die', 'drei', 'Millionen', 'Jahren', ',', 'die', 'die', 'USA', 'der', 'USA', 'war', '.', '</s>']
2025-05-27 11:07:58,252 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 11:07:58,252 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 11:07:58,252 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ or<unk> @ t Jahr , ich habe diese zwei Jahren zu sehen , um die zwei Millionen , um die letzten drei Millionen , die die drei Millionen , die die drei Millionen , die drei Millionen , die die drei Millionen , die die drei Millionen , die die drei Millionen , die die die drei Millionen , die die die drei Millionen Jahren , die die USA der USA war .
2025-05-27 11:07:58,252 - INFO - joeynmt.training - Example #1
2025-05-27 11:07:58,252 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 11:07:58,252 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 11:07:58,252 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'Ver@@', '<unk>', '@', 'gan@@', '<unk>', '@', 'genheit', 'der', 'Ver@@', '<unk>', '@', 'k@@', '<unk>', '@', 'ut@@', '<unk>', '@', 'ung', 'der', 'Problem', 'der', 'Problem', 'der', 'Problem', 'der', 'Problem', 'der', 'Welt', ',', 'weil', 'es', 'nicht', 'die', 'Ver@@', '<unk>', '@', 'gan@@', '<unk>', '@', 'genheit', 'der', 'der', 'Ver@@', '<unk>', '@', 'gan@@', '<unk>', '@', 'genheit', 'der', 'der', 'Re@@', '<unk>', '@', 'st', 'der', 'der', 'Ver@@', '<unk>', '@', 's@@', '<unk>', '@', 'icht', '.', '</s>']
2025-05-27 11:07:58,252 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 11:07:58,252 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 11:07:58,252 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Ver<unk> @ gan<unk> @ genheit der Ver<unk> @ k<unk> @ ut<unk> @ ung der Problem der Problem der Problem der Problem der Welt , weil es nicht die Ver<unk> @ gan<unk> @ genheit der der Ver<unk> @ gan<unk> @ genheit der der Re<unk> @ st der der Ver<unk> @ s<unk> @ icht .
2025-05-27 11:07:58,252 - INFO - joeynmt.training - Example #2
2025-05-27 11:07:58,252 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 11:07:58,252 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 11:07:58,252 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'F@@', '<unk>', '@', 'o@@', '<unk>', '@', 'f', 'auf', 'den', 'N@@', '<unk>', '@', 'amen', 'auf', 'den', 'N@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'el', 'ist', 'in', 'den', 'K@@', '<unk>', '@', 'u@@', '<unk>', '@', '-@@', '<unk>', '@', 'S@@', '<unk>', '@', 'il@@', '<unk>', '@', 'l', 'des', 'Ver@@', '<unk>', '@', 'gan@@', '<unk>', '@', 'genheit', '.', '</s>']
2025-05-27 11:07:58,253 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 11:07:58,253 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 11:07:58,253 - INFO - joeynmt.training - 	Hypothesis: Die F<unk> @ o<unk> @ f auf den N<unk> @ amen auf den N<unk> @ ot<unk> @ el ist in den K<unk> @ u<unk> @ -<unk> @ S<unk> @ il<unk> @ l des Ver<unk> @ gan<unk> @ genheit .
2025-05-27 11:07:58,253 - INFO - joeynmt.training - Example #3
2025-05-27 11:07:58,253 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 11:07:58,253 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 11:07:58,253 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'den', 'K@@', '<unk>', '@', 'r@@', '<unk>', '@', 'und', 'die', 'F@@', '<unk>', '@', 'lü@@', '<unk>', '@', 'ss@@', '<unk>', '@', 'el', '.', '</s>']
2025-05-27 11:07:58,253 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 11:07:58,253 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 11:07:58,253 - INFO - joeynmt.training - 	Hypothesis: Es ist in den K<unk> @ r<unk> @ und die F<unk> @ lü<unk> @ ss<unk> @ el .
2025-05-27 11:07:58,253 - INFO - joeynmt.training - Example #4
2025-05-27 11:07:58,253 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 11:07:58,253 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 11:07:58,253 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', ',', 'die', 'ich', 'Ihnen', 'sehen', ',', 'ist', 'eine', 'Sch@@', '<unk>', '@', 'n@@', '<unk>', '@', 'ein', ',', 'was', 'die', 'letzten', 'letzten', '20', 'Jahren', 'ist', '.', '</s>']
2025-05-27 11:07:58,253 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 11:07:58,253 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 11:07:58,253 - INFO - joeynmt.training - 	Hypothesis: Die nächste , die ich Ihnen sehen , ist eine Sch<unk> @ n<unk> @ ein , was die letzten letzten 20 Jahren ist .
2025-05-27 11:08:43,919 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     1.615023, Batch Acc: 0.576163, Tokens per Sec:     3401, Lr: 0.000300
2025-05-27 11:09:26,551 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     1.573450, Batch Acc: 0.581883, Tokens per Sec:     3631, Lr: 0.000300
2025-05-27 11:10:07,887 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     1.679386, Batch Acc: 0.586118, Tokens per Sec:     3696, Lr: 0.000300
2025-05-27 11:10:49,633 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     1.682508, Batch Acc: 0.589498, Tokens per Sec:     3697, Lr: 0.000300
2025-05-27 11:11:33,428 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     1.474412, Batch Acc: 0.594188, Tokens per Sec:     3554, Lr: 0.000300
2025-05-27 11:11:33,429 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 11:11:33,429 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:16:25,004 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   4.99, acc:   0.58, generation: 291.5617[sec], evaluation: 0.0000[sec]
2025-05-27 11:16:25,009 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:16:25,133 - INFO - joeynmt.helpers - delete models/bpe_4k_model/1500.ckpt
2025-05-27 11:16:25,139 - INFO - joeynmt.training - Example #0
2025-05-27 11:16:25,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 11:16:25,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 11:16:25,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'so', 'habe', 'ich', 'diese', 'zwei', 'Millionen', 'Jahren', ',', 'um', 'die', 'ich', 'zwei', 'Jahre', 'zeigen', ',', 'um', 'die', 'P@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'en@@', '<unk>', '@', 'de@@', '<unk>', '@', 't@@', '<unk>', '@', '-@@', '<unk>', '@', 'K@@', '<unk>', '@', 'am@@', '<unk>', '@', 'a@@', '<unk>', '@', 'f', ',', 'die', 'die', 'letzten', 'drei', 'Jahren', ',', 'die', 'die', 'drei', 'Millionen', 'Jahren', ',', 'die', 'die', 'die', 'drei', 'Millionen', 'Jahren', ',', 'die', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'die', 'die', 'drei', 'Jahren', ',', 'die', 'die', 'die', 'drei', 'Millionen', 'Jahren', ',', 'die', 'die', 'die', 'T@@', '<unk>', '@', 'ro@@', '<unk>', '@', 'm@@', '<unk>', '@', '-@@', '<unk>', '@', 'U@@', '<unk>', '@', '-@@', '<unk>', '@', 'U@@', '<unk>', '@', '-@@', '<unk>', '@', 'E@@', '<unk>', '@', 'ffe@@', '<unk>', '@', 'kt', ',', 'die', 'die', 'letzten', 'zwei', 'Millionen', 'Jahren', '.', '</s>']
2025-05-27 11:16:25,139 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 11:16:25,139 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 11:16:25,139 - INFO - joeynmt.training - 	Hypothesis: Und so habe ich diese zwei Millionen Jahren , um die ich zwei Jahre zeigen , um die P<unk> @ ot<unk> @ en<unk> @ de<unk> @ t<unk> @ -<unk> @ K<unk> @ am<unk> @ a<unk> @ f , die die letzten drei Jahren , die die drei Millionen Jahren , die die die drei Millionen Jahren , die die die letzten drei Millionen Jahren , die die drei Jahren , die die die drei Millionen Jahren , die die die T<unk> @ ro<unk> @ m<unk> @ -<unk> @ U<unk> @ -<unk> @ U<unk> @ -<unk> @ E<unk> @ ffe<unk> @ kt , die die letzten zwei Millionen Jahren .
2025-05-27 11:16:25,139 - INFO - joeynmt.training - Example #1
2025-05-27 11:16:25,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 11:16:25,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 11:16:25,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'der', 'Ver@@', '<unk>', '@', 'su@@', '<unk>', '@', 'cht', ',', 'die', 'die', 'Ver@@', '<unk>', '@', 'su@@', '<unk>', '@', 'cht', 'der', ',', 'weil', 'die', 'die', 'Ver@@', '<unk>', '@', 'su@@', '<unk>', '@', 'cht', 'der', 'der', 'der', 'K@@', '<unk>', '@', 'apit@@', '<unk>', '@', 'el', 'des', 'Sch@@', '<unk>', '@', 'la@@', '<unk>', '@', 'ss', ',', '</s>']
2025-05-27 11:16:25,139 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 11:16:25,139 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 11:16:25,140 - INFO - joeynmt.training - 	Hypothesis: Aber das ist der Ver<unk> @ su<unk> @ cht , die die Ver<unk> @ su<unk> @ cht der , weil die die Ver<unk> @ su<unk> @ cht der der der K<unk> @ apit<unk> @ el des Sch<unk> @ la<unk> @ ss ,
2025-05-27 11:16:25,140 - INFO - joeynmt.training - Example #2
2025-05-27 11:16:25,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 11:16:25,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 11:16:25,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', '<unk>', '@', 'amer@@', '<unk>', '@', 'a@@', '<unk>', '@', 'k', 'auf', 'der', 'N@@', '<unk>', '@', 'amen', ',', 'der', 'N@@', '<unk>', '@', 'l', 'ist', 'in', 'der', 'K@@', '<unk>', '@', 'apit@@', '<unk>', '@', 'el', 'des', 'P@@', '<unk>', '@', 'reis', 'der', 'glob@@', '<unk>', '@', 'alen', 'K@@', '<unk>', '@', 'apit@@', '<unk>', '@', 'al', '.', '</s>']
2025-05-27 11:16:25,140 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 11:16:25,140 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 11:16:25,140 - INFO - joeynmt.training - 	Hypothesis: Die K<unk> @ amer<unk> @ a<unk> @ k auf der N<unk> @ amen , der N<unk> @ l ist in der K<unk> @ apit<unk> @ el des P<unk> @ reis der glob<unk> @ alen K<unk> @ apit<unk> @ al .
2025-05-27 11:16:25,140 - INFO - joeynmt.training - Example #3
2025-05-27 11:16:25,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 11:16:25,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 11:16:25,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'aus', 'der', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'un@@', '<unk>', '@', 'm@@', '<unk>', '@', 'om@@', '<unk>', '@', 'er', 'in', 'der', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'm', '.', '</s>']
2025-05-27 11:16:25,140 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 11:16:25,140 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 11:16:25,140 - INFO - joeynmt.training - 	Hypothesis: Es ist aus der K<unk> @ ri<unk> @ m<unk> @ un<unk> @ m<unk> @ om<unk> @ er in der K<unk> @ ri<unk> @ m<unk> @ m .
2025-05-27 11:16:25,140 - INFO - joeynmt.training - Example #4
2025-05-27 11:16:25,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 11:16:25,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 11:16:25,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', ',', 'die', 'ich', 'zeigen', ',', 'ist', 'eine', 'Menge', 'von', 'der', 'letzten', '25', 'Jahren', ',', 'was', 'ich', 'hier', 'ist', ',', 'was', 'die', 'letzten', '25', 'Jahren', 'ist', '.', '</s>']
2025-05-27 11:16:25,141 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 11:16:25,141 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 11:16:25,141 - INFO - joeynmt.training - 	Hypothesis: Die nächste , die ich zeigen , ist eine Menge von der letzten 25 Jahren , was ich hier ist , was die letzten 25 Jahren ist .
2025-05-27 11:17:08,178 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     1.603334, Batch Acc: 0.595764, Tokens per Sec:     3558, Lr: 0.000300
2025-05-27 11:17:51,614 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     1.449857, Batch Acc: 0.599562, Tokens per Sec:     3545, Lr: 0.000300
2025-05-27 11:18:32,430 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     1.461586, Batch Acc: 0.606020, Tokens per Sec:     3739, Lr: 0.000300
2025-05-27 11:19:14,971 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     1.500073, Batch Acc: 0.608481, Tokens per Sec:     3628, Lr: 0.000300
2025-05-27 11:19:58,677 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     1.515335, Batch Acc: 0.611698, Tokens per Sec:     3543, Lr: 0.000300
2025-05-27 11:19:58,678 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 11:19:58,678 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:24:38,732 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.64, acc:   0.60, generation: 280.0416[sec], evaluation: 0.0000[sec]
2025-05-27 11:24:38,734 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:24:38,873 - INFO - joeynmt.helpers - delete models/bpe_4k_model/2000.ckpt
2025-05-27 11:24:38,877 - INFO - joeynmt.training - Example #0
2025-05-27 11:24:38,877 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 11:24:38,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 11:24:38,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'in', 'der', 'letzten', 'Jahr', ',', 'die', 'ich', 'zwei', 'Jahre', 'alt', ',', 'um', 'die', 'P@@', '<unk>', '@', 'la@@', '<unk>', '@', 'g@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ik', 'zu', 'zeigen', ',', 'um', 'die', 'P@@', '<unk>', '@', 'la@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'tig', 'hatte', ',', 'die', 'drei', 'Millionen', 'Jahren', 'in', 'der', 'letzten', 'drei', 'Jahren', 'in', 'den', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'die', 'drei', 'Jahren', 'drei', 'Jahren', 'in', 'den', 'USA', ',', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', 'in', 'den', 'USA', ',', 'die', 'USA', ',', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', 'war', '.', '</s>']
2025-05-27 11:24:38,877 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 11:24:38,878 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 11:24:38,878 - INFO - joeynmt.training - 	Hypothesis: Und in der letzten Jahr , die ich zwei Jahre alt , um die P<unk> @ la<unk> @ g<unk> @ st<unk> @ ik zu zeigen , um die P<unk> @ la<unk> @ st<unk> @ ar<unk> @ tig hatte , die drei Millionen Jahren in der letzten drei Jahren in den letzten drei Millionen Jahren , die drei Jahren drei Jahren in den USA , die letzten drei Millionen Jahren in den USA , die USA , die letzten drei Millionen Jahren war .
2025-05-27 11:24:38,878 - INFO - joeynmt.training - Example #1
2025-05-27 11:24:38,878 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 11:24:38,878 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 11:24:38,878 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'der', 'Ver@@', '<unk>', '@', 'su@@', '<unk>', '@', 'cht', 'der', ',', 'die', 'die', 'die', 'Ver@@', '<unk>', '@', 'sion', 'des', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'i@@', '<unk>', '@', 'zi@@', '<unk>', '@', 'p', 'des', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'i@@', '<unk>', '@', 'zi@@', '<unk>', '@', 'ent', '.', '</s>']
2025-05-27 11:24:38,878 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 11:24:38,878 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 11:24:38,878 - INFO - joeynmt.training - 	Hypothesis: Aber das ist der Ver<unk> @ su<unk> @ cht der , die die die Ver<unk> @ sion des E<unk> @ is<unk> @ i<unk> @ zi<unk> @ p des E<unk> @ is<unk> @ i<unk> @ zi<unk> @ ent .
2025-05-27 11:24:38,878 - INFO - joeynmt.training - Example #2
2025-05-27 11:24:38,879 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 11:24:38,879 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 11:24:38,879 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'der', 'N@@', '<unk>', '@', 'amen', 'auf', 'der', 'N@@', '<unk>', '@', 'amen', 'N@@', '<unk>', '@', 'amen', 'ist', 'in', 'der', 'N@@', '<unk>', '@', 'amen', 'der', 'N@@', '<unk>', '@', 'amen', 'des', 'glob@@', '<unk>', '@', 'alen', 'Ver@@', '<unk>', '@', 'sion', 'des', 'glob@@', '<unk>', '@', 'alen', 'Ver@@', '<unk>', '@', 'sion', 'des', 'des', 'glob@@', '<unk>', '@', 'alen', 'Ver@@', '<unk>', '@', 'sion', '.', '</s>']
2025-05-27 11:24:38,879 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 11:24:38,879 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 11:24:38,879 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ e der N<unk> @ amen auf der N<unk> @ amen N<unk> @ amen ist in der N<unk> @ amen der N<unk> @ amen des glob<unk> @ alen Ver<unk> @ sion des glob<unk> @ alen Ver<unk> @ sion des des glob<unk> @ alen Ver<unk> @ sion .
2025-05-27 11:24:38,879 - INFO - joeynmt.training - Example #3
2025-05-27 11:24:38,879 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 11:24:38,879 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 11:24:38,879 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'aus', 'der', 'Sch@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'se', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'se', '.', '</s>']
2025-05-27 11:24:38,879 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 11:24:38,879 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 11:24:38,879 - INFO - joeynmt.training - 	Hypothesis: Es ist aus der Sch<unk> @ ri<unk> @ m<unk> @ se und K<unk> @ ri<unk> @ m<unk> @ se .
2025-05-27 11:24:38,879 - INFO - joeynmt.training - Example #4
2025-05-27 11:24:38,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 11:24:38,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 11:24:38,880 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'nächste', 'sehen', ',', 'die', 'ich', 'zeigen', ',', 'die', 'ich', 'zeigen', ',', 'was', 'die', 'letzten', '25', 'Jahren', 'ist', '.', '</s>']
2025-05-27 11:24:38,880 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 11:24:38,880 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 11:24:38,880 - INFO - joeynmt.training - 	Hypothesis: Die nächste nächste sehen , die ich zeigen , die ich zeigen , was die letzten 25 Jahren ist .
2025-05-27 11:25:18,697 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     1.290507, Batch Acc: 0.617761, Tokens per Sec:     3783, Lr: 0.000300
2025-05-27 11:26:00,580 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     1.518684, Batch Acc: 0.618335, Tokens per Sec:     3719, Lr: 0.000300
2025-05-27 11:26:43,208 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     1.421564, Batch Acc: 0.622985, Tokens per Sec:     3560, Lr: 0.000300
2025-05-27 11:27:24,501 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     1.343878, Batch Acc: 0.625347, Tokens per Sec:     3673, Lr: 0.000300
2025-05-27 11:28:04,233 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     1.283047, Batch Acc: 0.626370, Tokens per Sec:     3780, Lr: 0.000300
2025-05-27 11:28:04,233 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 11:28:04,233 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:32:24,729 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.39, acc:   0.61, generation: 260.4826[sec], evaluation: 0.0000[sec]
2025-05-27 11:32:24,734 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:32:24,859 - INFO - joeynmt.helpers - delete models/bpe_4k_model/2500.ckpt
2025-05-27 11:32:24,864 - INFO - joeynmt.training - Example #0
2025-05-27 11:32:24,864 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 11:32:24,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 11:32:24,864 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'or@@', '<unk>', '@', 'ig', 'Jahr', 'Jahr', ',', 'um', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'e', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'ie', 'der', ',', 'die', 'die', 'drei', 'Millionen', 'Jahren', 'drei', 'Jahren', 'drei', 'Jahren', 'drei', 'Jahren', 'hatte', ',', 'die', 'drei', 'Millionen', 'Jahren', 'drei', 'Jahren', 'drei', 'Jahren', 'in', 'den', 'letzten', 'drei', 'Jahren', 'drei', 'Jahren', ',', 'die', 'drei', 'Prozent', 'der', 'USA', ',', 'die', 'drei', 'Millionen', 'von', '40', 'Prozent', 'der', 'USA', ',', 'die', 'drei', 'Prozent', 'der', 'USA', ',', 'die', 'drei', 'Prozent', 'der', 'USA', ',', 'die', 'drei', 'Prozent', 'der', 'USA', ',', 'die', 'drei', 'Millionen', 'von', '40', 'Prozent', 'der', 'USA', ',', 'die', 'drei', 'Prozent', 'der', 'USA', ',', 'die', 'drei', 'Prozent', 'der', 'USA', ',', '</s>']
2025-05-27 11:32:24,865 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 11:32:24,865 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 11:32:24,865 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ or<unk> @ ig Jahr Jahr , um die P<unk> @ oo<unk> @ e zu zeigen , dass die P<unk> @ oo<unk> @ lin<unk> @ ie der , die die drei Millionen Jahren drei Jahren drei Jahren drei Jahren hatte , die drei Millionen Jahren drei Jahren drei Jahren in den letzten drei Jahren drei Jahren , die drei Prozent der USA , die drei Millionen von 40 Prozent der USA , die drei Prozent der USA , die drei Prozent der USA , die drei Prozent der USA , die drei Millionen von 40 Prozent der USA , die drei Prozent der USA , die drei Prozent der USA ,
2025-05-27 11:32:24,865 - INFO - joeynmt.training - Example #1
2025-05-27 11:32:24,865 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 11:32:24,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 11:32:24,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', ',', 'dass', 'das', 'wirklich', 'die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'i@@', '<unk>', '@', 'versu@@', '<unk>', '@', 'ms', ',', 'weil', 'es', 'nicht', 'die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ik@@', '<unk>', '@', 'o', 'der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ges', 'Problem', 'ist', '.', '</s>']
2025-05-27 11:32:24,865 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 11:32:24,865 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 11:32:24,865 - INFO - joeynmt.training - 	Hypothesis: Aber das ist , dass das wirklich die E<unk> @ is<unk> @ i<unk> @ versu<unk> @ ms , weil es nicht die E<unk> @ is<unk> @ ik<unk> @ o der E<unk> @ is<unk> @ i<unk> @ ges Problem ist .
2025-05-27 11:32:24,865 - INFO - joeynmt.training - Example #2
2025-05-27 11:32:24,865 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 11:32:24,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 11:32:24,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'in@@', '<unk>', '@', 'e', 'auf', 'der', 'N@@', '<unk>', '@', 'um@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'm@@', '<unk>', '@', 'mer@@', '<unk>', '@', 'n', 'ist', 'in', 'den', 'Se@@', '<unk>', '@', 'kunde', 'von', 'uns', 'glob@@', '<unk>', '@', 'alen', 'K@@', '<unk>', '@', 'la@@', '<unk>', '@', 'ss', 'von', 'der', 'glob@@', '<unk>', '@', 'alen', 'K@@', '<unk>', '@', 'ommun@@', '<unk>', '@', 'ik@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 11:32:24,866 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 11:32:24,866 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 11:32:24,866 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ k<unk> @ in<unk> @ e auf der N<unk> @ um<unk> @ sch<unk> @ m<unk> @ mer<unk> @ n ist in den Se<unk> @ kunde von uns glob<unk> @ alen K<unk> @ la<unk> @ ss von der glob<unk> @ alen K<unk> @ ommun<unk> @ ik<unk> @ stem .
2025-05-27 11:32:24,866 - INFO - joeynmt.training - Example #3
2025-05-27 11:32:24,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 11:32:24,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 11:32:24,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'B@@', '<unk>', '@', 'rü@@', '<unk>', '@', 'ck@@', '<unk>', '@', 's', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'eines', 'S@@', '<unk>', '@', 'om@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 11:32:24,866 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 11:32:24,866 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 11:32:24,866 - INFO - joeynmt.training - 	Hypothesis: Es ist in der B<unk> @ rü<unk> @ ck<unk> @ s und k<unk> @ ri<unk> @ m<unk> @ eines S<unk> @ om<unk> @ er .
2025-05-27 11:32:24,866 - INFO - joeynmt.training - Example #4
2025-05-27 11:32:24,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 11:32:24,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 11:32:24,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'e@@', '<unk>', '@', 'dia', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'was', 'die', '25', 'Jahre', 'Jahre', 'Jahre', 'Jahre', 'alt', 'ist', '.', '</s>']
2025-05-27 11:32:24,866 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 11:32:24,866 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 11:32:24,866 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ e<unk> @ dia , die ich Ihnen zeigen , was die 25 Jahre Jahre Jahre Jahre alt ist .
2025-05-27 11:33:08,723 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     1.508206, Batch Acc: 0.625158, Tokens per Sec:     3535, Lr: 0.000300
2025-05-27 11:33:53,384 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     1.351671, Batch Acc: 0.630024, Tokens per Sec:     3476, Lr: 0.000300
2025-05-27 11:34:35,983 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     1.427557, Batch Acc: 0.631428, Tokens per Sec:     3560, Lr: 0.000300
2025-05-27 11:35:20,795 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     1.439179, Batch Acc: 0.632644, Tokens per Sec:     3458, Lr: 0.000300
2025-05-27 11:36:02,491 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     1.392257, Batch Acc: 0.637837, Tokens per Sec:     3698, Lr: 0.000300
2025-05-27 11:36:02,491 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 11:36:02,491 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:40:03,745 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.21, acc:   0.62, generation: 241.2411[sec], evaluation: 0.0000[sec]
2025-05-27 11:40:03,749 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:40:03,872 - INFO - joeynmt.helpers - delete models/bpe_4k_model/3000.ckpt
2025-05-27 11:40:03,874 - INFO - joeynmt.training - Example #0
2025-05-27 11:40:03,875 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 11:40:03,875 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 11:40:03,875 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'beiden', 'F@@', '<unk>', '@', 'amil@@', '<unk>', '@', 'ien', ',', 'die', 'ich', 'diese', 'beiden', 'F@@', '<unk>', '@', 'amil@@', '<unk>', '@', 'ien', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'von', 'der', 'letzten', 'drei', 'Jahren', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', 'hatte', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'mit', 'der', 'der', 'letzten', 'drei', 'Millionen', 'von', '40', 'Jahren', ',', 'mit', 'den', 'letzten', 'drei', 'Millionen', 'von', '40', 'Prozent', 'der', 'USA', ',', 'mit', 'den', 'den', 'den', 'USA', ',', 'mit', 'den', 'den', 'den', 'den', 'USA', ',', 'die', 'die', 'USA', ',', 'die', 'die', '40', 'Prozent', 'der', 'USA', ',', 'mit', 'den', 'USA', ',', 'mit', '40', 'Prozent', 'der', '40', 'Prozent', 'der', 'USA', ',', '</s>']
2025-05-27 11:40:03,875 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 11:40:03,875 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 11:40:03,875 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese beiden F<unk> @ amil<unk> @ ien , die ich diese beiden F<unk> @ amil<unk> @ ien , die die letzten drei Millionen von der letzten drei Jahren , die die letzten drei Millionen Jahren , die die letzten drei Millionen Jahren hatte , die die letzten drei Millionen Jahren , mit der der letzten drei Millionen von 40 Jahren , mit den letzten drei Millionen von 40 Prozent der USA , mit den den den USA , mit den den den den USA , die die USA , die die 40 Prozent der USA , mit den USA , mit 40 Prozent der 40 Prozent der USA ,
2025-05-27 11:40:03,875 - INFO - joeynmt.training - Example #1
2025-05-27 11:40:03,875 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 11:40:03,875 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 11:40:03,875 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'E@@', '<unk>', '@', 'moti@@', '<unk>', '@', 'onen', 'von', 'diesem', 'spezi@@', '<unk>', '@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', '<unk>', '@', 're@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er@@', '<unk>', '@', 'for@@', '<unk>', '@', 'men', '.', '</s>']
2025-05-27 11:40:03,875 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 11:40:03,875 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 11:40:03,875 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die E<unk> @ moti<unk> @ onen von diesem spezi<unk> @ elle Problem , weil es nicht die D<unk> @ re<unk> @ ck<unk> @ er<unk> @ for<unk> @ men .
2025-05-27 11:40:03,875 - INFO - joeynmt.training - Example #2
2025-05-27 11:40:03,875 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 11:40:03,875 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 11:40:03,875 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'N@@', '<unk>', '@', 'ur', 'der', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', 'Krie@@', '<unk>', '@', 'g', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', '&quot;', 'W@@', '<unk>', '@', 'ür@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ig', '&quot;', '</s>']
2025-05-27 11:40:03,876 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 11:40:03,876 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 11:40:03,876 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf der N<unk> @ ur der &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; Krie<unk> @ g &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; W<unk> @ ür<unk> @ f<unk> @ ig &quot;
2025-05-27 11:40:03,876 - INFO - joeynmt.training - Example #3
2025-05-27 11:40:03,876 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 11:40:03,876 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 11:40:03,876 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'aus', 'der', 'W@@', '<unk>', '@', 'ert', ',', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'itten', 'in', 'der', 'S@@', '<unk>', '@', 'om@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 11:40:03,876 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 11:40:03,876 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 11:40:03,876 - INFO - joeynmt.training - 	Hypothesis: Es ist aus der W<unk> @ ert , und k<unk> @ ri<unk> @ m<unk> @ itten in der S<unk> @ om<unk> @ er .
2025-05-27 11:40:03,876 - INFO - joeynmt.training - Example #4
2025-05-27 11:40:03,876 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 11:40:03,876 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 11:40:03,876 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'ern@@', '<unk>', '@', 'seh@@', '<unk>', '@', 'e', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'was', 'die', 'letzten', '25', 'Jahren', 'ist', ',', 'was', 'die', 'letzten', '25', 'Jahren', 'ist', ',', 'ist', 'das', 'letzten', '25', 'Jahren', 'ist', '.', '</s>']
2025-05-27 11:40:03,876 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 11:40:03,876 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 11:40:03,877 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ ern<unk> @ seh<unk> @ e , die ich Ihnen zeigen , was die letzten 25 Jahren ist , was die letzten 25 Jahren ist , ist das letzten 25 Jahren ist .
2025-05-27 11:40:45,178 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     1.429265, Batch Acc: 0.637884, Tokens per Sec:     3638, Lr: 0.000300
2025-05-27 11:41:26,367 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     1.273518, Batch Acc: 0.636357, Tokens per Sec:     3708, Lr: 0.000300
2025-05-27 11:42:06,870 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     1.290150, Batch Acc: 0.641699, Tokens per Sec:     3718, Lr: 0.000300
2025-05-27 11:42:47,800 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     1.400185, Batch Acc: 0.642594, Tokens per Sec:     3654, Lr: 0.000300
2025-05-27 11:43:32,476 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     1.377777, Batch Acc: 0.641761, Tokens per Sec:     3472, Lr: 0.000300
2025-05-27 11:43:32,477 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 11:43:32,477 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:47:33,177 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.09, acc:   0.62, generation: 240.6877[sec], evaluation: 0.0000[sec]
2025-05-27 11:47:33,182 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:47:33,301 - INFO - joeynmt.helpers - delete models/bpe_4k_model/3500.ckpt
2025-05-27 11:47:33,307 - INFO - joeynmt.training - Example #0
2025-05-27 11:47:33,307 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 11:47:33,307 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 11:47:33,307 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'or@@', '<unk>', '@', 'ge', 'ich', 'diese', 'zwei', 'F@@', '<unk>', '@', 'la@@', '<unk>', '@', 'g@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ück', ',', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'p', ',', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', 'hatte', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', 'hatte', ',', 'die', 'größ@@', '<unk>', '@', 'te', 'drei', 'Millionen', 'Jahren', ',', 'um', 'die', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', '</s>']
2025-05-27 11:47:33,307 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 11:47:33,307 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 11:47:33,307 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ or<unk> @ ge ich diese zwei F<unk> @ la<unk> @ g<unk> @ st<unk> @ ück , die P<unk> @ oo<unk> @ l<unk> @ ar<unk> @ p , die letzten drei Millionen Jahren hatte , die die letzten drei Millionen Jahren , die die letzten drei Millionen Jahren hatte , die größ<unk> @ te drei Millionen Jahren , um die USA , mit 40 Prozent der USA ,
2025-05-27 11:47:33,307 - INFO - joeynmt.training - Example #1
2025-05-27 11:47:33,307 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 11:47:33,307 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 11:47:33,307 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'der', 'Ver@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'lag', 'der', 'E@@', '<unk>', '@', 't@@', '<unk>', '@', 'was', 'das', 'ist', 'der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 's', 'der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 's', ',', 'weil', 'es', 'nicht', 'die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 's', 'der', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 's', 'sehen', '.', '</s>']
2025-05-27 11:47:33,308 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 11:47:33,308 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 11:47:33,308 - INFO - joeynmt.training - 	Hypothesis: Aber das ist der Ver<unk> @ sch<unk> @ lag der E<unk> @ t<unk> @ was das ist der E<unk> @ is<unk> @ s der E<unk> @ is<unk> @ s , weil es nicht die E<unk> @ is<unk> @ s der E<unk> @ is des E<unk> @ is<unk> @ s sehen .
2025-05-27 11:47:33,308 - INFO - joeynmt.training - Example #2
2025-05-27 11:47:33,308 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 11:47:33,308 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 11:47:33,308 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'dem', 'N@@', '<unk>', '@', 'oor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ool', 'ist', 'in', 'der', 'glob@@', '<unk>', '@', 'alen', 'Her@@', '<unk>', '@', 'z@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'r@@', '<unk>', '@', 'icht', '.', '</s>']
2025-05-27 11:47:33,308 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 11:47:33,308 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 11:47:33,308 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf dem N<unk> @ oor<unk> @ d<unk> @ p<unk> @ ool ist in der glob<unk> @ alen Her<unk> @ z<unk> @ sch<unk> @ r<unk> @ icht .
2025-05-27 11:47:33,309 - INFO - joeynmt.training - Example #3
2025-05-27 11:47:33,309 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 11:47:33,309 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 11:47:33,309 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'ft', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'ft', 'in', 'der', 'S@@', '<unk>', '@', 'om@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 11:47:33,309 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 11:47:33,309 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 11:47:33,309 - INFO - joeynmt.training - 	Hypothesis: Es ist in der K<unk> @ ri<unk> @ m<unk> @ ft und K<unk> @ ri<unk> @ m<unk> @ ft in der S<unk> @ om<unk> @ er .
2025-05-27 11:47:33,309 - INFO - joeynmt.training - Example #4
2025-05-27 11:47:33,309 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 11:47:33,309 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 11:47:33,309 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'ik@@', '<unk>', '@', 'a', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2025-05-27 11:47:33,309 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 11:47:33,309 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 11:47:33,309 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ ik<unk> @ a ist eine Ver<unk> @ sion von dem letzten 25 Jahren passiert .
2025-05-27 11:48:17,149 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     1.239082, Batch Acc: 0.645589, Tokens per Sec:     3505, Lr: 0.000300
2025-05-27 11:48:57,797 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     1.255640, Batch Acc: 0.644586, Tokens per Sec:     3777, Lr: 0.000300
2025-05-27 11:49:19,791 - INFO - joeynmt.training - Epoch   2: total training loss 4605.54
2025-05-27 11:49:19,792 - INFO - joeynmt.training - EPOCH 3
2025-05-27 11:49:42,055 - INFO - joeynmt.training - Epoch   3, Step:     6300, Batch Loss:     1.275303, Batch Acc: 0.652910, Tokens per Sec:     3398, Lr: 0.000300
2025-05-27 11:50:24,888 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     1.306642, Batch Acc: 0.658881, Tokens per Sec:     3541, Lr: 0.000300
2025-05-27 11:51:09,845 - INFO - joeynmt.training - Epoch   3, Step:     6500, Batch Loss:     1.244245, Batch Acc: 0.657514, Tokens per Sec:     3456, Lr: 0.000300
2025-05-27 11:51:09,846 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 11:51:09,846 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:55:42,035 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.99, acc:   0.63, generation: 272.1761[sec], evaluation: 0.0000[sec]
2025-05-27 11:55:42,041 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:55:42,168 - INFO - joeynmt.helpers - delete models/bpe_4k_model/4000.ckpt
2025-05-27 11:55:42,174 - INFO - joeynmt.training - Example #0
2025-05-27 11:55:42,175 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 11:55:42,175 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 11:55:42,175 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'so', ',', 'dass', 'die', 'letzten', 'zwei', 'F@@', '<unk>', '@', 'amil@@', '<unk>', '@', 'ien', 'sehen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'wen@@', '<unk>', '@', 'dung', ',', 'die', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'die', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', 'hatte', ',', 'die', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'mit', 'dem', 'dem', 'F@@', '<unk>', '@', 'ern@@', '<unk>', '@', 'seh@@', '<unk>', '@', 'e', ',', 'die', 'die', 'drei', 'Prozent', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', 'den', 'F@@', '<unk>', '@', 'ern@@', '<unk>', '@', 'seh@@', '<unk>', '@', 'e', 'war', '.', '</s>']
2025-05-27 11:55:42,175 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 11:55:42,175 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 11:55:42,175 - INFO - joeynmt.training - 	Hypothesis: Und so , dass die letzten zwei F<unk> @ amil<unk> @ ien sehen , um zu zeigen , dass die P<unk> @ ot<unk> @ wen<unk> @ dung , die die die letzten drei Millionen Jahren , die die die letzten drei Millionen Jahren hatte , die die die letzten drei Millionen Jahren , mit dem dem F<unk> @ ern<unk> @ seh<unk> @ e , die die drei Prozent der Verein<unk> @ igten Staaten , mit den F<unk> @ ern<unk> @ seh<unk> @ e war .
2025-05-27 11:55:42,175 - INFO - joeynmt.training - Example #1
2025-05-27 11:55:42,175 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 11:55:42,175 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 11:55:42,175 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'der', 'Ver@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'l@@', '<unk>', '@', 'ungs@@', '<unk>', '@', 'l@@', '<unk>', '@', 'imm@@', '<unk>', '@', 'el', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'men@@', '<unk>', '@', 'si@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'te', 'des', 'E@@', '<unk>', '@', 'is', 'der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'i@@', '<unk>', '@', 'versu@@', '<unk>', '@', 'ms', 'sehen', '.', '</s>']
2025-05-27 11:55:42,175 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 11:55:42,175 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 11:55:42,175 - INFO - joeynmt.training - 	Hypothesis: Aber das ist der Ver<unk> @ sch<unk> @ l<unk> @ ungs<unk> @ l<unk> @ imm<unk> @ el , weil es nicht die Di<unk> @ men<unk> @ si<unk> @ ck<unk> @ te des E<unk> @ is der E<unk> @ is<unk> @ i<unk> @ versu<unk> @ ms sehen .
2025-05-27 11:55:42,175 - INFO - joeynmt.training - Example #2
2025-05-27 11:55:42,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 11:55:42,176 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 11:55:42,176 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'N@@', '<unk>', '@', 'or@@', '<unk>', '@', 'd@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ool', 'ist', 'in', 'die', 'Se@@', '<unk>', '@', 'ch@@', '<unk>', '@', 's@@', '<unk>', '@', 'eite', 'von', 'uns', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wan@@', '<unk>', '@', 'del', '.', '</s>']
2025-05-27 11:55:42,176 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 11:55:42,176 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 11:55:42,176 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf der N<unk> @ or<unk> @ d<unk> @ p<unk> @ ool ist in die Se<unk> @ ch<unk> @ s<unk> @ eite von uns glob<unk> @ alen Klima<unk> @ wan<unk> @ del .
2025-05-27 11:55:42,176 - INFO - joeynmt.training - Example #3
2025-05-27 11:55:42,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 11:55:42,176 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 11:55:42,176 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'aus', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'ft', 'in', 'den', 'S@@', '<unk>', '@', 'om@@', '<unk>', '@', 'er', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'ft', '.', '</s>']
2025-05-27 11:55:42,176 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 11:55:42,176 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 11:55:42,176 - INFO - joeynmt.training - 	Hypothesis: Es ist aus der Win<unk> @ ter und K<unk> @ ri<unk> @ m<unk> @ ft in den S<unk> @ om<unk> @ er und K<unk> @ ri<unk> @ ft .
2025-05-27 11:55:42,176 - INFO - joeynmt.training - Example #4
2025-05-27 11:55:42,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 11:55:42,176 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 11:55:42,176 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'zei@@', '<unk>', '@', 'ge', ',', 'die', 'ich', 'von', '25', 'Jahren', 'ist', 'ein', 'Ver@@', '<unk>', '@', 'sion', 'von', '25', 'Jahren', '.', '</s>']
2025-05-27 11:55:42,177 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 11:55:42,177 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 11:55:42,177 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich zei<unk> @ ge , die ich von 25 Jahren ist ein Ver<unk> @ sion von 25 Jahren .
2025-05-27 11:56:24,495 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     1.202964, Batch Acc: 0.658554, Tokens per Sec:     3539, Lr: 0.000300
2025-05-27 11:57:05,908 - INFO - joeynmt.training - Epoch   3, Step:     6700, Batch Loss:     1.340417, Batch Acc: 0.656542, Tokens per Sec:     3672, Lr: 0.000300
2025-05-27 11:57:49,272 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     1.215735, Batch Acc: 0.659069, Tokens per Sec:     3518, Lr: 0.000300
2025-05-27 12:00:09,537 - INFO - joeynmt.training - Epoch   3, Step:     6900, Batch Loss:     1.137669, Batch Acc: 0.658302, Tokens per Sec:     1112, Lr: 0.000300
2025-05-27 12:01:53,247 - INFO - joeynmt.training - Epoch   3, Step:     7000, Batch Loss:     1.323973, Batch Acc: 0.656573, Tokens per Sec:     1455, Lr: 0.000300
2025-05-27 12:01:53,248 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 12:01:53,248 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 12:09:58,171 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.92, acc:   0.64, generation: 484.9115[sec], evaluation: 0.0000[sec]
2025-05-27 12:09:58,178 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 12:09:58,299 - INFO - joeynmt.helpers - delete models/bpe_4k_model/4500.ckpt
2025-05-27 12:09:58,305 - INFO - joeynmt.training - Example #0
2025-05-27 12:09:58,305 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 12:09:58,305 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 12:09:58,305 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'zwei', 'F@@', '<unk>', '@', 'or@@', '<unk>', '@', 'ge', 'von', 'den', 'zwei', 'F@@', '<unk>', '@', 'amil@@', '<unk>', '@', 'ien', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'op@@', '<unk>', '@', '-@@', '<unk>', '@', 'C@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'die', 'die', 'die', 'Größ@@', '<unk>', '@', 'e', 'der', 'USA', ',', 'mit', 'den', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'mit', 'den', 'letzten', 'drei', 'Jahren', ',', 'mit', 'der', 'USA', ',', 'mit', 'den', 'den', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'än@@', '<unk>', '@', 'gt', '.', '</s>']
2025-05-27 12:09:58,305 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 12:09:58,305 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 12:09:58,305 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese zwei F<unk> @ or<unk> @ ge von den zwei F<unk> @ amil<unk> @ ien zeigen , dass die P<unk> @ op<unk> @ op<unk> @ -<unk> @ C<unk> @ a<unk> @ p , die die letzten drei Millionen Jahren , die die die Größ<unk> @ e der USA , mit den letzten drei Millionen Jahren , mit den letzten drei Jahren , mit der USA , mit den den USA , mit 40 Prozent der USA , mit 40 Prozent der USA , mit 40 Prozent ver<unk> @ r<unk> @ än<unk> @ gt .
2025-05-27 12:09:58,305 - INFO - joeynmt.training - Example #1
2025-05-27 12:09:58,305 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 12:09:58,305 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 12:09:58,305 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'ein', 'Teil', 'der', 'Sch@@', '<unk>', '@', 'n@@', '<unk>', '@', 'an@@', '<unk>', '@', 'z', ',', 'weil', 'es', 'nicht', 'die', 'D@@', '<unk>', '@', 'at@@', '<unk>', '@', 'en@@', '<unk>', '@', 'ze', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', '.', '</s>']
2025-05-27 12:09:58,306 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 12:09:58,306 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 12:09:58,306 - INFO - joeynmt.training - 	Hypothesis: Aber das ist ein Teil der Sch<unk> @ n<unk> @ an<unk> @ z , weil es nicht die D<unk> @ at<unk> @ en<unk> @ ze des E<unk> @ is des E<unk> @ is .
2025-05-27 12:09:58,306 - INFO - joeynmt.training - Example #2
2025-05-27 12:09:58,306 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 12:09:58,306 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 12:09:58,306 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is', 'der', 'N@@', '<unk>', '@', 'amen', 'auf', 'dem', 'N@@', '<unk>', '@', 'or@@', '<unk>', '@', 'd@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ool', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', ',', 'die', 'K@@', '<unk>', '@', 'amp@@', '<unk>', '@', 'f', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wan@@', '<unk>', '@', 'del', '.', '</s>']
2025-05-27 12:09:58,306 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 12:09:58,306 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 12:09:58,306 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is der N<unk> @ amen auf dem N<unk> @ or<unk> @ d<unk> @ p<unk> @ ool ist in gew<unk> @ is<unk> @ se , die K<unk> @ amp<unk> @ f des glob<unk> @ alen Klima<unk> @ wan<unk> @ del .
2025-05-27 12:09:58,306 - INFO - joeynmt.training - Example #3
2025-05-27 12:09:58,306 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 12:09:58,306 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 12:09:58,306 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'om@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 12:09:58,306 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 12:09:58,306 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 12:09:58,306 - INFO - joeynmt.training - 	Hypothesis: Es ist in der K<unk> @ ri<unk> @ m<unk> @ pt und K<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ om<unk> @ er .
2025-05-27 12:09:58,306 - INFO - joeynmt.training - Example #4
2025-05-27 12:09:58,307 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 12:09:58,307 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 12:09:58,307 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'ern@@', '<unk>', '@', 'st', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'die', 'letzten', '25', 'Jahren', 'ist', 'passiert', '.', '</s>']
2025-05-27 12:09:58,307 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 12:09:58,307 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 12:09:58,307 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ ern<unk> @ st , die ich Ihnen zeigen , ist eine Ver<unk> @ sion von dem , was die letzten 25 Jahren ist passiert .
2025-05-27 12:10:38,528 - INFO - joeynmt.training - Epoch   3, Step:     7100, Batch Loss:     1.277768, Batch Acc: 0.656651, Tokens per Sec:     3676, Lr: 0.000300
2025-05-27 12:11:21,756 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     1.204371, Batch Acc: 0.660653, Tokens per Sec:     3555, Lr: 0.000300
2025-05-27 12:12:03,815 - INFO - joeynmt.training - Epoch   3, Step:     7300, Batch Loss:     1.282984, Batch Acc: 0.659763, Tokens per Sec:     3655, Lr: 0.000300
2025-05-27 12:12:47,983 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     1.211666, Batch Acc: 0.661884, Tokens per Sec:     3492, Lr: 0.000300
2025-05-27 12:13:31,404 - INFO - joeynmt.training - Epoch   3, Step:     7500, Batch Loss:     1.266952, Batch Acc: 0.661613, Tokens per Sec:     3618, Lr: 0.000300
2025-05-27 12:13:31,405 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 12:13:31,405 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 12:17:58,953 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.85, acc:   0.64, generation: 267.5355[sec], evaluation: 0.0000[sec]
2025-05-27 12:17:58,954 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 12:17:59,071 - INFO - joeynmt.helpers - delete models/bpe_4k_model/5000.ckpt
2025-05-27 12:17:59,074 - INFO - joeynmt.training - Example #0
2025-05-27 12:17:59,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 12:17:59,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 12:17:59,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'or@@', '<unk>', '@', 'ig', 'Jahr', 'lie@@', '<unk>', '@', 'ß', 'ich', 'diese', 'zwei', 'F@@', '<unk>', '@', 'olie', 'sehen', ',', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', 'vor', 'etwa', '40', 'Prozent', 'der', 'Größ@@', '<unk>', '@', 'e', 'der', 'USA', ',', 'die', 'die', 'Größ@@', '<unk>', '@', 'e', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'ach@@', '<unk>', '@', 'el', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'ern@@', '<unk>', '@', 'seh@@', '<unk>', '@', 'e', 'war', '.', '</s>']
2025-05-27 12:17:59,074 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 12:17:59,074 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 12:17:59,074 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ or<unk> @ ig Jahr lie<unk> @ ß ich diese zwei F<unk> @ olie sehen , die P<unk> @ oo<unk> @ sk<unk> @ a<unk> @ p , die die letzten drei Millionen Jahren vor etwa 40 Prozent der Größ<unk> @ e der USA , die die Größ<unk> @ e der USA , mit 40 Prozent der F<unk> @ ach<unk> @ el , mit 40 Prozent der USA , mit 40 Prozent der F<unk> @ ern<unk> @ seh<unk> @ e war .
2025-05-27 12:17:59,074 - INFO - joeynmt.training - Example #1
2025-05-27 12:17:59,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 12:17:59,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 12:17:59,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'wirklich', 'die', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 's', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', '.', '</s>']
2025-05-27 12:17:59,075 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 12:17:59,075 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 12:17:59,075 - INFO - joeynmt.training - 	Hypothesis: Aber das ist wirklich die E<unk> @ is des E<unk> @ is<unk> @ s , weil es nicht die Di<unk> @ kte des E<unk> @ is des E<unk> @ is des E<unk> @ is des E<unk> @ is .
2025-05-27 12:17:59,075 - INFO - joeynmt.training - Example #2
2025-05-27 12:17:59,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 12:17:59,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 12:17:59,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'dem', 'N@@', '<unk>', '@', 'oor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ool', 'ist', 'in', 'gew@@', '<unk>', '@', 'alt@@', '<unk>', '@', 'ige', 'S@@', '<unk>', '@', 'to@@', '<unk>', '@', 'ff@@', '<unk>', '@', 'ent@@', '<unk>', '@', 'lich', '.', '</s>']
2025-05-27 12:17:59,075 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 12:17:59,075 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 12:17:59,075 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf dem N<unk> @ oor<unk> @ d<unk> @ p<unk> @ ool ist in gew<unk> @ alt<unk> @ ige S<unk> @ to<unk> @ ff<unk> @ ent<unk> @ lich .
2025-05-27 12:17:59,075 - INFO - joeynmt.training - Example #3
2025-05-27 12:17:59,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 12:17:59,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 12:17:59,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', ',', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'om@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 12:17:59,076 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 12:17:59,076 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 12:17:59,076 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter , und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ om<unk> @ er .
2025-05-27 12:17:59,076 - INFO - joeynmt.training - Example #4
2025-05-27 12:17:59,076 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 12:17:59,076 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 12:17:59,076 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'zeigt', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'es', 'passiert', 'passiert', 'ist', '.', '</s>']
2025-05-27 12:17:59,076 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 12:17:59,076 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 12:17:59,076 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich zeigt , ist eine Ver<unk> @ sion von dem , was es passiert passiert ist .
2025-05-27 12:18:42,798 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     1.243414, Batch Acc: 0.661206, Tokens per Sec:     3488, Lr: 0.000300
2025-05-27 12:19:24,098 - INFO - joeynmt.training - Epoch   3, Step:     7700, Batch Loss:     1.291669, Batch Acc: 0.666108, Tokens per Sec:     3652, Lr: 0.000300
2025-05-27 12:20:07,634 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     1.267857, Batch Acc: 0.662855, Tokens per Sec:     3592, Lr: 0.000300
2025-05-27 12:20:48,696 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     1.228070, Batch Acc: 0.665507, Tokens per Sec:     3703, Lr: 0.000300
2025-05-27 12:21:30,527 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     1.265857, Batch Acc: 0.663690, Tokens per Sec:     3681, Lr: 0.000300
2025-05-27 12:21:30,527 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 12:21:30,527 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 12:25:12,757 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.33, ppl:   3.78, acc:   0.64, generation: 222.2177[sec], evaluation: 0.0000[sec]
2025-05-27 12:25:12,760 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 12:25:12,888 - INFO - joeynmt.helpers - delete models/bpe_4k_model/5500.ckpt
2025-05-27 12:25:12,892 - INFO - joeynmt.training - Example #0
2025-05-27 12:25:12,893 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 12:25:12,893 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 12:25:12,893 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'zwei', 'F@@', '<unk>', '@', 'rü@@', '<unk>', '@', 'h@@', '<unk>', '@', 'nung', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'ik@@', '<unk>', '@', 'a', 'zeigen', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', 'in', 'den', 'letzten', 'drei', 'Jahren', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'von', 'Jahren', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'von', 'der', 'USA', 'hatte', 'etwa', 'der', 'Größ@@', '<unk>', '@', 'e', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 'n', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 't', 'hatte', '.', '</s>']
2025-05-27 12:25:12,893 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 12:25:12,893 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 12:25:12,893 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese zwei F<unk> @ rü<unk> @ h<unk> @ nung , dass die P<unk> @ ot<unk> @ ik<unk> @ a zeigen , die die letzten drei Millionen Jahren in den letzten drei Jahren , die die letzten drei Millionen von Jahren , die die letzten drei Millionen von der USA hatte etwa der Größ<unk> @ e der Verein<unk> @ igten Staaten , mit 40 Prozent ver<unk> @ wandel<unk> @ n der USA , mit 40 Prozent ver<unk> @ wandel<unk> @ t hatte .
2025-05-27 12:25:12,893 - INFO - joeynmt.training - Example #1
2025-05-27 12:25:12,893 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 12:25:12,893 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 12:25:12,893 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'tatsächlich', 'die', 'Sch@@', '<unk>', '@', 'atten', 'von', 'diesem', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ischen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Ver@@', '<unk>', '@', 'su@@', '<unk>', '@', 'ch', 'der', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 12:25:12,893 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 12:25:12,893 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 12:25:12,893 - INFO - joeynmt.training - 	Hypothesis: Aber das ist tatsächlich die Sch<unk> @ atten von diesem spezi<unk> @ f<unk> @ ischen Problem , weil es nicht die Ver<unk> @ su<unk> @ ch der E<unk> @ is zeigt .
2025-05-27 12:25:12,893 - INFO - joeynmt.training - Example #2
2025-05-27 12:25:12,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 12:25:12,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 12:25:12,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is', 'ist', 'der', 'N@@', '<unk>', '@', 'etz', ',', 'der', 'N@@', '<unk>', '@', 'oor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ool', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', ',', 'dass', 'es', 'ein', 'kl@@', '<unk>', '@', 'eines', 'Kl@@', '<unk>', '@', 'im@@', '<unk>', '@', 'ation@@', '<unk>', '@', 's@@', '<unk>', '@', 'ssy@@', '<unk>', '@', 'stem', 'ist', '.', '</s>']
2025-05-27 12:25:12,894 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 12:25:12,894 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 12:25:12,894 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is ist der N<unk> @ etz , der N<unk> @ oor<unk> @ d<unk> @ p<unk> @ ool ist in gew<unk> @ is<unk> @ ser Weise , dass es ein kl<unk> @ eines Kl<unk> @ im<unk> @ ation<unk> @ s<unk> @ ssy<unk> @ stem ist .
2025-05-27 12:25:12,894 - INFO - joeynmt.training - Example #3
2025-05-27 12:25:12,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 12:25:12,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 12:25:12,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'aus', 'der', 'W@@', '<unk>', '@', 'ür@@', '<unk>', '@', 'de', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'ft', 'in', 'der', 'S@@', '<unk>', '@', 'om@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 12:25:12,894 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 12:25:12,894 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 12:25:12,894 - INFO - joeynmt.training - 	Hypothesis: Es ist aus der W<unk> @ ür<unk> @ de und K<unk> @ ri<unk> @ m<unk> @ ft in der S<unk> @ om<unk> @ er .
2025-05-27 12:25:12,894 - INFO - joeynmt.training - Example #4
2025-05-27 12:25:12,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 12:25:12,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 12:25:12,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'die', 'nächste', 'F@@', '<unk>', '@', 'ern@@', '<unk>', '@', 'seh@@', '<unk>', '@', 'e', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'habe', ',', 'was', 'es', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'passiert', '.', '</s>']
2025-05-27 12:25:12,895 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 12:25:12,895 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 12:25:12,895 - INFO - joeynmt.training - 	Hypothesis: Und die nächste F<unk> @ ern<unk> @ seh<unk> @ e , die ich Ihnen zeigen habe , was es in den letzten 25 Jahren passiert passiert .
2025-05-27 12:25:54,434 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     1.146922, Batch Acc: 0.665648, Tokens per Sec:     3635, Lr: 0.000300
2025-05-27 12:26:39,766 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     1.183043, Batch Acc: 0.662479, Tokens per Sec:     3478, Lr: 0.000300
2025-05-27 12:27:14,184 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     1.262774, Batch Acc: 0.667548, Tokens per Sec:     4340, Lr: 0.000300
2025-05-27 12:27:53,391 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     1.353173, Batch Acc: 0.664702, Tokens per Sec:     3899, Lr: 0.000300
2025-05-27 12:28:35,401 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     1.162842, Batch Acc: 0.670123, Tokens per Sec:     3673, Lr: 0.000300
2025-05-27 12:28:35,401 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 12:28:35,402 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 12:32:47,052 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.73, acc:   0.65, generation: 251.6374[sec], evaluation: 0.0000[sec]
2025-05-27 12:32:47,056 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 12:32:47,174 - INFO - joeynmt.helpers - delete models/bpe_4k_model/6000.ckpt
2025-05-27 12:32:47,176 - INFO - joeynmt.training - Example #0
2025-05-27 12:32:47,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 12:32:47,176 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 12:32:47,176 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'beiden', 'F@@', '<unk>', '@', 'rü@@', '<unk>', '@', 'h@@', '<unk>', '@', 'nung', ',', 'die', 'ich', 'mit', 'der', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'h@@', '<unk>', '@', 'ä@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ern', ',', 'die', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'vor', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'vor', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'Groß@@', '<unk>', '@', 'br@@', '<unk>', '@', 'it@@', '<unk>', '@', 'an@@', '<unk>', '@', 'ischen', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 't', '.', '</s>']
2025-05-27 12:32:47,176 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 12:32:47,176 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 12:32:47,176 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese beiden F<unk> @ rü<unk> @ h<unk> @ nung , die ich mit der P<unk> @ op<unk> @ h<unk> @ ä<unk> @ f<unk> @ ern , die die die letzten drei Millionen Jahre vor , die die letzten drei Millionen Jahre vor 40 Prozent der USA , mit 40 Prozent der Groß<unk> @ br<unk> @ it<unk> @ an<unk> @ ischen , mit 40 Prozent ver<unk> @ wandel<unk> @ t .
2025-05-27 12:32:47,176 - INFO - joeynmt.training - Example #1
2025-05-27 12:32:47,177 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 12:32:47,177 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 12:32:47,177 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'der', 'Sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'z@@', '<unk>', '@', 'ung', ',', 'dass', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 12:32:47,177 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 12:32:47,177 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 12:32:47,177 - INFO - joeynmt.training - 	Hypothesis: Aber das ist der Sch<unk> @ ät<unk> @ z<unk> @ ung , dass es nicht die Di<unk> @ kte des E<unk> @ is des E<unk> @ is des E<unk> @ is zeigt .
2025-05-27 12:32:47,177 - INFO - joeynmt.training - Example #2
2025-05-27 12:32:47,177 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 12:32:47,177 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 12:32:47,177 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is', 'auf', 'der', 'N@@', '<unk>', '@', 'etz', 'ist', ',', 'in', 'der', 'N@@', '<unk>', '@', 'or@@', '<unk>', '@', 'd@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ing', 'das', 'G@@', '<unk>', '@', 'lau@@', '<unk>', '@', 'ben', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wan@@', '<unk>', '@', 'del', '.', '</s>']
2025-05-27 12:32:47,177 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 12:32:47,177 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 12:32:47,177 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is auf der N<unk> @ etz ist , in der N<unk> @ or<unk> @ d<unk> @ f<unk> @ ing das G<unk> @ lau<unk> @ ben des glob<unk> @ alen Klima<unk> @ wan<unk> @ del .
2025-05-27 12:32:47,177 - INFO - joeynmt.training - Example #3
2025-05-27 12:32:47,177 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 12:32:47,177 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 12:32:47,177 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'ann', '.', '</s>']
2025-05-27 12:32:47,178 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 12:32:47,178 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 12:32:47,178 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und K<unk> @ ri<unk> @ m<unk> @ ann .
2025-05-27 12:32:47,178 - INFO - joeynmt.training - Example #4
2025-05-27 12:32:47,178 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 12:32:47,178 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 12:32:47,178 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'ern@@', '<unk>', '@', 'seh@@', '<unk>', '@', 'e', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'was', 'die', 'letzten', '25', 'Jahre', 'passiert', 'ist', '.', '</s>']
2025-05-27 12:32:47,178 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 12:32:47,178 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 12:32:47,178 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ ern<unk> @ seh<unk> @ e , die ich Ihnen zeigen werde , was die letzten 25 Jahre passiert ist .
2025-05-27 12:33:27,820 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     1.207269, Batch Acc: 0.667904, Tokens per Sec:     3701, Lr: 0.000300
2025-05-27 12:34:13,966 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     1.177748, Batch Acc: 0.670228, Tokens per Sec:     3400, Lr: 0.000300
2025-05-27 12:35:00,471 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     1.296616, Batch Acc: 0.668000, Tokens per Sec:     3382, Lr: 0.000300
2025-05-27 12:35:43,623 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     1.183914, Batch Acc: 0.668610, Tokens per Sec:     3530, Lr: 0.000300
2025-05-27 12:36:26,977 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     1.247633, Batch Acc: 0.668596, Tokens per Sec:     3571, Lr: 0.000300
2025-05-27 12:36:26,980 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 12:36:26,980 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 12:40:10,156 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.30, ppl:   3.68, acc:   0.65, generation: 223.1640[sec], evaluation: 0.0000[sec]
2025-05-27 12:40:10,164 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 12:40:10,278 - INFO - joeynmt.helpers - delete models/bpe_4k_model/6500.ckpt
2025-05-27 12:40:10,284 - INFO - joeynmt.training - Example #0
2025-05-27 12:40:10,284 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 12:40:10,284 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 12:40:10,284 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'so', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'wen@@', '<unk>', '@', 'dung', 'von', 'zwei', 'F@@', '<unk>', '@', 'ern@@', '<unk>', '@', 'sehen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', '-@@', '<unk>', '@', 'L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 't@@', '<unk>', '@', 'au@@', '<unk>', '@', 'chen', ',', 'die', 'die', 'die', 'drei', 'Millionen', 'Jahren', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'K@@', '<unk>', '@', 'rei@@', '<unk>', '@', 'b@@', '<unk>', '@', 's@@', '<unk>', '@', 'ro@@', '<unk>', '@', 'm@@', '<unk>', '@', 'fen', '.', '</s>']
2025-05-27 12:40:10,284 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 12:40:10,284 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 12:40:10,284 - INFO - joeynmt.training - 	Hypothesis: Und so , dass die P<unk> @ ot<unk> @ wen<unk> @ dung von zwei F<unk> @ ern<unk> @ sehen , um zu zeigen , dass die P<unk> @ oo<unk> @ l<unk> @ -<unk> @ L<unk> @ etz<unk> @ t<unk> @ au<unk> @ chen , die die die drei Millionen Jahren der USA , mit 40 Prozent der USA , mit 40 Prozent der USA , mit 40 Prozent der USA , mit 40 Prozent der K<unk> @ rei<unk> @ b<unk> @ s<unk> @ ro<unk> @ m<unk> @ fen .
2025-05-27 12:40:10,284 - INFO - joeynmt.training - Example #1
2025-05-27 12:40:10,285 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 12:40:10,285 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 12:40:10,285 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'das', 'The@@', '<unk>', '@', 'ma', ',', 'die', 'die', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', '.', '</s>']
2025-05-27 12:40:10,285 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 12:40:10,285 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 12:40:10,285 - INFO - joeynmt.training - 	Hypothesis: Aber das ist das The<unk> @ ma , die die E<unk> @ is des E<unk> @ is des E<unk> @ is des E<unk> @ is des E<unk> @ is des E<unk> @ is des E<unk> @ is .
2025-05-27 12:40:10,285 - INFO - joeynmt.training - Example #2
2025-05-27 12:40:10,285 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 12:40:10,285 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 12:40:10,285 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'z', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'S@@', '<unk>', '@', 'to@@', '<unk>', '@', 'ff@@', '<unk>', '@', 'ent@@', '<unk>', '@', 's', '.', '</s>']
2025-05-27 12:40:10,285 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 12:40:10,285 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 12:40:10,285 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ p<unk> @ ol<unk> @ z ist in gew<unk> @ is<unk> @ se S<unk> @ to<unk> @ ff<unk> @ ent<unk> @ s .
2025-05-27 12:40:10,285 - INFO - joeynmt.training - Example #3
2025-05-27 12:40:10,285 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 12:40:10,285 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 12:40:10,285 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'mer@@', '<unk>', '@', 'n', '.', '</s>']
2025-05-27 12:40:10,286 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 12:40:10,286 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 12:40:10,286 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und K<unk> @ ri<unk> @ m<unk> @ mer<unk> @ n .
2025-05-27 12:40:10,286 - INFO - joeynmt.training - Example #4
2025-05-27 12:40:10,286 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 12:40:10,286 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 12:40:10,286 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'Ihnen', 'zeigen', 'werde', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'die', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2025-05-27 12:40:10,286 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 12:40:10,286 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 12:40:10,286 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen Ihnen zeigen werde , ist eine Ver<unk> @ sion von dem , was die letzten 25 Jahren passiert .
2025-05-27 12:40:52,719 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     1.155394, Batch Acc: 0.668881, Tokens per Sec:     3619, Lr: 0.000300
2025-05-27 12:41:33,593 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     1.154674, Batch Acc: 0.674465, Tokens per Sec:     3742, Lr: 0.000300
2025-05-27 12:42:17,538 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     1.258382, Batch Acc: 0.668706, Tokens per Sec:     3579, Lr: 0.000300
2025-05-27 12:42:46,770 - INFO - joeynmt.training - Epoch   3: total training loss 3901.31
2025-05-27 12:42:46,770 - INFO - joeynmt.training - EPOCH 4
2025-05-27 12:42:57,422 - INFO - joeynmt.training - Epoch   4, Step:     9400, Batch Loss:     1.105573, Batch Acc: 0.684256, Tokens per Sec:     3590, Lr: 0.000300
2025-05-27 12:43:38,034 - INFO - joeynmt.training - Epoch   4, Step:     9500, Batch Loss:     1.166983, Batch Acc: 0.683432, Tokens per Sec:     3776, Lr: 0.000300
2025-05-27 12:43:38,034 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 12:43:38,034 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 12:47:43,479 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.29, ppl:   3.65, acc:   0.65, generation: 245.4327[sec], evaluation: 0.0000[sec]
2025-05-27 12:47:43,487 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 12:47:43,605 - INFO - joeynmt.helpers - delete models/bpe_4k_model/7000.ckpt
2025-05-27 12:47:43,611 - INFO - joeynmt.training - Example #0
2025-05-27 12:47:43,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 12:47:43,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 12:47:43,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'beiden', 'Di@@', '<unk>', '@', 'a@@', '<unk>', '@', 'gram@@', '<unk>', '@', 'me', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'h@@', '<unk>', '@', 'ä@@', '<unk>', '@', 'ul@@', '<unk>', '@', 'e', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', 'der', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'i@@', '<unk>', '@', 'versu@@', '<unk>', '@', 'ms', ',', 'die', 'die', 'drei', 'Millionen', 'von', 'Jahren', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'K@@', '<unk>', '@', 'i@@', '<unk>', '@', 'pf@@', '<unk>', '@', 'eh@@', '<unk>', '@', 'men', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'stän@@', '<unk>', '@', 'dlich', 'war', '.', '</s>']
2025-05-27 12:47:43,612 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 12:47:43,612 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 12:47:43,612 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese beiden Di<unk> @ a<unk> @ gram<unk> @ me zeigen , dass die P<unk> @ op<unk> @ h<unk> @ ä<unk> @ ul<unk> @ e , die die letzten drei Millionen Jahren der Größ<unk> @ e des V<unk> @ ar<unk> @ i<unk> @ versu<unk> @ ms , die die drei Millionen von Jahren der USA , mit 40 Prozent der K<unk> @ i<unk> @ pf<unk> @ eh<unk> @ men , mit 40 Prozent ver<unk> @ stän<unk> @ dlich war .
2025-05-27 12:47:43,612 - INFO - joeynmt.training - Example #1
2025-05-27 12:47:43,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 12:47:43,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 12:47:43,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zen', 'tatsächlich', 'die', 'E@@', '<unk>', '@', 'is', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ischen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 12:47:43,612 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 12:47:43,612 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 12:47:43,612 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zen tatsächlich die E<unk> @ is dieses spezi<unk> @ f<unk> @ ischen Problem , weil es nicht die Di<unk> @ kte des E<unk> @ is zeigt .
2025-05-27 12:47:43,612 - INFO - joeynmt.training - Example #2
2025-05-27 12:47:43,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 12:47:43,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 12:47:43,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'le', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'S@@', '<unk>', '@', 'icht', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 12:47:43,613 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 12:47:43,613 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 12:47:43,613 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ p<unk> @ ol<unk> @ le ist in gew<unk> @ is<unk> @ ser S<unk> @ icht des glob<unk> @ alen Klima<unk> @ sy<unk> @ stem .
2025-05-27 12:47:43,613 - INFO - joeynmt.training - Example #3
2025-05-27 12:47:43,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 12:47:43,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 12:47:43,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'ft', 'in', 'der', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 12:47:43,613 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 12:47:43,613 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 12:47:43,613 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und K<unk> @ ri<unk> @ m<unk> @ ft in der S<unk> @ omm<unk> @ er .
2025-05-27 12:47:43,613 - INFO - joeynmt.training - Example #4
2025-05-27 12:47:43,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 12:47:43,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 12:47:43,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', '<unk>', '@', 'a@@', '<unk>', '@', 'gram@@', '<unk>', '@', 'm', ',', 'die', 'ich', 'zei@@', '<unk>', '@', 'ge', ',', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 12:47:43,614 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 12:47:43,614 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 12:47:43,614 - INFO - joeynmt.training - 	Hypothesis: Die nächste Di<unk> @ a<unk> @ gram<unk> @ m , die ich zei<unk> @ ge , die letzten 25 Jahren passiert ist .
2025-05-27 12:48:26,198 - INFO - joeynmt.training - Epoch   4, Step:     9600, Batch Loss:     1.100276, Batch Acc: 0.684487, Tokens per Sec:     3636, Lr: 0.000300
2025-05-27 12:49:08,456 - INFO - joeynmt.training - Epoch   4, Step:     9700, Batch Loss:     1.125837, Batch Acc: 0.685584, Tokens per Sec:     3649, Lr: 0.000300
2025-05-27 12:49:54,311 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     1.101325, Batch Acc: 0.681192, Tokens per Sec:     3371, Lr: 0.000300
2025-05-27 12:50:36,457 - INFO - joeynmt.training - Epoch   4, Step:     9900, Batch Loss:     1.257480, Batch Acc: 0.682409, Tokens per Sec:     3639, Lr: 0.000300
2025-05-27 12:51:21,130 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     1.130944, Batch Acc: 0.682052, Tokens per Sec:     3501, Lr: 0.000300
2025-05-27 12:51:21,131 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 12:51:21,131 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 12:55:17,931 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.29, ppl:   3.63, acc:   0.65, generation: 236.7875[sec], evaluation: 0.0000[sec]
2025-05-27 12:55:17,937 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 12:55:18,053 - INFO - joeynmt.helpers - delete models/bpe_4k_model/7500.ckpt
2025-05-27 12:55:18,058 - INFO - joeynmt.training - Example #0
2025-05-27 12:55:18,058 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 12:55:18,058 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 12:55:18,058 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'diese', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'ie', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'at@@', '<unk>', '@', 'z@@', '<unk>', '@', 'ent@@', '<unk>', '@', 's', ',', 'die', 'die', 'drei', 'Millionen', 'Jahren', 'vor', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'el@@', '<unk>', '@', 's@@', '<unk>', '@', 'au@@', '<unk>', '@', 'm@@', '<unk>', '@', 'eln', 'war', '.', '</s>']
2025-05-27 12:55:18,058 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 12:55:18,058 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 12:55:18,058 - INFO - joeynmt.training - 	Hypothesis: Und ich diese F<unk> @ olie , die zwei F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ oo<unk> @ l<unk> @ lin<unk> @ ie , die die letzten drei Millionen Jahren etwa die Größ<unk> @ e des V<unk> @ at<unk> @ z<unk> @ ent<unk> @ s , die die drei Millionen Jahren vor 40 Prozent der F<unk> @ el<unk> @ s<unk> @ au<unk> @ m<unk> @ eln war .
2025-05-27 12:55:18,058 - INFO - joeynmt.training - Example #1
2025-05-27 12:55:18,058 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 12:55:18,058 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 12:55:18,058 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zen', 'eigentlich', 'die', 'Art', 'von', 'dieser', 'spezi@@', '<unk>', '@', 'ellen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 12:55:18,059 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 12:55:18,059 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 12:55:18,059 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zen eigentlich die Art von dieser spezi<unk> @ ellen Problem , weil es nicht die E<unk> @ is des E<unk> @ is des E<unk> @ is des E<unk> @ is zeigt .
2025-05-27 12:55:18,059 - INFO - joeynmt.training - Example #2
2025-05-27 12:55:18,059 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 12:55:18,059 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 12:55:18,059 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ool', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', ',', 'die', 'K@@', '<unk>', '@', 'letter@@', '<unk>', '@', 'n@@', '<unk>', '@', 'utz@@', '<unk>', '@', 'ung', 'des', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', 's', 'G@@', '<unk>', '@', 'eld@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 12:55:18,059 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 12:55:18,059 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 12:55:18,059 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ p<unk> @ ool ist in gew<unk> @ is<unk> @ ser Weise , die K<unk> @ letter<unk> @ n<unk> @ utz<unk> @ ung des G<unk> @ lo<unk> @ b<unk> @ s G<unk> @ eld<unk> @ sy<unk> @ stem .
2025-05-27 12:55:18,059 - INFO - joeynmt.training - Example #3
2025-05-27 12:55:18,059 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 12:55:18,059 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 12:55:18,059 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'mer', ',', 'und', 'es', 'ist', 'die', 'S@@', '<unk>', '@', 'om@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 12:55:18,059 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 12:55:18,059 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 12:55:18,060 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und K<unk> @ ri<unk> @ m<unk> @ mer , und es ist die S<unk> @ om<unk> @ er .
2025-05-27 12:55:18,060 - INFO - joeynmt.training - Example #4
2025-05-27 12:55:18,060 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 12:55:18,060 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 12:55:18,060 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'zei@@', '<unk>', '@', 'ge', ',', 'die', 'ich', 'zei@@', '<unk>', '@', 'ge', ',', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 12:55:18,060 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 12:55:18,060 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 12:55:18,060 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich zei<unk> @ ge , die ich zei<unk> @ ge , die letzten 25 Jahren passiert ist .
2025-05-27 12:56:01,425 - INFO - joeynmt.training - Epoch   4, Step:    10100, Batch Loss:     1.261878, Batch Acc: 0.678270, Tokens per Sec:     3550, Lr: 0.000300
2025-05-27 12:56:44,929 - INFO - joeynmt.training - Epoch   4, Step:    10200, Batch Loss:     1.160165, Batch Acc: 0.680293, Tokens per Sec:     3504, Lr: 0.000300
2025-05-27 12:57:27,747 - INFO - joeynmt.training - Epoch   4, Step:    10300, Batch Loss:     1.233868, Batch Acc: 0.679295, Tokens per Sec:     3539, Lr: 0.000300
2025-05-27 12:58:10,852 - INFO - joeynmt.training - Epoch   4, Step:    10400, Batch Loss:     1.109210, Batch Acc: 0.681132, Tokens per Sec:     3487, Lr: 0.000300
2025-05-27 12:58:51,071 - INFO - joeynmt.training - Epoch   4, Step:    10500, Batch Loss:     1.175798, Batch Acc: 0.683739, Tokens per Sec:     3800, Lr: 0.000300
2025-05-27 12:58:51,072 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 12:58:51,072 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 13:01:59,719 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.28, ppl:   3.61, acc:   0.65, generation: 188.6345[sec], evaluation: 0.0000[sec]
2025-05-27 13:01:59,723 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 13:01:59,833 - INFO - joeynmt.helpers - delete models/bpe_4k_model/8000.ckpt
2025-05-27 13:01:59,837 - INFO - joeynmt.training - Example #0
2025-05-27 13:01:59,838 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 13:01:59,838 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 13:01:59,838 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'zwei', 'Di@@', '<unk>', '@', 'as', 'ge@@', '<unk>', '@', 'zeigt', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'h@@', '<unk>', '@', 'ä@@', '<unk>', '@', 'ten', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'von', 'Jahren', 'von', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'der', 'USA', 'hatte', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'ungefähr', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'ün@@', '<unk>', '@', 'der', '.', '</s>']
2025-05-27 13:01:59,838 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 13:01:59,838 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 13:01:59,838 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese zwei Di<unk> @ as ge<unk> @ zeigt , dass die P<unk> @ op<unk> @ h<unk> @ ä<unk> @ ten , die die letzten drei Millionen Jahre von Jahren von etwa die Größ<unk> @ e der USA hatte , die die letzten drei Millionen Jahre ungefähr etwa die Größ<unk> @ e der USA , mit 40 Prozent der USA , mit 40 Prozent der USA , mit 40 Prozent der USA , mit 40 Prozent der F<unk> @ ün<unk> @ der .
2025-05-27 13:01:59,838 - INFO - joeynmt.training - Example #1
2025-05-27 13:01:59,838 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 13:01:59,838 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 13:01:59,838 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ische', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 13:01:59,838 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 13:01:59,838 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 13:01:59,838 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das spezi<unk> @ f<unk> @ ische Problem , weil es nicht die Di<unk> @ kte des E<unk> @ is des E<unk> @ is des E<unk> @ is zeigt .
2025-05-27 13:01:59,838 - INFO - joeynmt.training - Example #2
2025-05-27 13:01:59,839 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 13:01:59,839 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 13:01:59,839 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'e', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'Weise', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 13:01:59,839 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 13:01:59,839 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 13:01:59,839 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ e E<unk> @ is<unk> @ e auf der Nor<unk> @ d<unk> @ p<unk> @ ol<unk> @ e ist in gew<unk> @ is<unk> @ se Weise des glob<unk> @ alen Klima<unk> @ sy<unk> @ stem .
2025-05-27 13:01:59,839 - INFO - joeynmt.training - Example #3
2025-05-27 13:01:59,839 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 13:01:59,839 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 13:01:59,839 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'aus', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 13:01:59,839 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 13:01:59,839 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 13:01:59,839 - INFO - joeynmt.training - 	Hypothesis: Es ist aus der Win<unk> @ ter und K<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 13:01:59,839 - INFO - joeynmt.training - Example #4
2025-05-27 13:01:59,839 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 13:01:59,839 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 13:01:59,839 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', '<unk>', '@', 'ät', ',', 'die', 'ich', 'Ihnen', 'Ihnen', 'zeigen', 'werde', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'passiert', '.', '</s>']
2025-05-27 13:01:59,840 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 13:01:59,840 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 13:01:59,840 - INFO - joeynmt.training - 	Hypothesis: Die nächste Di<unk> @ ät , die ich Ihnen Ihnen zeigen werde , ist eine Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert passiert .
2025-05-27 13:02:44,905 - INFO - joeynmt.training - Epoch   4, Step:    10600, Batch Loss:     1.199354, Batch Acc: 0.682008, Tokens per Sec:     3495, Lr: 0.000300
2025-05-27 13:03:27,084 - INFO - joeynmt.training - Epoch   4, Step:    10700, Batch Loss:     1.055904, Batch Acc: 0.683888, Tokens per Sec:     3576, Lr: 0.000300
2025-05-27 13:04:07,507 - INFO - joeynmt.training - Epoch   4, Step:    10800, Batch Loss:     1.156675, Batch Acc: 0.683110, Tokens per Sec:     3702, Lr: 0.000300
2025-05-27 13:04:50,588 - INFO - joeynmt.training - Epoch   4, Step:    10900, Batch Loss:     1.202513, Batch Acc: 0.681743, Tokens per Sec:     3663, Lr: 0.000300
2025-05-27 13:05:34,886 - INFO - joeynmt.training - Epoch   4, Step:    11000, Batch Loss:     1.109986, Batch Acc: 0.685538, Tokens per Sec:     3427, Lr: 0.000300
2025-05-27 13:05:34,887 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 13:05:34,887 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 13:09:38,378 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.27, ppl:   3.57, acc:   0.66, generation: 243.4779[sec], evaluation: 0.0000[sec]
2025-05-27 13:09:38,385 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 13:09:38,506 - INFO - joeynmt.helpers - delete models/bpe_4k_model/8500.ckpt
2025-05-27 13:09:38,511 - INFO - joeynmt.training - Example #0
2025-05-27 13:09:38,511 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 13:09:38,511 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 13:09:38,511 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'en@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', ',', 'die', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'etwa', 'der', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'f', 'von', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'Größ@@', '<unk>', '@', 'e', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'fol@@', '<unk>', '@', 'gt', 'hatte', '.', '</s>']
2025-05-27 13:09:38,511 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 13:09:38,511 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 13:09:38,511 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese beiden F<unk> @ olie zeigen , dass die P<unk> @ ot<unk> @ en<unk> @ sk<unk> @ a<unk> @ p , die die die letzten drei Millionen Jahre , die die letzten drei Millionen Jahre etwa der Größ<unk> @ e des V<unk> @ öl<unk> @ f von 40 Prozent der USA , mit 40 Prozent der Größ<unk> @ e der USA , mit 40 Prozent ver<unk> @ fol<unk> @ gt hatte .
2025-05-27 13:09:38,511 - INFO - joeynmt.training - Example #1
2025-05-27 13:09:38,511 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 13:09:38,511 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 13:09:38,511 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'tatsächlich', ',', 'dass', 'es', 'wirklich', 'ern@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ha@@', '<unk>', '@', 'ft', ',', 'weil', 'es', 'nicht', 'die', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 13:09:38,512 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 13:09:38,512 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 13:09:38,512 - INFO - joeynmt.training - 	Hypothesis: Aber das ist tatsächlich , dass es wirklich ern<unk> @ st<unk> @ ha<unk> @ ft , weil es nicht die E<unk> @ is des E<unk> @ is des E<unk> @ is zeigt .
2025-05-27 13:09:38,512 - INFO - joeynmt.training - Example #2
2025-05-27 13:09:38,512 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 13:09:38,512 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 13:09:38,512 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ool', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', ',', 'das', 'ist', 'der', 'glob@@', '<unk>', '@', 'ale', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 13:09:38,512 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 13:09:38,512 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 13:09:38,512 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ p<unk> @ ool ist in gew<unk> @ is<unk> @ ser Weise , das ist der glob<unk> @ ale Klima<unk> @ sy<unk> @ stem .
2025-05-27 13:09:38,512 - INFO - joeynmt.training - Example #3
2025-05-27 13:09:38,512 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 13:09:38,512 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 13:09:38,512 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'elt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 13:09:38,512 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 13:09:38,512 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 13:09:38,512 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ elt in den S<unk> @ omm<unk> @ er .
2025-05-27 13:09:38,512 - INFO - joeynmt.training - Example #4
2025-05-27 13:09:38,513 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 13:09:38,513 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 13:09:38,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zeigt', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', '25', 'Jahren', 'passiert', 'ist', ',', '</s>']
2025-05-27 13:09:38,513 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 13:09:38,513 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 13:09:38,513 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zeigt , ist eine Ver<unk> @ sion von dem , was in 25 Jahren passiert ist ,
2025-05-27 13:10:20,497 - INFO - joeynmt.training - Epoch   4, Step:    11100, Batch Loss:     1.183498, Batch Acc: 0.681277, Tokens per Sec:     3680, Lr: 0.000300
2025-05-27 13:11:02,671 - INFO - joeynmt.training - Epoch   4, Step:    11200, Batch Loss:     1.189277, Batch Acc: 0.683362, Tokens per Sec:     3663, Lr: 0.000300
2025-05-27 13:11:43,949 - INFO - joeynmt.training - Epoch   4, Step:    11300, Batch Loss:     1.083632, Batch Acc: 0.684106, Tokens per Sec:     3675, Lr: 0.000300
2025-05-27 13:12:26,538 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     1.120567, Batch Acc: 0.686236, Tokens per Sec:     3591, Lr: 0.000300
2025-05-27 13:13:10,039 - INFO - joeynmt.training - Epoch   4, Step:    11500, Batch Loss:     1.105460, Batch Acc: 0.684338, Tokens per Sec:     3502, Lr: 0.000300
2025-05-27 13:13:10,040 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 13:13:10,040 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 13:17:31,671 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.27, ppl:   3.55, acc:   0.66, generation: 261.6182[sec], evaluation: 0.0000[sec]
2025-05-27 13:17:31,677 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 13:17:31,808 - INFO - joeynmt.helpers - delete models/bpe_4k_model/9000.ckpt
2025-05-27 13:17:31,814 - INFO - joeynmt.training - Example #0
2025-05-27 13:17:31,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 13:17:31,814 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 13:17:31,814 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'die', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'f', ',', 'die', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', '-@@', '<unk>', '@', 'E@@', '<unk>', '@', 'ben@@', '<unk>', '@', 'e', 'der', 'USA', 'war', ',', 'die', 'größ@@', '<unk>', '@', 'te', 'drei', 'Millionen', 'von', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'K@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ö@@', '<unk>', '@', 'p@@', '<unk>', '@', 'fe', 'war', '.', '</s>']
2025-05-27 13:17:31,814 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 13:17:31,814 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 13:17:31,814 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese beiden F<unk> @ olie zeigen , die zwei F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ op<unk> @ f , die die P<unk> @ oo<unk> @ l<unk> @ -<unk> @ E<unk> @ ben<unk> @ e der USA war , die größ<unk> @ te drei Millionen von der USA , mit 40 Prozent der USA , mit 40 Prozent der USA , mit 40 Prozent der USA , mit 40 Prozent der K<unk> @ r<unk> @ ö<unk> @ p<unk> @ fe war .
2025-05-27 13:17:31,814 - INFO - joeynmt.training - Example #1
2025-05-27 13:17:31,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 13:17:31,814 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 13:17:31,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'tatsächlich', 'die', 'Sch@@', '<unk>', '@', 'atten', 'des', 'spezi@@', '<unk>', '@', 'ellen', 'Problem', 'der', 'spezi@@', '<unk>', '@', 'ellen', 'Sache', ',', 'weil', 'es', 'nicht', 'die', 'D@@', '<unk>', '@', 'ing', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 13:17:31,815 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 13:17:31,815 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 13:17:31,815 - INFO - joeynmt.training - 	Hypothesis: Aber das ist tatsächlich die Sch<unk> @ atten des spezi<unk> @ ellen Problem der spezi<unk> @ ellen Sache , weil es nicht die D<unk> @ ing des E<unk> @ is des E<unk> @ is zeigt .
2025-05-27 13:17:31,815 - INFO - joeynmt.training - Example #2
2025-05-27 13:17:31,815 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 13:17:31,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 13:17:31,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'auf', 'den', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'f@@', '<unk>', '@', 'lü@@', '<unk>', '@', 'ss@@', '<unk>', '@', 'ige', 'Weise', 'ist', ',', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'Weise', ',', 'die', 'die', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', '-@@', '<unk>', '@', 'System', 'der', 'glob@@', '<unk>', '@', 'al', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 13:17:31,815 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 13:17:31,815 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 13:17:31,815 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ e auf den Nor<unk> @ d<unk> @ f<unk> @ lü<unk> @ ss<unk> @ ige Weise ist , in gew<unk> @ is<unk> @ se Weise , die die G<unk> @ lo<unk> @ b<unk> @ -<unk> @ System der glob<unk> @ al Klima<unk> @ sy<unk> @ stem .
2025-05-27 13:17:31,815 - INFO - joeynmt.training - Example #3
2025-05-27 13:17:31,815 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 13:17:31,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 13:17:31,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'aus', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'elt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 13:17:31,815 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 13:17:31,816 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 13:17:31,816 - INFO - joeynmt.training - 	Hypothesis: Es ist aus der Win<unk> @ ter und K<unk> @ ri<unk> @ m<unk> @ elt in den S<unk> @ omm<unk> @ er .
2025-05-27 13:17:31,816 - INFO - joeynmt.training - Example #4
2025-05-27 13:17:31,816 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 13:17:31,816 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 13:17:31,816 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 13:17:31,816 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 13:17:31,816 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 13:17:31,816 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zeigen werde , ist eine Ver<unk> @ sion von dem , was die letzten 25 Jahren passiert ist .
2025-05-27 13:18:14,305 - INFO - joeynmt.training - Epoch   4, Step:    11600, Batch Loss:     1.176257, Batch Acc: 0.684694, Tokens per Sec:     3595, Lr: 0.000300
2025-05-27 13:18:56,873 - INFO - joeynmt.training - Epoch   4, Step:    11700, Batch Loss:     1.170153, Batch Acc: 0.684354, Tokens per Sec:     3588, Lr: 0.000300
2025-05-27 13:19:40,077 - INFO - joeynmt.training - Epoch   4, Step:    11800, Batch Loss:     1.158120, Batch Acc: 0.687477, Tokens per Sec:     3530, Lr: 0.000300
2025-05-27 13:20:25,275 - INFO - joeynmt.training - Epoch   4, Step:    11900, Batch Loss:     1.125055, Batch Acc: 0.687338, Tokens per Sec:     3425, Lr: 0.000300
2025-05-27 13:21:08,711 - INFO - joeynmt.training - Epoch   4, Step:    12000, Batch Loss:     1.090518, Batch Acc: 0.685303, Tokens per Sec:     3544, Lr: 0.000300
2025-05-27 13:21:08,712 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 13:21:08,712 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 13:24:33,061 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.51, acc:   0.66, generation: 204.3360[sec], evaluation: 0.0000[sec]
2025-05-27 13:24:33,064 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 13:24:33,192 - INFO - joeynmt.helpers - delete models/bpe_4k_model/9500.ckpt
2025-05-27 13:24:33,200 - INFO - joeynmt.training - Example #0
2025-05-27 13:24:33,200 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 13:24:33,200 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 13:24:33,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'en@@', '<unk>', '@', '-@@', '<unk>', '@', 'Jahr', 'sehen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'ie', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'der', 'USA', ',', 'die', 'Größ@@', '<unk>', '@', 'e', 'der', 'USA', ',', 'die', 'Größ@@', '<unk>', '@', 'e', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'el@@', '<unk>', '@', 's@@', '<unk>', '@', 'an@@', '<unk>', '@', 'z', 'der', 'USA', 'war', '.', '</s>']
2025-05-27 13:24:33,201 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 13:24:33,201 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 13:24:33,201 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese zwei F<unk> @ olie zeigen , dass die P<unk> @ ot<unk> @ en<unk> @ -<unk> @ Jahr sehen , dass die P<unk> @ oo<unk> @ lin<unk> @ ie , die die drei Millionen Jahre der USA , die Größ<unk> @ e der USA , die Größ<unk> @ e der USA , mit 40 Prozent der F<unk> @ el<unk> @ s<unk> @ an<unk> @ z der USA war .
2025-05-27 13:24:33,201 - INFO - joeynmt.training - Example #1
2025-05-27 13:24:33,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 13:24:33,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 13:24:33,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'Sch@@', '<unk>', '@', 'al@@', '<unk>', '@', 'ter', 'dieser', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ischen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 13:24:33,201 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 13:24:33,201 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 13:24:33,202 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die Sch<unk> @ al<unk> @ ter dieser spezi<unk> @ f<unk> @ ischen Problem , weil es nicht die E<unk> @ is des E<unk> @ is zeigt .
2025-05-27 13:24:33,202 - INFO - joeynmt.training - Example #2
2025-05-27 13:24:33,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 13:24:33,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 13:24:33,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'S@@', '<unk>', '@', 'icht', 'der', 'G@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'qu@@', '<unk>', '@', 'al@@', '<unk>', '@', '-@@', '<unk>', '@', 'System', '.', '</s>']
2025-05-27 13:24:33,202 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 13:24:33,202 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 13:24:33,202 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ se S<unk> @ icht der G<unk> @ ar<unk> @ qu<unk> @ al<unk> @ -<unk> @ System .
2025-05-27 13:24:33,202 - INFO - joeynmt.training - Example #3
2025-05-27 13:24:33,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 13:24:33,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 13:24:33,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'elt', '.', '</s>']
2025-05-27 13:24:33,202 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 13:24:33,202 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 13:24:33,202 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und K<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ elt .
2025-05-27 13:24:33,202 - INFO - joeynmt.training - Example #4
2025-05-27 13:24:33,203 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 13:24:33,203 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 13:24:33,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'die', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 13:24:33,203 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 13:24:33,203 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 13:24:33,203 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zeigen werde , ist eine Ver<unk> @ sion von dem , was die letzten 25 Jahren passiert ist .
2025-05-27 13:25:14,524 - INFO - joeynmt.training - Epoch   4, Step:    12100, Batch Loss:     1.087668, Batch Acc: 0.688169, Tokens per Sec:     3711, Lr: 0.000300
2025-05-27 13:25:57,233 - INFO - joeynmt.training - Epoch   4, Step:    12200, Batch Loss:     1.162432, Batch Acc: 0.679273, Tokens per Sec:     3561, Lr: 0.000300
2025-05-27 13:26:36,997 - INFO - joeynmt.training - Epoch   4, Step:    12300, Batch Loss:     1.175768, Batch Acc: 0.686732, Tokens per Sec:     3824, Lr: 0.000300
2025-05-27 13:27:20,803 - INFO - joeynmt.training - Epoch   4, Step:    12400, Batch Loss:     1.082319, Batch Acc: 0.685156, Tokens per Sec:     3483, Lr: 0.000300
2025-05-27 13:28:02,391 - INFO - joeynmt.training - Epoch   4, Step:    12500, Batch Loss:     1.228937, Batch Acc: 0.686385, Tokens per Sec:     3651, Lr: 0.000300
2025-05-27 13:28:02,391 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 13:28:02,391 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 13:31:13,232 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.50, acc:   0.66, generation: 190.8282[sec], evaluation: 0.0000[sec]
2025-05-27 13:31:13,235 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 13:31:13,369 - INFO - joeynmt.helpers - delete models/bpe_4k_model/10000.ckpt
2025-05-27 13:31:13,374 - INFO - joeynmt.training - Example #0
2025-05-27 13:31:13,374 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 13:31:13,375 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 13:31:13,375 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'so', ',', 'dass', 'ich', 'diese', 'beiden', 'Di@@', '<unk>', '@', 'as', 'sehen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'en@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ück', 'der', 'A@@', '<unk>', '@', 'ch@@', '<unk>', '@', 'se', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'von', 'ungefähr', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'ungefähr', 'die', 'Größ@@', '<unk>', '@', 'e', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'die', 'die', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', '40', 'Prozent', 'von', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'fol@@', '<unk>', '@', 'gen', '.', '</s>']
2025-05-27 13:31:13,375 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 13:31:13,375 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 13:31:13,375 - INFO - joeynmt.training - 	Hypothesis: Und so , dass ich diese beiden Di<unk> @ as sehen , dass die P<unk> @ ot<unk> @ en<unk> @ st<unk> @ ück der A<unk> @ ch<unk> @ se , die die letzten drei Millionen von ungefähr die letzten drei Millionen Jahre ungefähr die Größ<unk> @ e der Verein<unk> @ igten Staaten , die die Verein<unk> @ igten Staaten , mit 40 Prozent von der Verein<unk> @ igten Staaten , mit 40 Prozent ver<unk> @ fol<unk> @ gen .
2025-05-27 13:31:13,375 - INFO - joeynmt.training - Example #1
2025-05-27 13:31:13,375 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 13:31:13,375 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 13:31:13,375 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'tatsächlich', 'der', 'ern@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ha@@', '<unk>', '@', 'ft', 'der', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ischen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 13:31:13,375 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 13:31:13,375 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 13:31:13,375 - INFO - joeynmt.training - 	Hypothesis: Aber das ist tatsächlich der ern<unk> @ st<unk> @ ha<unk> @ ft der spezi<unk> @ f<unk> @ ischen Problem , weil es nicht die Di<unk> @ kte des E<unk> @ is zeigt .
2025-05-27 13:31:13,375 - INFO - joeynmt.training - Example #2
2025-05-27 13:31:13,375 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 13:31:13,375 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 13:31:13,375 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'S@@', '<unk>', '@', 'icht@@', '<unk>', '@', 'weise', 'der', 'glob@@', '<unk>', '@', 'al', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 13:31:13,376 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 13:31:13,376 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 13:31:13,376 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ k<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser S<unk> @ icht<unk> @ weise der glob<unk> @ al Klima<unk> @ sy<unk> @ stem .
2025-05-27 13:31:13,376 - INFO - joeynmt.training - Example #3
2025-05-27 13:31:13,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 13:31:13,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 13:31:13,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'elt', ',', 'und', 'es', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'iert', '.', '</s>']
2025-05-27 13:31:13,376 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 13:31:13,376 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 13:31:13,376 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ elt , und es k<unk> @ ri<unk> @ m<unk> @ iert .
2025-05-27 13:31:13,376 - INFO - joeynmt.training - Example #4
2025-05-27 13:31:13,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 13:31:13,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 13:31:13,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', '<unk>', '@', 'a', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'möchte', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'der', 'letzten', '25', 'Jahre', 'passiert', '.', '</s>']
2025-05-27 13:31:13,376 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 13:31:13,376 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 13:31:13,376 - INFO - joeynmt.training - 	Hypothesis: Die nächste Di<unk> @ a , die ich Ihnen zeigen möchte , ist eine Ver<unk> @ sion von der letzten 25 Jahre passiert .
2025-05-27 13:31:13,383 - INFO - joeynmt.training - Epoch   4: total training loss 3631.64
2025-05-27 13:31:13,383 - INFO - joeynmt.training - EPOCH 5
2025-05-27 13:31:55,176 - INFO - joeynmt.training - Epoch   5, Step:    12600, Batch Loss:     1.156786, Batch Acc: 0.698329, Tokens per Sec:     3616, Lr: 0.000300
2025-05-27 13:32:41,797 - INFO - joeynmt.training - Epoch   5, Step:    12700, Batch Loss:     1.015593, Batch Acc: 0.696961, Tokens per Sec:     3355, Lr: 0.000300
2025-05-27 13:33:23,100 - INFO - joeynmt.training - Epoch   5, Step:    12800, Batch Loss:     1.079066, Batch Acc: 0.695663, Tokens per Sec:     3675, Lr: 0.000300
2025-05-27 13:34:02,780 - INFO - joeynmt.training - Epoch   5, Step:    12900, Batch Loss:     1.200910, Batch Acc: 0.700219, Tokens per Sec:     3851, Lr: 0.000300
2025-05-27 13:34:48,554 - INFO - joeynmt.training - Epoch   5, Step:    13000, Batch Loss:     1.025280, Batch Acc: 0.698642, Tokens per Sec:     3438, Lr: 0.000300
2025-05-27 13:34:48,555 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 13:34:48,555 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 13:38:16,854 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.49, acc:   0.66, generation: 208.2840[sec], evaluation: 0.0000[sec]
2025-05-27 13:38:16,867 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 13:38:16,988 - INFO - joeynmt.helpers - delete models/bpe_4k_model/10500.ckpt
2025-05-27 13:38:16,994 - INFO - joeynmt.training - Example #0
2025-05-27 13:38:16,994 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 13:38:16,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 13:38:16,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'f', ',', 'die', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'fen', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'etwa', 'die', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 's', 'war', '.', '</s>']
2025-05-27 13:38:16,994 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 13:38:16,994 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 13:38:16,994 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese zwei F<unk> @ olie zeigen , dass die P<unk> @ op<unk> @ f , die die P<unk> @ op<unk> @ fen , die die letzten drei Millionen Jahre etwa die letzten drei Millionen Jahren , die die letzten drei Millionen Jahre etwa die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ s war .
2025-05-27 13:38:16,994 - INFO - joeynmt.training - Example #1
2025-05-27 13:38:16,994 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 13:38:16,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 13:38:16,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'tatsächlich', 'die', 'ern@@', '<unk>', '@', 'st', ',', 'dass', 'das', 'Problem', 'der', 'ern@@', '<unk>', '@', 'st', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 13:38:16,995 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 13:38:16,995 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 13:38:16,995 - INFO - joeynmt.training - 	Hypothesis: Aber das ist tatsächlich die ern<unk> @ st , dass das Problem der ern<unk> @ st , weil es nicht die Di<unk> @ kte des E<unk> @ is des E<unk> @ is des E<unk> @ is zeigt .
2025-05-27 13:38:16,995 - INFO - joeynmt.training - Example #2
2025-05-27 13:38:16,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 13:38:16,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 13:38:16,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'la@@', '<unk>', '@', 'f', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'S@@', '<unk>', '@', 'atz', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', 'Kl@@', '<unk>', '@', 'im@@', '<unk>', '@', 'pro@@', '<unk>', '@', 'b@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 13:38:16,995 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 13:38:16,995 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 13:38:16,995 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sch<unk> @ la<unk> @ f auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ se S<unk> @ atz des glob<unk> @ alen Klima<unk> @ -<unk> @ -<unk> @ Kl<unk> @ im<unk> @ pro<unk> @ b<unk> @ sy<unk> @ stem .
2025-05-27 13:38:16,995 - INFO - joeynmt.training - Example #3
2025-05-27 13:38:16,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 13:38:16,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 13:38:16,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'elt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'elt', '.', '</s>']
2025-05-27 13:38:16,995 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 13:38:16,995 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 13:38:16,995 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ elt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ elt .
2025-05-27 13:38:16,995 - INFO - joeynmt.training - Example #4
2025-05-27 13:38:16,996 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 13:38:16,996 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 13:38:16,996 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2025-05-27 13:38:16,996 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 13:38:16,996 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 13:38:16,996 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert .
2025-05-27 13:38:57,987 - INFO - joeynmt.training - Epoch   5, Step:    13100, Batch Loss:     1.055572, Batch Acc: 0.696892, Tokens per Sec:     3715, Lr: 0.000300
2025-05-27 13:39:39,789 - INFO - joeynmt.training - Epoch   5, Step:    13200, Batch Loss:     1.157396, Batch Acc: 0.696124, Tokens per Sec:     3616, Lr: 0.000300
2025-05-27 13:40:20,355 - INFO - joeynmt.training - Epoch   5, Step:    13300, Batch Loss:     1.080330, Batch Acc: 0.699254, Tokens per Sec:     3747, Lr: 0.000300
2025-05-27 13:41:04,478 - INFO - joeynmt.training - Epoch   5, Step:    13400, Batch Loss:     1.081587, Batch Acc: 0.696342, Tokens per Sec:     3530, Lr: 0.000300
2025-05-27 13:41:47,438 - INFO - joeynmt.training - Epoch   5, Step:    13500, Batch Loss:     0.979797, Batch Acc: 0.695519, Tokens per Sec:     3576, Lr: 0.000300
2025-05-27 13:41:47,438 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 13:41:47,438 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 13:45:00,248 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.24, ppl:   3.46, acc:   0.67, generation: 192.7970[sec], evaluation: 0.0000[sec]
2025-05-27 13:45:00,254 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 13:45:00,366 - INFO - joeynmt.helpers - delete models/bpe_4k_model/11000.ckpt
2025-05-27 13:45:00,370 - INFO - joeynmt.training - Example #0
2025-05-27 13:45:00,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 13:45:00,370 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 13:45:00,370 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'f', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ul@@', '<unk>', '@', 'e', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', '-@@', '<unk>', '@', 'L@@', '<unk>', '@', 'ändern', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'w@@', '<unk>', '@', 'ün@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'te', '.', '</s>']
2025-05-27 13:45:00,370 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 13:45:00,370 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 13:45:00,370 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese zwei F<unk> @ olie zeigen , die P<unk> @ op<unk> @ f zeigen , dass die P<unk> @ op<unk> @ ul<unk> @ e , die die letzten drei Millionen Jahre etwa die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ -<unk> @ L<unk> @ ändern der USA , mit 40 Prozent der USA , mit 40 Prozent ver<unk> @ w<unk> @ ün<unk> @ sch<unk> @ te .
2025-05-27 13:45:00,371 - INFO - joeynmt.training - Example #1
2025-05-27 13:45:00,371 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 13:45:00,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 13:45:00,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zen', 'wirklich', 'die', 'ern@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ha@@', '<unk>', '@', 'ft', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 13:45:00,371 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 13:45:00,371 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 13:45:00,371 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zen wirklich die ern<unk> @ st<unk> @ ha<unk> @ ft , weil es nicht die Di<unk> @ kte des E<unk> @ is des E<unk> @ is zeigt .
2025-05-27 13:45:00,371 - INFO - joeynmt.training - Example #2
2025-05-27 13:45:00,371 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 13:45:00,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 13:45:00,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'auf', 'den', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', ',', 'das', 'ist', 'ein', 'kl@@', '<unk>', '@', 'eines', 'Her@@', '<unk>', '@', 'zen', 'von', 'uns', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 'n', '.', '</s>']
2025-05-27 13:45:00,371 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 13:45:00,371 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 13:45:00,371 - INFO - joeynmt.training - 	Hypothesis: Das E<unk> @ is<unk> @ e auf den Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise , das ist ein kl<unk> @ eines Her<unk> @ zen von uns glob<unk> @ alen Klima<unk> @ wandel<unk> @ n .
2025-05-27 13:45:00,371 - INFO - joeynmt.training - Example #3
2025-05-27 13:45:00,371 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 13:45:00,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 13:45:00,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'elt', '.', '</s>']
2025-05-27 13:45:00,372 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 13:45:00,372 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 13:45:00,372 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ elt .
2025-05-27 13:45:00,372 - INFO - joeynmt.training - Example #4
2025-05-27 13:45:00,372 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 13:45:00,372 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 13:45:00,372 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', 'was', 'die', 'letzten', '25', 'Jahre', 'passiert', 'ist', '.', '</s>']
2025-05-27 13:45:00,372 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 13:45:00,372 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 13:45:00,372 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zeigen werde , ist eine Ver<unk> @ sion von dem was die letzten 25 Jahre passiert ist .
2025-05-27 13:45:40,204 - INFO - joeynmt.training - Epoch   5, Step:    13600, Batch Loss:     1.150264, Batch Acc: 0.695140, Tokens per Sec:     3842, Lr: 0.000300
2025-05-27 13:46:23,461 - INFO - joeynmt.training - Epoch   5, Step:    13700, Batch Loss:     1.059270, Batch Acc: 0.695621, Tokens per Sec:     3567, Lr: 0.000300
2025-05-27 13:47:05,422 - INFO - joeynmt.training - Epoch   5, Step:    13800, Batch Loss:     1.124920, Batch Acc: 0.692694, Tokens per Sec:     3623, Lr: 0.000300
2025-05-27 13:47:45,110 - INFO - joeynmt.training - Epoch   5, Step:    13900, Batch Loss:     1.110065, Batch Acc: 0.699110, Tokens per Sec:     3717, Lr: 0.000300
2025-05-27 13:48:22,997 - INFO - joeynmt.training - Epoch   5, Step:    14000, Batch Loss:     1.122248, Batch Acc: 0.697525, Tokens per Sec:     3974, Lr: 0.000300
2025-05-27 13:48:22,998 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 13:48:22,998 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 13:51:39,153 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.24, ppl:   3.46, acc:   0.67, generation: 196.1461[sec], evaluation: 0.0000[sec]
2025-05-27 13:51:39,160 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 13:51:39,246 - INFO - joeynmt.helpers - delete models/bpe_4k_model/11500.ckpt
2025-05-27 13:51:39,251 - INFO - joeynmt.training - Example #0
2025-05-27 13:51:39,251 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 13:51:39,251 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 13:51:39,251 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'vor', ',', 'vor', 'zwei', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'die', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 's', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'etwa', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', '-@@', '<unk>', '@', 'L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 't@@', '<unk>', '@', 'um', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'te', ',', '</s>']
2025-05-27 13:51:39,251 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 13:51:39,251 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 13:51:39,251 - INFO - joeynmt.training - 	Hypothesis: Und vor , vor zwei F<unk> @ olie , die die zwei F<unk> @ olie zeigen , die P<unk> @ op<unk> @ s , die die drei Millionen Jahre etwa die letzten drei Millionen Jahre etwa die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ -<unk> @ L<unk> @ etz<unk> @ t<unk> @ um 40 Prozent der USA , mit 40 Prozent ver<unk> @ r<unk> @ ück<unk> @ te ,
2025-05-27 13:51:39,251 - INFO - joeynmt.training - Example #1
2025-05-27 13:51:39,251 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 13:51:39,251 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 13:51:39,251 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'eigentlich', 'der', 'Sch@@', '<unk>', '@', 'atten', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ischen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 13:51:39,251 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 13:51:39,251 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 13:51:39,251 - INFO - joeynmt.training - 	Hypothesis: Aber das ist eigentlich der Sch<unk> @ atten dieses spezi<unk> @ f<unk> @ ischen Problem , weil es nicht die Di<unk> @ kte des E<unk> @ is des E<unk> @ is zeigt .
2025-05-27 13:51:39,251 - INFO - joeynmt.training - Example #2
2025-05-27 13:51:39,251 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 13:51:39,251 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 13:51:39,252 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'z@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'en@@', '<unk>', '@', 'z', 'in', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'Weise', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 13:51:39,252 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 13:51:39,252 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 13:51:39,252 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ z<unk> @ sch<unk> @ en<unk> @ z in Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ se Weise des glob<unk> @ alen Klima<unk> @ sy<unk> @ stem .
2025-05-27 13:51:39,252 - INFO - joeynmt.training - Example #3
2025-05-27 13:51:39,252 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 13:51:39,252 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 13:51:39,252 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'iert', '.', '</s>']
2025-05-27 13:51:39,252 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 13:51:39,252 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 13:51:39,252 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ iert .
2025-05-27 13:51:39,252 - INFO - joeynmt.training - Example #4
2025-05-27 13:51:39,252 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 13:51:39,252 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 13:51:39,252 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zeigt', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 13:51:39,252 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 13:51:39,252 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 13:51:39,252 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zeigt , ist eine Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert ist .
2025-05-27 13:52:12,237 - INFO - joeynmt.training - Epoch   5, Step:    14100, Batch Loss:     1.207483, Batch Acc: 0.695093, Tokens per Sec:     4742, Lr: 0.000300
2025-05-27 13:52:46,564 - INFO - joeynmt.training - Epoch   5, Step:    14200, Batch Loss:     1.092082, Batch Acc: 0.696857, Tokens per Sec:     4528, Lr: 0.000300
2025-05-27 13:53:20,138 - INFO - joeynmt.training - Epoch   5, Step:    14300, Batch Loss:     1.033324, Batch Acc: 0.698926, Tokens per Sec:     4476, Lr: 0.000300
2025-05-27 13:53:52,494 - INFO - joeynmt.training - Epoch   5, Step:    14400, Batch Loss:     1.125025, Batch Acc: 0.696664, Tokens per Sec:     4785, Lr: 0.000300
2025-05-27 13:54:26,175 - INFO - joeynmt.training - Epoch   5, Step:    14500, Batch Loss:     1.048680, Batch Acc: 0.695863, Tokens per Sec:     4637, Lr: 0.000300
2025-05-27 13:54:26,202 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 13:54:26,202 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 13:57:26,573 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.23, ppl:   3.43, acc:   0.67, generation: 180.3621[sec], evaluation: 0.0000[sec]
2025-05-27 13:57:26,578 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 13:57:26,659 - INFO - joeynmt.helpers - delete models/bpe_4k_model/12000.ckpt
2025-05-27 13:57:26,665 - INFO - joeynmt.training - Example #0
2025-05-27 13:57:26,665 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 13:57:26,665 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 13:57:26,665 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', '-@@', '<unk>', '@', 'L@@', '<unk>', '@', 'auf@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', ',', 'die', 'drei', 'Millionen', 'Jahre', 'etwa', 'etwa', 'etwa', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 's', ',', 'die', 'die', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'ten', '.', '</s>']
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese beiden F<unk> @ olie zeigen , um diese beiden F<unk> @ olie zu zeigen , dass die P<unk> @ op<unk> @ -<unk> @ L<unk> @ auf<unk> @ sk<unk> @ a<unk> @ p , die drei Millionen Jahre etwa etwa etwa etwa die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ s , die die Verein<unk> @ igten Staaten , mit 40 Prozent ver<unk> @ r<unk> @ ück<unk> @ ten .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - Example #1
2025-05-27 13:57:26,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 13:57:26,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 13:57:26,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'Sch@@', '<unk>', '@', 'atten', 'dieses', 'spezi@@', '<unk>', '@', 'ellen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er', 'des', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die Sch<unk> @ atten dieses spezi<unk> @ ellen Problem , weil es nicht die Di<unk> @ ck<unk> @ er des E<unk> @ is des E<unk> @ is zeigt .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - Example #2
2025-05-27 13:57:26,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 13:57:26,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 13:57:26,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'Sin@@', '<unk>', '@', 'ne', 'des', 'G@@', '<unk>', '@', 'lü@@', '<unk>', '@', 'ss@@', '<unk>', '@', 'el@@', '<unk>', '@', '-@@', '<unk>', '@', 'K@@', '<unk>', '@', 'letter@@', '<unk>', '@', '-@@', '<unk>', '@', 'System', '.', '</s>']
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ se Sin<unk> @ ne des G<unk> @ lü<unk> @ ss<unk> @ el<unk> @ -<unk> @ K<unk> @ letter<unk> @ -<unk> @ System .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - Example #3
2025-05-27 13:57:26,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 13:57:26,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 13:57:26,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'aus', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'elt', '.', '</s>']
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - 	Hypothesis: Es ist aus der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ elt .
2025-05-27 13:57:26,666 - INFO - joeynmt.training - Example #4
2025-05-27 13:57:26,667 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 13:57:26,667 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 13:57:26,667 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'die', 'ich', 'Ihnen', 'eine', 'sch@@', '<unk>', '@', 're@@', '<unk>', '@', 'du@@', '<unk>', '@', 'ziert', 'habe', '.', '</s>']
2025-05-27 13:57:26,667 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 13:57:26,667 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 13:57:26,667 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , die ich Ihnen eine sch<unk> @ re<unk> @ du<unk> @ ziert habe .
2025-05-27 13:58:00,478 - INFO - joeynmt.training - Epoch   5, Step:    14600, Batch Loss:     1.185175, Batch Acc: 0.694819, Tokens per Sec:     4616, Lr: 0.000300
2025-05-27 13:58:34,090 - INFO - joeynmt.training - Epoch   5, Step:    14700, Batch Loss:     1.086855, Batch Acc: 0.697653, Tokens per Sec:     4622, Lr: 0.000300
2025-05-27 13:59:09,815 - INFO - joeynmt.training - Epoch   5, Step:    14800, Batch Loss:     1.114392, Batch Acc: 0.695118, Tokens per Sec:     4262, Lr: 0.000300
2025-05-27 13:59:50,312 - INFO - joeynmt.training - Epoch   5, Step:    14900, Batch Loss:     1.077819, Batch Acc: 0.695019, Tokens per Sec:     3741, Lr: 0.000300
2025-05-27 14:00:36,655 - INFO - joeynmt.training - Epoch   5, Step:    15000, Batch Loss:     1.147993, Batch Acc: 0.695852, Tokens per Sec:     3420, Lr: 0.000300
2025-05-27 14:00:36,657 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 14:00:36,657 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 14:04:06,329 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.23, ppl:   3.43, acc:   0.67, generation: 209.6633[sec], evaluation: 0.0000[sec]
2025-05-27 14:04:06,333 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 14:04:06,423 - INFO - joeynmt.helpers - delete models/bpe_4k_model/12500.ckpt
2025-05-27 14:04:06,428 - INFO - joeynmt.training - Example #0
2025-05-27 14:04:06,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 14:04:06,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 14:04:06,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Als', 'ich', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'diese', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'n@@', '<unk>', '@', 'es', 'war', ',', 'die', 'drei', 'Millionen', 'Jahre', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', '%', 'ver@@', '<unk>', '@', 'w@@', '<unk>', '@', 'ün@@', '<unk>', '@', 'det', '.', '</s>']
2025-05-27 14:04:06,428 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 14:04:06,428 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 14:04:06,429 - INFO - joeynmt.training - 	Hypothesis: Als ich diese beiden F<unk> @ olie diese zwei F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ op<unk> @ lin<unk> @ sk<unk> @ a<unk> @ p , die die drei Millionen Jahre etwa die Größ<unk> @ e des V<unk> @ ar<unk> @ sy<unk> @ n<unk> @ es war , die drei Millionen Jahre der USA , mit 40 Prozent der USA , mit 40 % ver<unk> @ w<unk> @ ün<unk> @ det .
2025-05-27 14:04:06,429 - INFO - joeynmt.training - Example #1
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'ern@@', '<unk>', '@', 'st', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ische', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 14:04:06,429 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 14:04:06,429 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 14:04:06,429 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die ern<unk> @ st dieses spezi<unk> @ f<unk> @ ische Problem , weil es nicht die Di<unk> @ kte des E<unk> @ is zeigt .
2025-05-27 14:04:06,429 - INFO - joeynmt.training - Example #2
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'Weise', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 's', '.', '</s>']
2025-05-27 14:04:06,429 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 14:04:06,429 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 14:04:06,429 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ k<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ se Weise des glob<unk> @ alen Klima<unk> @ wandel<unk> @ s .
2025-05-27 14:04:06,429 - INFO - joeynmt.training - Example #3
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'der', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 14:04:06,429 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 14:04:06,429 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 14:04:06,429 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in der S<unk> @ omm<unk> @ er .
2025-05-27 14:04:06,429 - INFO - joeynmt.training - Example #4
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 14:04:06,429 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'es', 'den', 'letzten', '25', 'Jahre', 'passiert', 'passiert', '.', '</s>']
2025-05-27 14:04:06,430 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 14:04:06,430 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 14:04:06,430 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine Ver<unk> @ sion von dem , was es den letzten 25 Jahre passiert passiert .
2025-05-27 14:04:37,773 - INFO - joeynmt.training - Epoch   5, Step:    15100, Batch Loss:     1.094530, Batch Acc: 0.695444, Tokens per Sec:     4832, Lr: 0.000300
2025-05-27 14:05:09,552 - INFO - joeynmt.training - Epoch   5, Step:    15200, Batch Loss:     1.130687, Batch Acc: 0.694686, Tokens per Sec:     4772, Lr: 0.000300
2025-05-27 14:05:43,293 - INFO - joeynmt.training - Epoch   5, Step:    15300, Batch Loss:     1.139909, Batch Acc: 0.695553, Tokens per Sec:     4533, Lr: 0.000300
2025-05-27 14:06:16,066 - INFO - joeynmt.training - Epoch   5, Step:    15400, Batch Loss:     1.114427, Batch Acc: 0.697150, Tokens per Sec:     4571, Lr: 0.000300
2025-05-27 14:06:48,957 - INFO - joeynmt.training - Epoch   5, Step:    15500, Batch Loss:     1.085060, Batch Acc: 0.694998, Tokens per Sec:     4646, Lr: 0.000300
2025-05-27 14:06:48,958 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 14:06:48,958 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 14:10:13,294 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.22, ppl:   3.39, acc:   0.67, generation: 204.3274[sec], evaluation: 0.0000[sec]
2025-05-27 14:10:13,302 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 14:10:13,396 - INFO - joeynmt.helpers - delete models/bpe_4k_model/13000.ckpt
2025-05-27 14:10:13,404 - INFO - joeynmt.training - Example #0
2025-05-27 14:10:13,404 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 14:10:13,404 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 14:10:13,405 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'ich', 'habe', 'diese', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', '-@@', '<unk>', '@', 'A@@', '<unk>', '@', 'pp@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'ien', ',', 'die', 'drei', 'Millionen', 'Jahre', 'ungefähr', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 'mor@@', '<unk>', '@', 'gens', ',', 'die', 'drei', 'Millionen', 'Jahre', 'ungefähr', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 's', 'war', '.', '</s>']
2025-05-27 14:10:13,405 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 14:10:13,405 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 14:10:13,405 - INFO - joeynmt.training - 	Hypothesis: Und ich habe diese zwei F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ op<unk> @ -<unk> @ A<unk> @ pp<unk> @ ar<unk> @ ien , die drei Millionen Jahre ungefähr die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ mor<unk> @ gens , die drei Millionen Jahre ungefähr die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ s war .
2025-05-27 14:10:13,405 - INFO - joeynmt.training - Example #1
2025-05-27 14:10:13,405 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 14:10:13,405 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 14:10:13,405 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'ern@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ha@@', '<unk>', '@', 'ft', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ischen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', '<unk>', '@', 'ing', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 14:10:13,405 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 14:10:13,405 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 14:10:13,405 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die ern<unk> @ st<unk> @ ha<unk> @ ft dieses spezi<unk> @ f<unk> @ ischen Problem , weil es nicht die D<unk> @ ing des E<unk> @ is zeigt .
2025-05-27 14:10:13,405 - INFO - joeynmt.training - Example #2
2025-05-27 14:10:13,405 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 14:10:13,405 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 14:10:13,405 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'issen', 'Sinn', ',', 'das', 'G@@', '<unk>', '@', 'eld@@', '<unk>', '@', 'ge@@', '<unk>', '@', 'ber', 'der', 'G@@', '<unk>', '@', 'eld@@', '<unk>', '@', '-@@', '<unk>', '@', 'System', '.', '</s>']
2025-05-27 14:10:13,405 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 14:10:13,405 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 14:10:13,405 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ issen Sinn , das G<unk> @ eld<unk> @ ge<unk> @ ber der G<unk> @ eld<unk> @ -<unk> @ System .
2025-05-27 14:10:13,405 - INFO - joeynmt.training - Example #3
2025-05-27 14:10:13,405 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 14:10:13,405 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 14:10:13,405 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 14:10:13,406 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 14:10:13,406 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 14:10:13,406 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 14:10:13,406 - INFO - joeynmt.training - Example #4
2025-05-27 14:10:13,406 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 14:10:13,406 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 14:10:13,406 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'möchte', ',', 'ist', 'eine', 'ver@@', '<unk>', '@', 'antwor@@', '<unk>', '@', 'tete', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 14:10:13,406 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 14:10:13,406 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 14:10:13,406 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zeigen möchte , ist eine ver<unk> @ antwor<unk> @ tete Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert ist .
2025-05-27 14:10:46,775 - INFO - joeynmt.training - Epoch   5, Step:    15600, Batch Loss:     1.098971, Batch Acc: 0.695136, Tokens per Sec:     4537, Lr: 0.000300
2025-05-27 14:10:55,281 - INFO - joeynmt.training - Epoch   5: total training loss 3464.06
2025-05-27 14:10:55,281 - INFO - joeynmt.training - EPOCH 6
2025-05-27 14:11:19,824 - INFO - joeynmt.training - Epoch   6, Step:    15700, Batch Loss:     1.003692, Batch Acc: 0.708011, Tokens per Sec:     4646, Lr: 0.000300
2025-05-27 14:11:53,742 - INFO - joeynmt.training - Epoch   6, Step:    15800, Batch Loss:     1.042529, Batch Acc: 0.710216, Tokens per Sec:     4483, Lr: 0.000300
2025-05-27 14:12:26,640 - INFO - joeynmt.training - Epoch   6, Step:    15900, Batch Loss:     1.155993, Batch Acc: 0.709056, Tokens per Sec:     4651, Lr: 0.000300
2025-05-27 14:12:58,328 - INFO - joeynmt.training - Epoch   6, Step:    16000, Batch Loss:     1.106782, Batch Acc: 0.705781, Tokens per Sec:     4804, Lr: 0.000300
2025-05-27 14:12:58,329 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 14:12:58,330 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 14:15:41,005 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.23, ppl:   3.41, acc:   0.67, generation: 162.6664[sec], evaluation: 0.0000[sec]
2025-05-27 14:15:41,099 - INFO - joeynmt.helpers - delete models/bpe_4k_model/13500.ckpt
2025-05-27 14:15:41,105 - INFO - joeynmt.training - Example #0
2025-05-27 14:15:41,105 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 14:15:41,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 14:15:41,105 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'den', 'letzten', 'letzten', 'Jahr@@', '<unk>', '@', 'zeh@@', '<unk>', '@', 'nt', 'zei@@', '<unk>', '@', 'gte', 'ich', 'diese', 'zwei', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'le', ',', 'die', 'drei', 'Millionen', 'Jahren', 'der', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'i@@', '<unk>', '@', 'versu@@', '<unk>', '@', 'ms', 'war', '.', '</s>']
2025-05-27 14:15:41,105 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 14:15:41,105 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 14:15:41,105 - INFO - joeynmt.training - 	Hypothesis: In den letzten letzten Jahr<unk> @ zeh<unk> @ nt zei<unk> @ gte ich diese zwei Di<unk> @ as zeigen , um zu zeigen , dass die P<unk> @ op<unk> @ ol<unk> @ le , die drei Millionen Jahren der Größ<unk> @ e des V<unk> @ ar<unk> @ i<unk> @ versu<unk> @ ms war .
2025-05-27 14:15:41,106 - INFO - joeynmt.training - Example #1
2025-05-27 14:15:41,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 14:15:41,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 14:15:41,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'z@@', '<unk>', '@', 'ung', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ischen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'te', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 14:15:41,106 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 14:15:41,106 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 14:15:41,106 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die sch<unk> @ ät<unk> @ z<unk> @ ung dieses spezi<unk> @ f<unk> @ ischen Problem , weil es nicht die Di<unk> @ ck<unk> @ te des E<unk> @ is zeigt .
2025-05-27 14:15:41,106 - INFO - joeynmt.training - Example #2
2025-05-27 14:15:41,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 14:15:41,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 14:15:41,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'en@@', '<unk>', '@', 'z', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'z', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', '-@@', '<unk>', '@', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 14:15:41,106 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 14:15:41,106 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 14:15:41,106 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sch<unk> @ en<unk> @ z auf der Nor<unk> @ d<unk> @ p<unk> @ ol<unk> @ z ist in gew<unk> @ is<unk> @ ser Weise das G<unk> @ lo<unk> @ b<unk> @ -<unk> @ Klima<unk> @ sy<unk> @ stem .
2025-05-27 14:15:41,106 - INFO - joeynmt.training - Example #3
2025-05-27 14:15:41,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 14:15:41,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 14:15:41,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 14:15:41,106 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 14:15:41,106 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 14:15:41,107 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 14:15:41,107 - INFO - joeynmt.training - Example #4
2025-05-27 14:15:41,107 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 14:15:41,107 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 14:15:41,107 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', '<unk>', '@', 'a@@', '<unk>', '@', 'gram@@', '<unk>', '@', 'm', ',', 'die', 'ich', 'Ihnen', 'Ihnen', 'zeigen', 'werde', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2025-05-27 14:15:41,107 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 14:15:41,107 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 14:15:41,107 - INFO - joeynmt.training - 	Hypothesis: Die nächste Di<unk> @ a<unk> @ gram<unk> @ m , die ich Ihnen Ihnen zeigen werde , ist eine Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert .
2025-05-27 14:16:13,512 - INFO - joeynmt.training - Epoch   6, Step:    16100, Batch Loss:     1.120106, Batch Acc: 0.707737, Tokens per Sec:     4674, Lr: 0.000300
2025-05-27 14:16:47,205 - INFO - joeynmt.training - Epoch   6, Step:    16200, Batch Loss:     1.061957, Batch Acc: 0.706511, Tokens per Sec:     4545, Lr: 0.000300
2025-05-27 14:17:20,048 - INFO - joeynmt.training - Epoch   6, Step:    16300, Batch Loss:     1.075510, Batch Acc: 0.708195, Tokens per Sec:     4653, Lr: 0.000300
2025-05-27 14:17:53,943 - INFO - joeynmt.training - Epoch   6, Step:    16400, Batch Loss:     0.998362, Batch Acc: 0.706432, Tokens per Sec:     4521, Lr: 0.000300
2025-05-27 14:18:25,796 - INFO - joeynmt.training - Epoch   6, Step:    16500, Batch Loss:     1.129412, Batch Acc: 0.707385, Tokens per Sec:     4826, Lr: 0.000300
2025-05-27 14:18:25,796 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 14:18:25,796 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 14:22:04,311 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.22, ppl:   3.38, acc:   0.67, generation: 218.5058[sec], evaluation: 0.0000[sec]
2025-05-27 14:22:04,317 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 14:22:04,404 - INFO - joeynmt.helpers - delete models/bpe_4k_model/14000.ckpt
2025-05-27 14:22:04,409 - INFO - joeynmt.training - Example #0
2025-05-27 14:22:04,409 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 14:22:04,409 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 14:22:04,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Als', 'ich', 'diese', 'beiden', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ul@@', '<unk>', '@', 'ier@@', '<unk>', '@', '-@@', '<unk>', '@', 'L@@', '<unk>', '@', 'og@@', '<unk>', '@', 'and', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'ar@@', '<unk>', '@', 's', 'der', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'ar@@', '<unk>', '@', 's', 'war', '.', '</s>']
2025-05-27 14:22:04,409 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 14:22:04,409 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 14:22:04,409 - INFO - joeynmt.training - 	Hypothesis: Als ich diese beiden Di<unk> @ as zeigen , um zu zeigen , dass die P<unk> @ op<unk> @ ul<unk> @ ier<unk> @ -<unk> @ L<unk> @ og<unk> @ and , die die drei Millionen Jahre etwa die Größ<unk> @ e des V<unk> @ ar<unk> @ s der Größ<unk> @ e des V<unk> @ ar<unk> @ s war .
2025-05-27 14:22:04,409 - INFO - joeynmt.training - Example #1
2025-05-27 14:22:04,409 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 14:22:04,409 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 14:22:04,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'tatsächlich', 'die', 'ern@@', '<unk>', '@', 'st', ',', 'dass', 'das', 'Lie@@', '<unk>', '@', 'b@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'gs@@', '<unk>', '@', 'proble@@', '<unk>', '@', 'm', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 14:22:04,409 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 14:22:04,409 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 14:22:04,409 - INFO - joeynmt.training - 	Hypothesis: Aber das ist tatsächlich die ern<unk> @ st , dass das Lie<unk> @ b<unk> @ lin<unk> @ gs<unk> @ proble<unk> @ m , weil es nicht die Di<unk> @ ck<unk> @ er des E<unk> @ is zeigt .
2025-05-27 14:22:04,409 - INFO - joeynmt.training - Example #2
2025-05-27 14:22:04,409 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 14:22:04,409 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 14:22:04,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'e', 'auf', 'den', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'Weise', 'das', 'glob@@', '<unk>', '@', 'ale', 'Klima@@', '<unk>', '@', 'k@@', '<unk>', '@', 'let@@', '<unk>', '@', 'tern', '.', '</s>']
2025-05-27 14:22:04,410 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 14:22:04,410 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 14:22:04,410 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ p<unk> @ ol<unk> @ e auf den Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ se Weise das glob<unk> @ ale Klima<unk> @ k<unk> @ let<unk> @ tern .
2025-05-27 14:22:04,410 - INFO - joeynmt.training - Example #3
2025-05-27 14:22:04,410 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 14:22:04,410 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 14:22:04,410 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'eine', 'S@@', '<unk>', '@', 'om@@', '<unk>', '@', 'mer@@', '<unk>', '@', 'n', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 14:22:04,410 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 14:22:04,410 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 14:22:04,410 - INFO - joeynmt.training - 	Hypothesis: Es ist eine S<unk> @ om<unk> @ mer<unk> @ n und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 14:22:04,410 - INFO - joeynmt.training - Example #4
2025-05-27 14:22:04,410 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 14:22:04,410 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 14:22:04,410 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'Ihnen', 'zeigen', 'habe', ',', 'ist', 'eine', 'ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 14:22:04,410 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 14:22:04,410 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 14:22:04,410 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen Ihnen zeigen habe , ist eine ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert ist .
2025-05-27 14:22:37,462 - INFO - joeynmt.training - Epoch   6, Step:    16600, Batch Loss:     1.073394, Batch Acc: 0.707115, Tokens per Sec:     4583, Lr: 0.000300
2025-05-27 14:23:08,398 - INFO - joeynmt.training - Epoch   6, Step:    16700, Batch Loss:     1.028258, Batch Acc: 0.706267, Tokens per Sec:     4876, Lr: 0.000300
2025-05-27 14:23:42,758 - INFO - joeynmt.training - Epoch   6, Step:    16800, Batch Loss:     1.003816, Batch Acc: 0.704138, Tokens per Sec:     4414, Lr: 0.000300
2025-05-27 14:24:13,781 - INFO - joeynmt.training - Epoch   6, Step:    16900, Batch Loss:     1.009101, Batch Acc: 0.708453, Tokens per Sec:     4911, Lr: 0.000300
2025-05-27 14:24:49,460 - INFO - joeynmt.training - Epoch   6, Step:    17000, Batch Loss:     1.102996, Batch Acc: 0.704861, Tokens per Sec:     4379, Lr: 0.000300
2025-05-27 14:24:49,460 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 14:24:49,460 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 14:27:55,013 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.22, ppl:   3.38, acc:   0.67, generation: 185.5439[sec], evaluation: 0.0000[sec]
2025-05-27 14:27:55,017 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 14:27:55,111 - INFO - joeynmt.helpers - delete models/bpe_4k_model/14500.ckpt
2025-05-27 14:27:55,117 - INFO - joeynmt.training - Example #0
2025-05-27 14:27:55,117 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 14:27:55,117 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 14:27:55,117 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'den', 'letzten', 'letzten', 'letzten', 'Jahr@@', '<unk>', '@', 'es', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ul@@', '<unk>', '@', 'ation', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', ',', 'um', 'die', 'Größ@@', '<unk>', '@', 'e', 'der', 'Größ@@', '<unk>', '@', 'e', 'der', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'syste@@', '<unk>', '@', 'ms', 'der', 'Größ@@', '<unk>', '@', 'e', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'ten', '.', '</s>']
2025-05-27 14:27:55,117 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 14:27:55,117 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 14:27:55,117 - INFO - joeynmt.training - 	Hypothesis: In den letzten letzten letzten Jahr<unk> @ es , um zu zeigen , dass die P<unk> @ op<unk> @ ul<unk> @ ation , die die drei Millionen Jahre , um die Größ<unk> @ e der Größ<unk> @ e der Größ<unk> @ e des V<unk> @ ar<unk> @ syste<unk> @ ms der Größ<unk> @ e der USA , mit 40 Prozent ver<unk> @ r<unk> @ ück<unk> @ ten .
2025-05-27 14:27:55,117 - INFO - joeynmt.training - Example #1
2025-05-27 14:27:55,117 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 14:27:55,117 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 14:27:55,117 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'ern@@', '<unk>', '@', 'ste', 'dieser', 'spezi@@', '<unk>', '@', 'ellen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'ung', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die ern<unk> @ ste dieser spezi<unk> @ ellen Problem , weil es nicht die Di<unk> @ ck<unk> @ ung des E<unk> @ is zeigt .
2025-05-27 14:27:55,118 - INFO - joeynmt.training - Example #2
2025-05-27 14:27:55,118 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 14:27:55,118 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 14:27:55,118 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', '-@@', '<unk>', '@', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise das G<unk> @ lo<unk> @ b<unk> @ -<unk> @ Klima<unk> @ sy<unk> @ stem .
2025-05-27 14:27:55,118 - INFO - joeynmt.training - Example #3
2025-05-27 14:27:55,118 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 14:27:55,118 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 14:27:55,118 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'das', ',', 'wie', 'man', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Hypothesis: Es ist das , wie man in der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 14:27:55,118 - INFO - joeynmt.training - Example #4
2025-05-27 14:27:55,118 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 14:27:55,118 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 14:27:55,118 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'ein', 'Ver@@', '<unk>', '@', 'sion', 'von', 'einem', 'ver@@', '<unk>', '@', 'a@@', '<unk>', '@', 'h@@', '<unk>', '@', 'nen', ',', 'was', 'die', 'letzten', '25', 'Jahre', 'passiert', 'ist', '.', '</s>']
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 14:27:55,118 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen ein Ver<unk> @ sion von einem ver<unk> @ a<unk> @ h<unk> @ nen , was die letzten 25 Jahre passiert ist .
2025-05-27 14:28:26,956 - INFO - joeynmt.training - Epoch   6, Step:    17100, Batch Loss:     1.046288, Batch Acc: 0.705433, Tokens per Sec:     4774, Lr: 0.000300
2025-05-27 14:29:07,767 - INFO - joeynmt.training - Epoch   6, Step:    17200, Batch Loss:     1.042192, Batch Acc: 0.708500, Tokens per Sec:     3772, Lr: 0.000300
2025-05-27 14:29:47,671 - INFO - joeynmt.training - Epoch   6, Step:    17300, Batch Loss:     1.006104, Batch Acc: 0.705647, Tokens per Sec:     3788, Lr: 0.000300
2025-05-27 14:30:30,887 - INFO - joeynmt.training - Epoch   6, Step:    17400, Batch Loss:     1.041955, Batch Acc: 0.705122, Tokens per Sec:     3578, Lr: 0.000300
2025-05-27 14:31:14,093 - INFO - joeynmt.training - Epoch   6, Step:    17500, Batch Loss:     1.083163, Batch Acc: 0.704232, Tokens per Sec:     3579, Lr: 0.000300
2025-05-27 14:31:14,094 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 14:31:14,094 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 14:34:39,953 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.21, ppl:   3.36, acc:   0.67, generation: 205.8452[sec], evaluation: 0.0000[sec]
2025-05-27 14:34:39,960 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 14:34:40,083 - INFO - joeynmt.helpers - delete models/bpe_4k_model/15000.ckpt
2025-05-27 14:34:40,088 - INFO - joeynmt.training - Example #0
2025-05-27 14:34:40,088 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 14:34:40,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 14:34:40,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Als', 'ich', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', '-@@', '<unk>', '@', 'A@@', '<unk>', '@', 'pp@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'ien', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', ',', 'die', 'in', 'den', 'letzten', 'drei', 'Millionen', 'Jahren', ',', 'die', 'in', 'der', 'USA', ',', 'mit', '40', '%', 'der', 'USA', ',', 'mit', '40', '%', 'der', 'USA', ',', 'mit', '40', '%', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'ten', ',', '</s>']
2025-05-27 14:34:40,089 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 14:34:40,089 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 14:34:40,089 - INFO - joeynmt.training - 	Hypothesis: Als ich diese beiden F<unk> @ olie zeigen , die ich Ihnen zeigen , dass die P<unk> @ op<unk> @ -<unk> @ A<unk> @ pp<unk> @ ar<unk> @ ien , die die letzten drei Millionen Jahre , die in den letzten drei Millionen Jahren , die in der USA , mit 40 % der USA , mit 40 % der USA , mit 40 % ver<unk> @ r<unk> @ ück<unk> @ ten ,
2025-05-27 14:34:40,089 - INFO - joeynmt.training - Example #1
2025-05-27 14:34:40,089 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 14:34:40,089 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 14:34:40,089 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Sch@@', '<unk>', '@', 'atten', 'dieses', 'Sch@@', '<unk>', '@', 'atten', 'dieses', 'spezi@@', '<unk>', '@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', '<unk>', '@', 'at@@', '<unk>', '@', 'en@@', '<unk>', '@', 'h@@', '<unk>', '@', 'nung', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 14:34:40,089 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 14:34:40,089 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 14:34:40,089 - INFO - joeynmt.training - 	Hypothesis: Aber das Sch<unk> @ atten dieses Sch<unk> @ atten dieses spezi<unk> @ elle Problem , weil es nicht die D<unk> @ at<unk> @ en<unk> @ h<unk> @ nung des E<unk> @ is zeigt .
2025-05-27 14:34:40,089 - INFO - joeynmt.training - Example #2
2025-05-27 14:34:40,089 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 14:34:40,089 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 14:34:40,089 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'E@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'Weise', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', '-@@', '<unk>', '@', 'K@@', '<unk>', '@', 'olle@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 14:34:40,090 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 14:34:40,090 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 14:34:40,090 - INFO - joeynmt.training - 	Hypothesis: Die E<unk> @ sk<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ se Weise das G<unk> @ lo<unk> @ b<unk> @ -<unk> @ K<unk> @ olle<unk> @ sy<unk> @ stem .
2025-05-27 14:34:40,090 - INFO - joeynmt.training - Example #3
2025-05-27 14:34:40,090 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 14:34:40,090 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 14:34:40,090 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'mer', '.', '</s>']
2025-05-27 14:34:40,090 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 14:34:40,090 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 14:34:40,090 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und K<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und K<unk> @ ri<unk> @ m<unk> @ mer .
2025-05-27 14:34:40,090 - INFO - joeynmt.training - Example #4
2025-05-27 14:34:40,090 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 14:34:40,090 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 14:34:40,090 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 14:34:40,090 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 14:34:40,090 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 14:34:40,090 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te , was in den letzten 25 Jahren passiert ist .
2025-05-27 14:35:21,029 - INFO - joeynmt.training - Epoch   6, Step:    17600, Batch Loss:     1.053538, Batch Acc: 0.705862, Tokens per Sec:     3740, Lr: 0.000300
2025-05-27 14:36:02,622 - INFO - joeynmt.training - Epoch   6, Step:    17700, Batch Loss:     1.156665, Batch Acc: 0.705477, Tokens per Sec:     3691, Lr: 0.000300
2025-05-27 14:36:43,854 - INFO - joeynmt.training - Epoch   6, Step:    17800, Batch Loss:     1.194318, Batch Acc: 0.704475, Tokens per Sec:     3787, Lr: 0.000300
2025-05-27 14:37:26,361 - INFO - joeynmt.training - Epoch   6, Step:    17900, Batch Loss:     0.964671, Batch Acc: 0.704922, Tokens per Sec:     3574, Lr: 0.000300
2025-05-27 14:38:06,882 - INFO - joeynmt.training - Epoch   6, Step:    18000, Batch Loss:     1.131146, Batch Acc: 0.704695, Tokens per Sec:     3766, Lr: 0.000300
2025-05-27 14:38:06,883 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 14:38:06,883 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 14:42:18,636 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.21, ppl:   3.36, acc:   0.68, generation: 251.7401[sec], evaluation: 0.0000[sec]
2025-05-27 14:42:18,642 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 14:42:18,757 - INFO - joeynmt.helpers - delete models/bpe_4k_model/16000.ckpt
2025-05-27 14:42:18,765 - INFO - joeynmt.training - Example #0
2025-05-27 14:42:18,765 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 14:42:18,765 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 14:42:18,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'letz@@', '<unk>', '@', 'tes', 'Jahr', 'zei@@', '<unk>', '@', 'gte', 'ich', 'diese', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', '-@@', '<unk>', '@', 'U@@', '<unk>', '@', '-@@', '<unk>', '@', 'U@@', '<unk>', '@', '-@@', '<unk>', '@', 'U@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', 'U@@', '<unk>', '@', '-@@', '<unk>', '@', 'U@@', '<unk>', '@', '-@@', '<unk>', '@', 'U@@', '<unk>', '@', '-@@', '<unk>', '@', 'U@@', '<unk>', '@', '-@@', '<unk>', '@', 'Bo@@', '<unk>', '@', 'den', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'ten', '.', '</s>']
2025-05-27 14:42:18,765 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 14:42:18,765 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 14:42:18,765 - INFO - joeynmt.training - 	Hypothesis: Und letz<unk> @ tes Jahr zei<unk> @ gte ich diese zwei F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ oo<unk> @ l<unk> @ -<unk> @ U<unk> @ -<unk> @ U<unk> @ -<unk> @ U<unk> @ -<unk> @ -<unk> @ U<unk> @ -<unk> @ U<unk> @ -<unk> @ U<unk> @ -<unk> @ U<unk> @ -<unk> @ Bo<unk> @ den , mit 40 Prozent ver<unk> @ r<unk> @ ück<unk> @ ten .
2025-05-27 14:42:18,765 - INFO - joeynmt.training - Example #1
2025-05-27 14:42:18,765 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 14:42:18,765 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 14:42:18,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'eigentlich', 'der', 'ern@@', '<unk>', '@', 'st', ',', 'dass', 'das', 'ern@@', '<unk>', '@', 'st', 'dieses', 'spezi@@', '<unk>', '@', 'ellen', 'Problem', 'ist', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 14:42:18,765 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 14:42:18,765 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 14:42:18,765 - INFO - joeynmt.training - 	Hypothesis: Aber das ist eigentlich der ern<unk> @ st , dass das ern<unk> @ st dieses spezi<unk> @ ellen Problem ist , weil es nicht die Di<unk> @ kte des E<unk> @ is zeigt .
2025-05-27 14:42:18,766 - INFO - joeynmt.training - Example #2
2025-05-27 14:42:18,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 14:42:18,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 14:42:18,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'en@@', '<unk>', '@', 'f', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'des', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'es', ',', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', '-@@', '<unk>', '@', 'System', '.', '</s>']
2025-05-27 14:42:18,766 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 14:42:18,766 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 14:42:18,766 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sch<unk> @ en<unk> @ f auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise des G<unk> @ lo<unk> @ b<unk> @ ot<unk> @ es , das G<unk> @ lo<unk> @ b<unk> @ -<unk> @ System .
2025-05-27 14:42:18,766 - INFO - joeynmt.training - Example #3
2025-05-27 14:42:18,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 14:42:18,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 14:42:18,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 14:42:18,766 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 14:42:18,766 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 14:42:18,766 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 14:42:18,766 - INFO - joeynmt.training - Example #4
2025-05-27 14:42:18,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 14:42:18,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 14:42:18,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'es', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 14:42:18,767 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 14:42:18,767 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 14:42:18,767 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von dem , was es in den letzten 25 Jahren passiert ist .
2025-05-27 14:43:04,480 - INFO - joeynmt.training - Epoch   6, Step:    18100, Batch Loss:     1.111964, Batch Acc: 0.706913, Tokens per Sec:     3355, Lr: 0.000300
2025-05-27 14:43:48,331 - INFO - joeynmt.training - Epoch   6, Step:    18200, Batch Loss:     1.028895, Batch Acc: 0.707154, Tokens per Sec:     3539, Lr: 0.000300
2025-05-27 14:44:33,528 - INFO - joeynmt.training - Epoch   6, Step:    18300, Batch Loss:     1.010607, Batch Acc: 0.705025, Tokens per Sec:     3421, Lr: 0.000300
2025-05-27 14:45:09,580 - INFO - joeynmt.training - Epoch   6, Step:    18400, Batch Loss:     1.034166, Batch Acc: 0.702493, Tokens per Sec:     4332, Lr: 0.000300
2025-05-27 14:45:46,801 - INFO - joeynmt.training - Epoch   6, Step:    18500, Batch Loss:     1.063274, Batch Acc: 0.701208, Tokens per Sec:     4121, Lr: 0.000300
2025-05-27 14:45:46,801 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 14:45:46,801 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 14:48:55,129 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.21, ppl:   3.35, acc:   0.67, generation: 188.3182[sec], evaluation: 0.0000[sec]
2025-05-27 14:48:55,135 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 14:48:55,225 - INFO - joeynmt.helpers - delete models/bpe_4k_model/15500.ckpt
2025-05-27 14:48:55,233 - INFO - joeynmt.training - Example #0
2025-05-27 14:48:55,233 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 14:48:55,233 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 14:48:55,233 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Im', 'letzten', 'Jahr', 'zei@@', '<unk>', '@', 'gte', 'ich', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'die', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zu', 'zeigen', ',', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', '-@@', '<unk>', '@', 'S@@', '<unk>', '@', '-@@', '<unk>', '@', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', ',', 'die', 'die', 'Größ@@', '<unk>', '@', 'e', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', 'der', 'USA', ',', 'mit', '40', '%', 'der', 'Größ@@', '<unk>', '@', 'e', 'der', 'USA', ',', 'mit', '40', '%', 'der', 'USA', ',', 'mit', '40', '%', 'der', 'F@@', '<unk>', '@', 'el@@', '<unk>', '@', 's@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ur', 'der', 'USA', '.', '</s>']
2025-05-27 14:48:55,233 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 14:48:55,233 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 14:48:55,233 - INFO - joeynmt.training - 	Hypothesis: Im letzten Jahr zei<unk> @ gte ich diese beiden F<unk> @ olie zeigen , um die zwei F<unk> @ olie zu zeigen , die P<unk> @ op<unk> @ -<unk> @ S<unk> @ -<unk> @ F<unk> @ olie , die die drei Millionen Jahre , die die Größ<unk> @ e der Verein<unk> @ igten Staaten der USA , mit 40 % der Größ<unk> @ e der USA , mit 40 % der USA , mit 40 % der F<unk> @ el<unk> @ s<unk> @ at<unk> @ ur der USA .
2025-05-27 14:48:55,233 - INFO - joeynmt.training - Example #1
2025-05-27 14:48:55,233 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 14:48:55,233 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 14:48:55,233 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'ern@@', '<unk>', '@', 'st', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ische', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'te', 'des', 'E@@', '<unk>', '@', 'is', '.', '</s>']
2025-05-27 14:48:55,234 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 14:48:55,234 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 14:48:55,234 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die ern<unk> @ st dieses spezi<unk> @ f<unk> @ ische Problem , weil es nicht die Di<unk> @ ck<unk> @ te des E<unk> @ is .
2025-05-27 14:48:55,234 - INFO - joeynmt.training - Example #2
2025-05-27 14:48:55,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 14:48:55,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 14:48:55,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'Weise', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'ck@@', '<unk>', '@', '-@@', '<unk>', '@', 'B@@', '<unk>', '@', 'erg@@', '<unk>', '@', 'ie', '.', '</s>']
2025-05-27 14:48:55,234 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 14:48:55,234 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 14:48:55,234 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sk<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ se Weise das G<unk> @ lo<unk> @ ck<unk> @ -<unk> @ B<unk> @ erg<unk> @ ie .
2025-05-27 14:48:55,234 - INFO - joeynmt.training - Example #3
2025-05-27 14:48:55,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 14:48:55,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 14:48:55,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 14:48:55,234 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 14:48:55,234 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 14:48:55,234 - INFO - joeynmt.training - 	Hypothesis: Es ist in der S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 14:48:55,234 - INFO - joeynmt.training - Example #4
2025-05-27 14:48:55,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 14:48:55,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 14:48:55,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', '@', 'ste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 14:48:55,234 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 14:48:55,234 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 14:48:55,235 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> @ ste F<unk> @ olie , die ich Ihnen zeigen werde , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert ist .
2025-05-27 14:49:28,401 - INFO - joeynmt.training - Epoch   6, Step:    18600, Batch Loss:     1.146672, Batch Acc: 0.707287, Tokens per Sec:     4610, Lr: 0.000300
2025-05-27 14:50:04,752 - INFO - joeynmt.training - Epoch   6, Step:    18700, Batch Loss:     1.118111, Batch Acc: 0.707267, Tokens per Sec:     4265, Lr: 0.000300
2025-05-27 14:50:21,546 - INFO - joeynmt.training - Epoch   6: total training loss 3342.06
2025-05-27 14:50:21,547 - INFO - joeynmt.training - EPOCH 7
2025-05-27 14:50:40,412 - INFO - joeynmt.training - Epoch   7, Step:    18800, Batch Loss:     1.030484, Batch Acc: 0.724427, Tokens per Sec:     4088, Lr: 0.000300
2025-05-27 14:51:14,475 - INFO - joeynmt.training - Epoch   7, Step:    18900, Batch Loss:     1.144655, Batch Acc: 0.719277, Tokens per Sec:     4502, Lr: 0.000300
2025-05-27 14:51:49,117 - INFO - joeynmt.training - Epoch   7, Step:    19000, Batch Loss:     1.025400, Batch Acc: 0.718933, Tokens per Sec:     4412, Lr: 0.000300
2025-05-27 14:51:49,117 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 14:51:49,117 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 14:55:15,419 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.21, ppl:   3.34, acc:   0.68, generation: 206.2916[sec], evaluation: 0.0000[sec]
2025-05-27 14:55:15,424 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 14:55:15,519 - INFO - joeynmt.helpers - delete models/bpe_4k_model/16500.ckpt
2025-05-27 14:55:15,527 - INFO - joeynmt.training - Example #0
2025-05-27 14:55:15,527 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 14:55:15,527 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 14:55:15,527 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'in', 'den', 'letzten', 'Jahr', 'zei@@', '<unk>', '@', 'gte', 'ich', 'diese', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', ',', 'die', 'die', 'Größ@@', '<unk>', '@', 'e', 'von', 'den', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'est@@', '<unk>', '@', 'ung', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'w@@', '<unk>', '@', 'ün@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'wer@@', '<unk>', '@', 'fen', '.', '</s>']
2025-05-27 14:55:15,527 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 14:55:15,527 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 14:55:15,527 - INFO - joeynmt.training - 	Hypothesis: Und in den letzten Jahr zei<unk> @ gte ich diese zwei F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ oo<unk> @ l<unk> @ lin<unk> @ sk<unk> @ a<unk> @ p , die die drei Millionen Jahre , die die Größ<unk> @ e von den USA , mit 40 Prozent der F<unk> @ est<unk> @ ung der Verein<unk> @ igten Staaten , mit 40 Prozent ver<unk> @ w<unk> @ ün<unk> @ sch<unk> @ wer<unk> @ fen .
2025-05-27 14:55:15,527 - INFO - joeynmt.training - Example #1
2025-05-27 14:55:15,527 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 14:55:15,527 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 14:55:15,527 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'wirklich', 'wirklich', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ische', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er@@', '<unk>', '@', 'ung', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 14:55:15,527 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 14:55:15,527 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 14:55:15,527 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das wirklich wirklich spezi<unk> @ f<unk> @ ische Problem , weil es nicht die Di<unk> @ ck<unk> @ er<unk> @ ung des E<unk> @ is zeigt .
2025-05-27 14:55:15,527 - INFO - joeynmt.training - Example #2
2025-05-27 14:55:15,527 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 14:55:15,527 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 14:55:15,527 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 14:55:15,528 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 14:55:15,528 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 14:55:15,528 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ e auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise des glob<unk> @ alen Klima<unk> @ sy<unk> @ stem .
2025-05-27 14:55:15,528 - INFO - joeynmt.training - Example #3
2025-05-27 14:55:15,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 14:55:15,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 14:55:15,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 14:55:15,528 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 14:55:15,528 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 14:55:15,528 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 14:55:15,528 - INFO - joeynmt.training - Example #4
2025-05-27 14:55:15,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 14:55:15,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 14:55:15,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'ver@@', '<unk>', '@', 'la@@', '<unk>', '@', 'sse', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 14:55:15,528 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 14:55:15,528 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 14:55:15,528 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich zei<unk> @ ge , ist eine ver<unk> @ la<unk> @ sse Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert ist .
2025-05-27 14:55:49,357 - INFO - joeynmt.training - Epoch   7, Step:    19100, Batch Loss:     1.043178, Batch Acc: 0.718858, Tokens per Sec:     4449, Lr: 0.000300
2025-05-27 14:56:20,762 - INFO - joeynmt.training - Epoch   7, Step:    19200, Batch Loss:     1.013088, Batch Acc: 0.715970, Tokens per Sec:     4773, Lr: 0.000300
2025-05-27 14:56:55,209 - INFO - joeynmt.training - Epoch   7, Step:    19300, Batch Loss:     0.967258, Batch Acc: 0.717430, Tokens per Sec:     4495, Lr: 0.000300
2025-05-27 14:57:32,635 - INFO - joeynmt.training - Epoch   7, Step:    19400, Batch Loss:     1.116137, Batch Acc: 0.712152, Tokens per Sec:     4129, Lr: 0.000300
2025-05-27 14:58:08,068 - INFO - joeynmt.training - Epoch   7, Step:    19500, Batch Loss:     1.151137, Batch Acc: 0.713611, Tokens per Sec:     4260, Lr: 0.000300
2025-05-27 14:58:08,069 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 14:58:08,069 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 15:01:02,630 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.21, ppl:   3.36, acc:   0.67, generation: 174.5522[sec], evaluation: 0.0000[sec]
2025-05-27 15:01:02,726 - INFO - joeynmt.helpers - delete models/bpe_4k_model/17000.ckpt
2025-05-27 15:01:02,733 - INFO - joeynmt.training - Example #0
2025-05-27 15:01:02,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 15:01:02,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 15:01:02,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Als', 'ich', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'op@@', '<unk>', '@', 'er@@', '<unk>', '@', '-@@', '<unk>', '@', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'drei', 'Millionen', 'Jahre', 'alt', 'war', ',', 'um', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 's', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'ten', ',', 'mit', '40', 'Prozent', 'der', 'USA', '.', '</s>']
2025-05-27 15:01:02,734 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 15:01:02,734 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 15:01:02,734 - INFO - joeynmt.training - 	Hypothesis: Als ich diese beiden F<unk> @ olie zeigen , um diese beiden F<unk> @ olie zu zeigen , dass die P<unk> @ op<unk> @ op<unk> @ er<unk> @ -<unk> @ F<unk> @ olie , die drei Millionen Jahre alt war , um die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ s der Verein<unk> @ igten Staaten , mit 40 Prozent ver<unk> @ r<unk> @ ück<unk> @ ten , mit 40 Prozent der USA .
2025-05-27 15:01:02,734 - INFO - joeynmt.training - Example #1
2025-05-27 15:01:02,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 15:01:02,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 15:01:02,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'ern@@', '<unk>', '@', 'ste', 'Sache', 'dieses', 'spezi@@', '<unk>', '@', 'ell@@', '<unk>', '@', 'es', 'Problem', ',', 'denn', 'es', 'nicht', 'der', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 's@@', '<unk>', '@', 'al', 'des', 'E@@', '<unk>', '@', 'is', '.', '</s>']
2025-05-27 15:01:02,734 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 15:01:02,734 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 15:01:02,734 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die ern<unk> @ ste Sache dieses spezi<unk> @ ell<unk> @ es Problem , denn es nicht der Di<unk> @ ck<unk> @ s<unk> @ al des E<unk> @ is .
2025-05-27 15:01:02,734 - INFO - joeynmt.training - Example #2
2025-05-27 15:01:02,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 15:01:02,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 15:01:02,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'en@@', '<unk>', '@', 'f', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 15:01:02,734 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 15:01:02,734 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 15:01:02,734 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sch<unk> @ en<unk> @ f auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise des glob<unk> @ alen Klima<unk> @ sy<unk> @ stem .
2025-05-27 15:01:02,734 - INFO - joeynmt.training - Example #3
2025-05-27 15:01:02,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 15:01:02,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 15:01:02,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 15:01:02,735 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 15:01:02,735 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 15:01:02,735 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 15:01:02,735 - INFO - joeynmt.training - Example #4
2025-05-27 15:01:02,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 15:01:02,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 15:01:02,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'möchte', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'es', 'den', 'letzten', '25', 'Jahren', 'passiert', 'passiert', 'ist', '.', '</s>']
2025-05-27 15:01:02,735 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 15:01:02,735 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 15:01:02,735 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zeigen möchte , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von dem , was es den letzten 25 Jahren passiert passiert ist .
2025-05-27 15:01:40,658 - INFO - joeynmt.training - Epoch   7, Step:    19600, Batch Loss:     0.957271, Batch Acc: 0.714750, Tokens per Sec:     3983, Lr: 0.000300
2025-05-27 15:02:21,478 - INFO - joeynmt.training - Epoch   7, Step:    19700, Batch Loss:     1.165501, Batch Acc: 0.714227, Tokens per Sec:     3785, Lr: 0.000300
2025-05-27 15:03:03,768 - INFO - joeynmt.training - Epoch   7, Step:    19800, Batch Loss:     0.984754, Batch Acc: 0.713452, Tokens per Sec:     3649, Lr: 0.000300
2025-05-27 15:03:44,641 - INFO - joeynmt.training - Epoch   7, Step:    19900, Batch Loss:     0.976221, Batch Acc: 0.713539, Tokens per Sec:     3754, Lr: 0.000300
2025-05-27 15:04:28,387 - INFO - joeynmt.training - Epoch   7, Step:    20000, Batch Loss:     0.995734, Batch Acc: 0.712071, Tokens per Sec:     3540, Lr: 0.000300
2025-05-27 15:04:28,388 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 15:04:28,388 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 15:08:10,973 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.20, ppl:   3.34, acc:   0.68, generation: 222.5716[sec], evaluation: 0.0000[sec]
2025-05-27 15:08:10,978 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 15:08:11,104 - INFO - joeynmt.helpers - delete models/bpe_4k_model/17500.ckpt
2025-05-27 15:08:11,108 - INFO - joeynmt.training - Example #0
2025-05-27 15:08:11,108 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 15:08:11,108 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 15:08:11,109 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'den', 'letzten', 'letzten', 'Jahren', 'habe', 'ich', 'diese', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', ',', 'die', 'drei', 'Millionen', 'Jahre', 'ungefähr', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 'mor@@', '<unk>', '@', 'gens', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', 'dem', 'F@@', '<unk>', '@', 'ün@@', '<unk>', '@', 'f', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'Ver@@', '<unk>', '@', 'k@@', '<unk>', '@', 'ro@@', '<unk>', '@', 'm@@', '<unk>', '@', 'eln', '.', '</s>']
2025-05-27 15:08:11,109 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 15:08:11,109 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 15:08:11,109 - INFO - joeynmt.training - 	Hypothesis: In den letzten letzten Jahren habe ich diese zwei F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ oo<unk> @ l<unk> @ lin<unk> @ sk<unk> @ a<unk> @ p , die drei Millionen Jahre ungefähr die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ mor<unk> @ gens , mit 40 Prozent der USA , mit dem F<unk> @ ün<unk> @ f der USA , mit 40 Prozent der Ver<unk> @ k<unk> @ ro<unk> @ m<unk> @ eln .
2025-05-27 15:08:11,109 - INFO - joeynmt.training - Example #1
2025-05-27 15:08:11,109 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 15:08:11,109 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 15:08:11,109 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ä@@', '<unk>', '@', 'ft', 'dieses', 'spezi@@', '<unk>', '@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', '<unk>', '@', 'at@@', '<unk>', '@', 'en@@', '<unk>', '@', 'h@@', '<unk>', '@', 'aus', ',', 'weil', 'es', 'nicht', 'die', 'D@@', '<unk>', '@', 'et@@', '<unk>', '@', 'a@@', '<unk>', '@', 'ck', 'sehen', '.', '</s>']
2025-05-27 15:08:11,109 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 15:08:11,109 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 15:08:11,109 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ä<unk> @ ft dieses spezi<unk> @ elle Problem , weil es nicht die D<unk> @ at<unk> @ en<unk> @ h<unk> @ aus , weil es nicht die D<unk> @ et<unk> @ a<unk> @ ck sehen .
2025-05-27 15:08:11,109 - INFO - joeynmt.training - Example #2
2025-05-27 15:08:11,109 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 15:08:11,109 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 15:08:11,109 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'en@@', '<unk>', '@', 'k', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Sin@@', '<unk>', '@', 'ne', ',', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', '-@@', '<unk>', '@', 'Klima@@', '<unk>', '@', '-@@', '<unk>', '@', 'Klima@@', '<unk>', '@', '-@@', '<unk>', '@', 'Klima@@', '<unk>', '@', '-@@', '<unk>', '@', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 15:08:11,110 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 15:08:11,110 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 15:08:11,110 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sch<unk> @ en<unk> @ k auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Sin<unk> @ ne , das G<unk> @ lo<unk> @ b<unk> @ -<unk> @ Klima<unk> @ -<unk> @ Klima<unk> @ -<unk> @ Klima<unk> @ -<unk> @ Klima<unk> @ sy<unk> @ stem .
2025-05-27 15:08:11,110 - INFO - joeynmt.training - Example #3
2025-05-27 15:08:11,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 15:08:11,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 15:08:11,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 15:08:11,110 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 15:08:11,110 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 15:08:11,110 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 15:08:11,110 - INFO - joeynmt.training - Example #4
2025-05-27 15:08:11,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 15:08:11,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 15:08:11,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', '<unk>', '@', 'sch@@', '<unk>', '@', 'la@@', '<unk>', '@', 'sse', ',', 'die', 'ich', 'Ihnen', 'zeigen', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 15:08:11,110 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 15:08:11,110 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 15:08:11,110 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie <unk> @ sch<unk> @ la<unk> @ sse , die ich Ihnen zeigen , ist eine Ver<unk> @ sion von was in den letzten 25 Jahren passiert ist .
2025-05-27 15:08:53,433 - INFO - joeynmt.training - Epoch   7, Step:    20100, Batch Loss:     1.085530, Batch Acc: 0.712193, Tokens per Sec:     3595, Lr: 0.000300
2025-05-27 15:09:39,322 - INFO - joeynmt.training - Epoch   7, Step:    20200, Batch Loss:     1.095230, Batch Acc: 0.711853, Tokens per Sec:     3418, Lr: 0.000300
2025-05-27 15:10:19,970 - INFO - joeynmt.training - Epoch   7, Step:    20300, Batch Loss:     1.018782, Batch Acc: 0.713313, Tokens per Sec:     3643, Lr: 0.000300
2025-05-27 15:11:01,186 - INFO - joeynmt.training - Epoch   7, Step:    20400, Batch Loss:     1.030759, Batch Acc: 0.711192, Tokens per Sec:     3767, Lr: 0.000300
2025-05-27 15:11:44,153 - INFO - joeynmt.training - Epoch   7, Step:    20500, Batch Loss:     1.128999, Batch Acc: 0.711772, Tokens per Sec:     3562, Lr: 0.000300
2025-05-27 15:11:44,154 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 15:11:44,154 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 15:15:33,129 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.20, ppl:   3.32, acc:   0.68, generation: 228.9617[sec], evaluation: 0.0000[sec]
2025-05-27 15:15:33,136 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 15:15:33,264 - INFO - joeynmt.helpers - delete models/bpe_4k_model/18000.ckpt
2025-05-27 15:15:33,269 - INFO - joeynmt.training - Example #0
2025-05-27 15:15:33,269 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 15:15:33,269 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 15:15:33,269 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'hat', 'ich', 'diese', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ul@@', '<unk>', '@', 'ation', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'etwa', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 'ns', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'die', 'etwa', '40', 'Prozent', 'der', 'USA', ',', 'mit', 'den', 'U@@', '<unk>', '@', '.@@', '<unk>', '@', '.', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'ten', ',', '</s>']
2025-05-27 15:15:33,269 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 15:15:33,269 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 15:15:33,269 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr hat ich diese zwei F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ op<unk> @ ul<unk> @ ation , die die letzten drei Millionen Jahre etwa etwa die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ ns der Verein<unk> @ igten Staaten , die etwa 40 Prozent der USA , mit den U<unk> @ .<unk> @ . 40 Prozent ver<unk> @ r<unk> @ ück<unk> @ ten ,
2025-05-27 15:15:33,269 - INFO - joeynmt.training - Example #1
2025-05-27 15:15:33,269 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 15:15:33,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 15:15:33,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'tatsächlich', 'die', 'ern@@', '<unk>', '@', 'ste', 'Sache', ',', 'die', 'die', 'ern@@', '<unk>', '@', 'ste', 'Sache', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ischen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 15:15:33,270 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 15:15:33,270 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 15:15:33,270 - INFO - joeynmt.training - 	Hypothesis: Aber das ist tatsächlich die ern<unk> @ ste Sache , die die ern<unk> @ ste Sache dieses spezi<unk> @ f<unk> @ ischen Problem , weil es nicht die Di<unk> @ kte des E<unk> @ is zeigt .
2025-05-27 15:15:33,270 - INFO - joeynmt.training - Example #2
2025-05-27 15:15:33,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 15:15:33,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 15:15:33,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'en@@', '<unk>', '@', 'k', 'auf', 'dem', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'das', 'ein', 'kl@@', '<unk>', '@', 'eines', 'Her@@', '<unk>', '@', 'z', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 's', '.', '</s>']
2025-05-27 15:15:33,270 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 15:15:33,270 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 15:15:33,270 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sch<unk> @ en<unk> @ k auf dem Nor<unk> @ d<unk> @ po<unk> @ l ist das ein kl<unk> @ eines Her<unk> @ z des glob<unk> @ alen Klima<unk> @ wandel<unk> @ s .
2025-05-27 15:15:33,270 - INFO - joeynmt.training - Example #3
2025-05-27 15:15:33,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 15:15:33,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 15:15:33,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 15:15:33,271 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 15:15:33,271 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 15:15:33,271 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 15:15:33,271 - INFO - joeynmt.training - Example #4
2025-05-27 15:15:33,271 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 15:15:33,271 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 15:15:33,271 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'es', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 15:15:33,271 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 15:15:33,271 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 15:15:33,271 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von dem , was es in den letzten 25 Jahren passiert ist .
2025-05-27 15:16:14,715 - INFO - joeynmt.training - Epoch   7, Step:    20600, Batch Loss:     1.055267, Batch Acc: 0.712998, Tokens per Sec:     3668, Lr: 0.000300
2025-05-27 15:16:57,027 - INFO - joeynmt.training - Epoch   7, Step:    20700, Batch Loss:     1.015148, Batch Acc: 0.710842, Tokens per Sec:     3650, Lr: 0.000300
2025-05-27 15:17:38,243 - INFO - joeynmt.training - Epoch   7, Step:    20800, Batch Loss:     1.103409, Batch Acc: 0.711009, Tokens per Sec:     3684, Lr: 0.000300
2025-05-27 15:18:19,766 - INFO - joeynmt.training - Epoch   7, Step:    20900, Batch Loss:     1.100238, Batch Acc: 0.711826, Tokens per Sec:     3733, Lr: 0.000300
2025-05-27 15:19:00,871 - INFO - joeynmt.training - Epoch   7, Step:    21000, Batch Loss:     1.055901, Batch Acc: 0.713677, Tokens per Sec:     3748, Lr: 0.000300
2025-05-27 15:19:00,871 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 15:19:00,871 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 15:22:38,102 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.19, ppl:   3.30, acc:   0.68, generation: 217.2188[sec], evaluation: 0.0000[sec]
2025-05-27 15:22:38,110 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 15:22:38,225 - INFO - joeynmt.helpers - delete models/bpe_4k_model/19500.ckpt
2025-05-27 15:22:38,227 - INFO - joeynmt.training - Example #0
2025-05-27 15:22:38,227 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 15:22:38,227 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 15:22:38,227 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'letzten', 'letzten', 'Jahr@@', '<unk>', '@', 'es', 'war', 'ich', 'diese', 'zwei', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'alt', 'war', ',', 'um', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'alt', ',', 'die', 'sich', 'die', 'Größ@@', '<unk>', '@', 'e', 'von', 'der', 'Größ@@', '<unk>', '@', 'e', 'von', 'V@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'i@@', '<unk>', '@', 'let@@', '<unk>', '@', 'scher', 'war', '.', '</s>']
2025-05-27 15:22:38,228 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 15:22:38,228 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 15:22:38,228 - INFO - joeynmt.training - 	Hypothesis: In letzten letzten Jahr<unk> @ es war ich diese zwei Di<unk> @ as zeigen , um die letzten drei Millionen Jahre alt war , um die letzten drei Millionen Jahre alt , die sich die Größ<unk> @ e von der Größ<unk> @ e von V<unk> @ ar<unk> @ i<unk> @ let<unk> @ scher war .
2025-05-27 15:22:38,228 - INFO - joeynmt.training - Example #1
2025-05-27 15:22:38,228 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 15:22:38,228 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 15:22:38,228 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'Problem', 'der', 'Er@@', '<unk>', '@', 'n@@', '<unk>', '@', 'st', 'dieses', 'spezi@@', '<unk>', '@', 'ellen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 15:22:38,228 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 15:22:38,228 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 15:22:38,228 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das Problem der Er<unk> @ n<unk> @ st dieses spezi<unk> @ ellen Problem , weil es nicht die Di<unk> @ kte des E<unk> @ is zeigt .
2025-05-27 15:22:38,228 - INFO - joeynmt.training - Example #2
2025-05-27 15:22:38,228 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 15:22:38,228 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 15:22:38,228 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'en@@', '<unk>', '@', 'k', 'auf', 'dem', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', ',', 'der', 'im', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'des', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', 's', 'glob@@', '<unk>', '@', 'al', 'Klima@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'r@@', '<unk>', '@', 'itt@@', '<unk>', '@', 'stell@@', '<unk>', '@', 'e', '.', '</s>']
2025-05-27 15:22:38,228 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 15:22:38,228 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 15:22:38,229 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sch<unk> @ en<unk> @ k auf dem Nor<unk> @ d<unk> @ po<unk> @ l ist , der im gew<unk> @ is<unk> @ ser Weise des G<unk> @ lo<unk> @ b<unk> @ s glob<unk> @ al Klima<unk> @ sch<unk> @ r<unk> @ itt<unk> @ stell<unk> @ e .
2025-05-27 15:22:38,229 - INFO - joeynmt.training - Example #3
2025-05-27 15:22:38,229 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 15:22:38,229 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 15:22:38,229 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', '.', '</s>']
2025-05-27 15:22:38,229 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 15:22:38,229 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 15:22:38,229 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt .
2025-05-27 15:22:38,229 - INFO - joeynmt.training - Example #4
2025-05-27 15:22:38,229 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 15:22:38,229 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 15:22:38,229 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'zei@@', '<unk>', '@', 'ge', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'gte', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'ste', 'Ver@@', '<unk>', '@', 'sion', 'von', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 15:22:38,229 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 15:22:38,229 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 15:22:38,229 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich zei<unk> @ ge , die ich Ihnen zei<unk> @ gte , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ ste Ver<unk> @ sion von was in den letzten 25 Jahren passiert ist .
2025-05-27 15:23:23,113 - INFO - joeynmt.training - Epoch   7, Step:    21100, Batch Loss:     0.949792, Batch Acc: 0.712867, Tokens per Sec:     3402, Lr: 0.000300
2025-05-27 15:24:08,888 - INFO - joeynmt.training - Epoch   7, Step:    21200, Batch Loss:     1.037344, Batch Acc: 0.711947, Tokens per Sec:     3337, Lr: 0.000300
2025-05-27 15:24:49,333 - INFO - joeynmt.training - Epoch   7, Step:    21300, Batch Loss:     1.031783, Batch Acc: 0.713457, Tokens per Sec:     3707, Lr: 0.000300
2025-05-27 15:25:31,976 - INFO - joeynmt.training - Epoch   7, Step:    21400, Batch Loss:     1.073625, Batch Acc: 0.710059, Tokens per Sec:     3623, Lr: 0.000300
2025-05-27 15:26:17,396 - INFO - joeynmt.training - Epoch   7, Step:    21500, Batch Loss:     0.978502, Batch Acc: 0.714927, Tokens per Sec:     3442, Lr: 0.000300
2025-05-27 15:26:17,397 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 15:26:17,397 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 15:29:51,658 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.19, ppl:   3.29, acc:   0.68, generation: 214.2513[sec], evaluation: 0.0000[sec]
2025-05-27 15:29:51,662 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 15:29:51,765 - INFO - joeynmt.helpers - delete models/bpe_4k_model/18500.ckpt
2025-05-27 15:29:51,772 - INFO - joeynmt.training - Example #0
2025-05-27 15:29:51,773 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 15:29:51,773 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 15:29:51,773 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'letz@@', '<unk>', '@', 'tes', 'Jahr', 'zei@@', '<unk>', '@', 'gte', 'ich', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'o@@', '<unk>', '@', 'gel@@', '<unk>', '@', 't', ',', 'die', 'drei', 'Millionen', 'Jahre', ',', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'F@@', '<unk>', '@', 'els', 'der', 'USA', ',', 'mit', 'den', 'Groß@@', '<unk>', '@', 'br@@', '<unk>', '@', 'it@@', '<unk>', '@', 'an@@', '<unk>', '@', 'ni@@', '<unk>', '@', 'en', 'der', 'USA', ',', 'mit', '40', '%', 'ver@@', '<unk>', '@', 'rü@@', '<unk>', '@', 'ckt', '.', '</s>']
2025-05-27 15:29:51,773 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 15:29:51,773 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 15:29:51,773 - INFO - joeynmt.training - 	Hypothesis: Und letz<unk> @ tes Jahr zei<unk> @ gte ich diese beiden F<unk> @ olie zeigen , dass die P<unk> @ op<unk> @ ol<unk> @ o<unk> @ gel<unk> @ t , die drei Millionen Jahre , die Größ<unk> @ e des F<unk> @ els der USA , mit den Groß<unk> @ br<unk> @ it<unk> @ an<unk> @ ni<unk> @ en der USA , mit 40 % ver<unk> @ rü<unk> @ ckt .
2025-05-27 15:29:51,773 - INFO - joeynmt.training - Example #1
2025-05-27 15:29:51,773 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 15:29:51,773 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 15:29:51,773 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'Sch@@', '<unk>', '@', 'atten', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ischen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er@@', '<unk>', '@', 'b@@', '<unk>', '@', 'and', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 15:29:51,773 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 15:29:51,773 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 15:29:51,773 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das Sch<unk> @ atten dieses spezi<unk> @ f<unk> @ ischen Problem , weil es nicht die Di<unk> @ ck<unk> @ er<unk> @ b<unk> @ and des E<unk> @ is zeigt .
2025-05-27 15:29:51,773 - INFO - joeynmt.training - Example #2
2025-05-27 15:29:51,773 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 15:29:51,773 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 15:29:51,773 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'der', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'se', 'Weise', 'des', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', 'es', 'im', 'Grunde', 'genommen', 'ist', '.', '</s>']
2025-05-27 15:29:51,773 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 15:29:51,773 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 15:29:51,774 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ e auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in der gew<unk> @ is<unk> @ se Weise des G<unk> @ lo<unk> @ b<unk> @ es im Grunde genommen ist .
2025-05-27 15:29:51,774 - INFO - joeynmt.training - Example #3
2025-05-27 15:29:51,774 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 15:29:51,774 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 15:29:51,774 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'mer', '.', '</s>']
2025-05-27 15:29:51,774 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 15:29:51,774 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 15:29:51,774 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ mer .
2025-05-27 15:29:51,774 - INFO - joeynmt.training - Example #4
2025-05-27 15:29:51,774 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 15:29:51,774 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 15:29:51,774 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'der', 'letzten', '25', 'Jahre', 'passiert', '.', '</s>']
2025-05-27 15:29:51,774 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 15:29:51,774 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 15:29:51,774 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine Ver<unk> @ sion von der letzten 25 Jahre passiert .
2025-05-27 15:30:29,595 - INFO - joeynmt.training - Epoch   7, Step:    21600, Batch Loss:     1.091823, Batch Acc: 0.713864, Tokens per Sec:     4177, Lr: 0.000300
2025-05-27 15:31:05,841 - INFO - joeynmt.training - Epoch   7, Step:    21700, Batch Loss:     1.018929, Batch Acc: 0.712877, Tokens per Sec:     4234, Lr: 0.000300
2025-05-27 15:31:43,580 - INFO - joeynmt.training - Epoch   7, Step:    21800, Batch Loss:     1.054093, Batch Acc: 0.711204, Tokens per Sec:     3997, Lr: 0.000300
2025-05-27 15:32:10,021 - INFO - joeynmt.training - Epoch   7: total training loss 3246.94
2025-05-27 15:32:10,021 - INFO - joeynmt.training - EPOCH 8
2025-05-27 15:32:18,617 - INFO - joeynmt.training - Epoch   8, Step:    21900, Batch Loss:     0.977434, Batch Acc: 0.724905, Tokens per Sec:     4449, Lr: 0.000300
2025-05-27 15:32:54,351 - INFO - joeynmt.training - Epoch   8, Step:    22000, Batch Loss:     1.011623, Batch Acc: 0.725571, Tokens per Sec:     4333, Lr: 0.000300
2025-05-27 15:32:54,352 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 15:32:54,352 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 15:35:41,451 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.20, ppl:   3.32, acc:   0.68, generation: 167.0903[sec], evaluation: 0.0000[sec]
2025-05-27 15:35:41,536 - INFO - joeynmt.helpers - delete models/bpe_4k_model/19000.ckpt
2025-05-27 15:35:41,543 - INFO - joeynmt.training - Example #0
2025-05-27 15:35:41,544 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 15:35:41,544 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 15:35:41,544 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'zei@@', '<unk>', '@', 'gte', 'ich', 'diese', 'beiden', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'li@@', '<unk>', '@', 'ziert', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'ungefähr', 'ungefähr', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 'mor@@', '<unk>', '@', 'gens', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'ten', '.', '</s>']
2025-05-27 15:35:41,544 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 15:35:41,544 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 15:35:41,544 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr zei<unk> @ gte ich diese beiden zwei F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ op<unk> @ ol<unk> @ li<unk> @ ziert , die die drei Millionen Jahre ungefähr ungefähr die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ mor<unk> @ gens der Verein<unk> @ igten Staaten , mit 40 Prozent ver<unk> @ r<unk> @ ück<unk> @ ten .
2025-05-27 15:35:41,544 - INFO - joeynmt.training - Example #1
2025-05-27 15:35:41,544 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 15:35:41,544 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 15:35:41,544 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'wirklich', 'das', 'Sch@@', '<unk>', '@', 'ä@@', '<unk>', '@', 'ss@@', '<unk>', '@', 'el@@', '<unk>', '@', 'proble@@', '<unk>', '@', 'm', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 15:35:41,544 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 15:35:41,544 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 15:35:41,544 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das wirklich das Sch<unk> @ ä<unk> @ ss<unk> @ el<unk> @ proble<unk> @ m , weil es nicht die Di<unk> @ ck<unk> @ er des E<unk> @ is zeigt .
2025-05-27 15:35:41,544 - INFO - joeynmt.training - Example #2
2025-05-27 15:35:41,544 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 15:35:41,544 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 15:35:41,544 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'auf', 'dem', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', '-@@', '<unk>', '@', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 15:35:41,544 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 15:35:41,544 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 15:35:41,544 - INFO - joeynmt.training - 	Hypothesis: Das E<unk> @ is<unk> @ e auf dem Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise das G<unk> @ lo<unk> @ b<unk> @ -<unk> @ Klima<unk> @ sy<unk> @ stem .
2025-05-27 15:35:41,544 - INFO - joeynmt.training - Example #3
2025-05-27 15:35:41,545 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 15:35:41,545 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 15:35:41,545 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'wird', 'in', 'den', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', '.', '</s>']
2025-05-27 15:35:41,545 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 15:35:41,545 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 15:35:41,545 - INFO - joeynmt.training - 	Hypothesis: Es wird in den Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt .
2025-05-27 15:35:41,545 - INFO - joeynmt.training - Example #4
2025-05-27 15:35:41,545 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 15:35:41,545 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 15:35:41,545 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'die', 'letzten', '25', 'Jahre', 'passiert', 'ist', '.', '</s>']
2025-05-27 15:35:41,545 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 15:35:41,545 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 15:35:41,545 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von dem , was die letzten 25 Jahre passiert ist .
2025-05-27 15:36:16,718 - INFO - joeynmt.training - Epoch   8, Step:    22100, Batch Loss:     0.977718, Batch Acc: 0.725177, Tokens per Sec:     4333, Lr: 0.000300
2025-05-27 15:36:53,028 - INFO - joeynmt.training - Epoch   8, Step:    22200, Batch Loss:     1.038612, Batch Acc: 0.722883, Tokens per Sec:     4240, Lr: 0.000300
2025-05-27 15:37:31,035 - INFO - joeynmt.training - Epoch   8, Step:    22300, Batch Loss:     0.962105, Batch Acc: 0.723217, Tokens per Sec:     4017, Lr: 0.000300
2025-05-27 15:38:07,218 - INFO - joeynmt.training - Epoch   8, Step:    22400, Batch Loss:     0.918136, Batch Acc: 0.722793, Tokens per Sec:     4153, Lr: 0.000300
2025-05-27 15:38:42,374 - INFO - joeynmt.training - Epoch   8, Step:    22500, Batch Loss:     0.996644, Batch Acc: 0.721250, Tokens per Sec:     4320, Lr: 0.000300
2025-05-27 15:38:42,374 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 15:38:42,374 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 15:41:29,375 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.20, ppl:   3.31, acc:   0.68, generation: 166.9914[sec], evaluation: 0.0000[sec]
2025-05-27 15:41:29,461 - INFO - joeynmt.helpers - delete models/bpe_4k_model/20000.ckpt
2025-05-27 15:41:29,467 - INFO - joeynmt.training - Example #0
2025-05-27 15:41:29,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 15:41:29,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 15:41:29,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'letz@@', '<unk>', '@', 'tes', 'Jahr', 'zei@@', '<unk>', '@', 'gte', 'ich', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'o@@', '<unk>', '@', '-@@', '<unk>', '@', 'Li@@', '<unk>', '@', 'ste', 'der', 'USA', ',', 'die', 'etwa', 'drei', 'Millionen', 'Jahre', ',', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'oll@@', '<unk>', '@', 'zei@@', '<unk>', '@', 't@@', '<unk>', '@', 'ens', ',', 'die', 'die', 'drei', 'Millionen', 'von', 'der', 'U@@', '<unk>', '@', 'S@@', '<unk>', '@', '-@@', '<unk>', '@', 'F@@', '<unk>', '@', 'olie', 'war', '.', '</s>']
2025-05-27 15:41:29,467 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 15:41:29,467 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 15:41:29,467 - INFO - joeynmt.training - 	Hypothesis: In letz<unk> @ tes Jahr zei<unk> @ gte ich diese beiden F<unk> @ olie zeigen , dass die P<unk> @ op<unk> @ ol<unk> @ o<unk> @ -<unk> @ Li<unk> @ ste der USA , die etwa drei Millionen Jahre , die Größ<unk> @ e des V<unk> @ oll<unk> @ zei<unk> @ t<unk> @ ens , die die drei Millionen von der U<unk> @ S<unk> @ -<unk> @ F<unk> @ olie war .
2025-05-27 15:41:29,467 - INFO - joeynmt.training - Example #1
2025-05-27 15:41:29,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 15:41:29,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 15:41:29,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'ern@@', '<unk>', '@', 'st', 'dieses', 'spezi@@', '<unk>', '@', 'ellen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 's', 'zeigt', '.', '</s>']
2025-05-27 15:41:29,467 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 15:41:29,467 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 15:41:29,467 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das ern<unk> @ st dieses spezi<unk> @ ellen Problem , weil es nicht die Di<unk> @ ck<unk> @ s zeigt .
2025-05-27 15:41:29,468 - INFO - joeynmt.training - Example #2
2025-05-27 15:41:29,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 15:41:29,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 15:41:29,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'ßen', 'ein', 'kl@@', '<unk>', '@', 'eines', 'Her@@', '<unk>', '@', 'z@@', '<unk>', '@', 'ens', '.', '</s>']
2025-05-27 15:41:29,468 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 15:41:29,468 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 15:41:29,468 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser<unk> @ ma<unk> @ ßen ein kl<unk> @ eines Her<unk> @ z<unk> @ ens .
2025-05-27 15:41:29,468 - INFO - joeynmt.training - Example #3
2025-05-27 15:41:29,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 15:41:29,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 15:41:29,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', '.', '</s>']
2025-05-27 15:41:29,468 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 15:41:29,468 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 15:41:29,468 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt .
2025-05-27 15:41:29,468 - INFO - joeynmt.training - Example #4
2025-05-27 15:41:29,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 15:41:29,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 15:41:29,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'gte', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 15:41:29,468 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 15:41:29,468 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 15:41:29,468 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ gte , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert ist .
2025-05-27 15:42:01,121 - INFO - joeynmt.training - Epoch   8, Step:    22600, Batch Loss:     1.007980, Batch Acc: 0.720363, Tokens per Sec:     4762, Lr: 0.000300
2025-05-27 15:42:35,903 - INFO - joeynmt.training - Epoch   8, Step:    22700, Batch Loss:     0.945684, Batch Acc: 0.721928, Tokens per Sec:     4432, Lr: 0.000300
2025-05-27 15:43:07,859 - INFO - joeynmt.training - Epoch   8, Step:    22800, Batch Loss:     1.092502, Batch Acc: 0.724483, Tokens per Sec:     4795, Lr: 0.000300
2025-05-27 15:43:41,760 - INFO - joeynmt.training - Epoch   8, Step:    22900, Batch Loss:     1.106730, Batch Acc: 0.720215, Tokens per Sec:     4498, Lr: 0.000300
2025-05-27 15:44:14,198 - INFO - joeynmt.training - Epoch   8, Step:    23000, Batch Loss:     0.857681, Batch Acc: 0.721962, Tokens per Sec:     4727, Lr: 0.000300
2025-05-27 15:44:14,199 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 15:44:14,199 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 15:47:23,997 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.19, ppl:   3.28, acc:   0.68, generation: 189.7888[sec], evaluation: 0.0000[sec]
2025-05-27 15:47:24,003 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 15:47:24,089 - INFO - joeynmt.helpers - delete models/bpe_4k_model/22000.ckpt
2025-05-27 15:47:24,091 - INFO - joeynmt.training - Example #0
2025-05-27 15:47:24,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 15:47:24,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 15:47:24,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'zwei', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'o@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'ie', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', ',', 'die', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 's', 'war', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'U@@', '<unk>', '@', 'S@@', '<unk>', '@', '-@@', '<unk>', '@', 'L@@', '<unk>', '@', 'än@@', '<unk>', '@', 'ge', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'ten', '.', '</s>']
2025-05-27 15:47:24,091 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 15:47:24,091 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 15:47:24,091 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese beiden zwei Di<unk> @ as zeigen , um zu zeigen , dass die P<unk> @ op<unk> @ o<unk> @ lin<unk> @ ie , die die drei Millionen Jahre , die die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ s war , mit 40 Prozent der USA , mit 40 Prozent der U<unk> @ S<unk> @ -<unk> @ L<unk> @ än<unk> @ ge der USA , mit 40 Prozent ver<unk> @ r<unk> @ ück<unk> @ ten .
2025-05-27 15:47:24,092 - INFO - joeynmt.training - Example #1
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'Problem', 'der', 'ern@@', '<unk>', '@', 'ste', 'Sache', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er@@', '<unk>', '@', 'e@@', '<unk>', '@', 'i', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 15:47:24,092 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 15:47:24,092 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 15:47:24,092 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das Problem der ern<unk> @ ste Sache , weil es nicht die Di<unk> @ ck<unk> @ er<unk> @ e<unk> @ i des E<unk> @ is zeigt .
2025-05-27 15:47:24,092 - INFO - joeynmt.training - Example #2
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'iss@@', '<unk>', '@', 'em', 'Sin@@', '<unk>', '@', 'ne', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', '-@@', '<unk>', '@', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 15:47:24,092 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 15:47:24,092 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 15:47:24,092 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ iss<unk> @ em Sin<unk> @ ne des glob<unk> @ alen Klima<unk> @ -<unk> @ Klima<unk> @ sy<unk> @ stem .
2025-05-27 15:47:24,092 - INFO - joeynmt.training - Example #3
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'im', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', '.', '</s>']
2025-05-27 15:47:24,092 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 15:47:24,092 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 15:47:24,092 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt im S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt .
2025-05-27 15:47:24,092 - INFO - joeynmt.training - Example #4
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 15:47:24,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', '<unk>', '@', 'ehen', 'ist', '.', '</s>']
2025-05-27 15:47:24,093 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 15:47:24,093 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 15:47:24,093 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von dem , was in den letzten 25 Jahren gesch<unk> @ ehen ist .
2025-05-27 15:47:58,981 - INFO - joeynmt.training - Epoch   8, Step:    23100, Batch Loss:     0.996881, Batch Acc: 0.723451, Tokens per Sec:     4472, Lr: 0.000300
2025-05-27 15:48:30,863 - INFO - joeynmt.training - Epoch   8, Step:    23200, Batch Loss:     0.965374, Batch Acc: 0.720583, Tokens per Sec:     4685, Lr: 0.000300
2025-05-27 15:49:05,956 - INFO - joeynmt.training - Epoch   8, Step:    23300, Batch Loss:     1.079838, Batch Acc: 0.717391, Tokens per Sec:     4380, Lr: 0.000300
2025-05-27 15:49:41,794 - INFO - joeynmt.training - Epoch   8, Step:    23400, Batch Loss:     1.059497, Batch Acc: 0.718633, Tokens per Sec:     4277, Lr: 0.000300
2025-05-27 15:50:17,172 - INFO - joeynmt.training - Epoch   8, Step:    23500, Batch Loss:     1.026572, Batch Acc: 0.716482, Tokens per Sec:     4369, Lr: 0.000300
2025-05-27 15:50:17,173 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 15:50:17,173 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 15:53:01,085 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.19, ppl:   3.30, acc:   0.68, generation: 163.9029[sec], evaluation: 0.0000[sec]
2025-05-27 15:53:01,187 - INFO - joeynmt.helpers - delete models/bpe_4k_model/20500.ckpt
2025-05-27 15:53:01,189 - INFO - joeynmt.training - Example #0
2025-05-27 15:53:01,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 15:53:01,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 15:53:01,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'letzten', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', '-@@', '<unk>', '@', 'S@@', '<unk>', '@', '-@@', '<unk>', '@', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'in', 'der', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 's', 'war', ',', 'die', 'die', 'letzten', 'drei', 'Millionen', 'von', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'ten', '.', '</s>']
2025-05-27 15:53:01,189 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 15:53:01,189 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 15:53:01,189 - INFO - joeynmt.training - 	Hypothesis: In letzten Jahr habe ich diese beiden Di<unk> @ as zeigen , um zu zeigen , dass die P<unk> @ oo<unk> @ l<unk> @ -<unk> @ S<unk> @ -<unk> @ F<unk> @ olie , die die drei Millionen Jahre in der Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ s war , die die letzten drei Millionen von der Verein<unk> @ igten Staaten , mit 40 Prozent ver<unk> @ r<unk> @ ück<unk> @ ten .
2025-05-27 15:53:01,189 - INFO - joeynmt.training - Example #1
2025-05-27 15:53:01,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 15:53:01,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 15:53:01,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'wirklich', 'die', 'ern@@', '<unk>', '@', 'st', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ische', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er@@', '<unk>', '@', '-@@', '<unk>', '@', 'Problem', 'ist', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er@@', '<unk>', '@', '-@@', '<unk>', '@', 'F@@', '<unk>', '@', 'eld', '.', '</s>']
2025-05-27 15:53:01,189 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 15:53:01,189 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 15:53:01,189 - INFO - joeynmt.training - 	Hypothesis: Aber das ist wirklich die ern<unk> @ st dieses spezi<unk> @ f<unk> @ ische Problem , weil es nicht die Di<unk> @ ck<unk> @ er<unk> @ -<unk> @ Problem ist , weil es nicht die Di<unk> @ ck<unk> @ er<unk> @ -<unk> @ F<unk> @ eld .
2025-05-27 15:53:01,189 - INFO - joeynmt.training - Example #2
2025-05-27 15:53:01,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 15:53:01,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 15:53:01,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', ',', 'das', 'G@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'ff', ',', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', '-@@', '<unk>', '@', 'Kl@@', '<unk>', '@', 'im@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-27 15:53:01,190 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 15:53:01,190 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 15:53:01,190 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise , das G<unk> @ ri<unk> @ ff , das G<unk> @ lo<unk> @ b<unk> @ -<unk> @ Kl<unk> @ im<unk> @ a .
2025-05-27 15:53:01,190 - INFO - joeynmt.training - Example #3
2025-05-27 15:53:01,190 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 15:53:01,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 15:53:01,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'mer', '.', '</s>']
2025-05-27 15:53:01,190 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 15:53:01,190 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 15:53:01,190 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und K<unk> @ ri<unk> @ m<unk> @ mer .
2025-05-27 15:53:01,190 - INFO - joeynmt.training - Example #4
2025-05-27 15:53:01,190 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 15:53:01,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 15:53:01,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'möchte', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2025-05-27 15:53:01,191 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 15:53:01,191 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 15:53:01,191 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zeigen möchte , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von was in den letzten 25 Jahren passiert .
2025-05-27 15:53:39,738 - INFO - joeynmt.training - Epoch   8, Step:    23600, Batch Loss:     1.038364, Batch Acc: 0.718928, Tokens per Sec:     3997, Lr: 0.000300
2025-05-27 15:54:13,533 - INFO - joeynmt.training - Epoch   8, Step:    23700, Batch Loss:     1.025226, Batch Acc: 0.718767, Tokens per Sec:     4493, Lr: 0.000300
2025-05-27 15:54:48,660 - INFO - joeynmt.training - Epoch   8, Step:    23800, Batch Loss:     0.977479, Batch Acc: 0.719758, Tokens per Sec:     4284, Lr: 0.000300
2025-05-27 15:55:21,905 - INFO - joeynmt.training - Epoch   8, Step:    23900, Batch Loss:     1.004755, Batch Acc: 0.719166, Tokens per Sec:     4718, Lr: 0.000300
2025-05-27 15:55:57,789 - INFO - joeynmt.training - Epoch   8, Step:    24000, Batch Loss:     1.080019, Batch Acc: 0.717989, Tokens per Sec:     4228, Lr: 0.000300
2025-05-27 15:55:57,790 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 15:55:57,790 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 15:58:56,727 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.19, ppl:   3.29, acc:   0.68, generation: 178.9280[sec], evaluation: 0.0000[sec]
2025-05-27 15:58:56,826 - INFO - joeynmt.helpers - delete models/bpe_4k_model/22500.ckpt
2025-05-27 15:58:56,830 - INFO - joeynmt.training - Example #0
2025-05-27 15:58:56,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 15:58:56,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 15:58:56,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'hat', 'ich', 'diese', 'beiden', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'ie', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 's', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 's', 'war', '.', '</s>']
2025-05-27 15:58:56,830 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 15:58:56,830 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 15:58:56,830 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr hat ich diese beiden zwei F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ oo<unk> @ lin<unk> @ ie , die die drei Millionen Jahre etwa die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ s , die die drei Millionen Jahre etwa die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ s war .
2025-05-27 15:58:56,830 - INFO - joeynmt.training - Example #1
2025-05-27 15:58:56,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 15:58:56,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 15:58:56,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'das', ',', 'was', 'die', 'ern@@', '<unk>', '@', 'ste', 'Sache', 'ist', ',', 'die', 'ern@@', '<unk>', '@', 'ste', 'Sache', ',', 'weil', 'es', 'nicht', 'die', 'ern@@', '<unk>', '@', 'st', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 15:58:56,830 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 15:58:56,830 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 15:58:56,831 - INFO - joeynmt.training - 	Hypothesis: Aber das ist das , was die ern<unk> @ ste Sache ist , die ern<unk> @ ste Sache , weil es nicht die ern<unk> @ st des E<unk> @ is zeigt .
2025-05-27 15:58:56,831 - INFO - joeynmt.training - Example #2
2025-05-27 15:58:56,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 15:58:56,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 15:58:56,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'icht', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'er@@', '<unk>', '@', '-@@', '<unk>', '@', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 15:58:56,831 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 15:58:56,831 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 15:58:56,831 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sch<unk> @ icht auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise des glob<unk> @ alen Klima<unk> @ er<unk> @ -<unk> @ Klima<unk> @ sy<unk> @ stem .
2025-05-27 15:58:56,831 - INFO - joeynmt.training - Example #3
2025-05-27 15:58:56,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 15:58:56,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 15:58:56,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 15:58:56,831 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 15:58:56,831 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 15:58:56,831 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 15:58:56,831 - INFO - joeynmt.training - Example #4
2025-05-27 15:58:56,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 15:58:56,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 15:58:56,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'der', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'gte', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'es', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 15:58:56,831 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 15:58:56,831 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 15:58:56,831 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , der ich Ihnen zei<unk> @ gte , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von dem , was es in den letzten 25 Jahren passiert ist .
2025-05-27 15:59:31,648 - INFO - joeynmt.training - Epoch   8, Step:    24100, Batch Loss:     1.025491, Batch Acc: 0.721842, Tokens per Sec:     4433, Lr: 0.000300
2025-05-27 16:00:09,449 - INFO - joeynmt.training - Epoch   8, Step:    24200, Batch Loss:     1.078450, Batch Acc: 0.717737, Tokens per Sec:     4083, Lr: 0.000300
2025-05-27 16:00:43,840 - INFO - joeynmt.training - Epoch   8, Step:    24300, Batch Loss:     0.994837, Batch Acc: 0.717374, Tokens per Sec:     4528, Lr: 0.000300
2025-05-27 16:01:17,366 - INFO - joeynmt.training - Epoch   8, Step:    24400, Batch Loss:     0.967119, Batch Acc: 0.722210, Tokens per Sec:     4562, Lr: 0.000300
2025-05-27 16:01:52,583 - INFO - joeynmt.training - Epoch   8, Step:    24500, Batch Loss:     0.968710, Batch Acc: 0.719572, Tokens per Sec:     4292, Lr: 0.000300
2025-05-27 16:01:52,583 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 16:01:52,583 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:05:10,067 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.19, ppl:   3.27, acc:   0.68, generation: 197.4736[sec], evaluation: 0.0000[sec]
2025-05-27 16:05:10,072 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 16:05:10,166 - INFO - joeynmt.helpers - delete models/bpe_4k_model/21000.ckpt
2025-05-27 16:05:10,169 - INFO - joeynmt.training - Example #0
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'letz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'beiden', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', 'it@@', '<unk>', '@', 'el@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', ',', 'die', 'drei', 'Millionen', 'Jahre', ',', 'die', 'in', 'etwa', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'äl@@', '<unk>', '@', 'le', 'des', 'V@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ers', 'war', '.', '</s>']
2025-05-27 16:05:10,170 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 16:05:10,170 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 16:05:10,170 - INFO - joeynmt.training - 	Hypothesis: Und letz<unk> @ tes Jahr habe ich diese beiden beiden Di<unk> @ as zeigen , um die P<unk> @ oo<unk> @ l<unk> @ it<unk> @ el<unk> @ sk<unk> @ a<unk> @ p , die drei Millionen Jahre , die in etwa 40 Prozent der USA , mit 40 Prozent der USA , mit 40 Prozent der F<unk> @ äl<unk> @ le des V<unk> @ at<unk> @ ers war .
2025-05-27 16:05:10,170 - INFO - joeynmt.training - Example #1
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'ern@@', '<unk>', '@', 'st', 'dieses', 'spezi@@', '<unk>', '@', 'ellen', 'Problem', ',', 'weil', 'es', 'nicht', 'den', 'Dur@@', '<unk>', '@', 'ch@@', '<unk>', '@', 'bru@@', '<unk>', '@', 'ch', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 16:05:10,170 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 16:05:10,170 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 16:05:10,170 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die ern<unk> @ st dieses spezi<unk> @ ellen Problem , weil es nicht den Dur<unk> @ ch<unk> @ bru<unk> @ ch des E<unk> @ is zeigt .
2025-05-27 16:05:10,170 - INFO - joeynmt.training - Example #2
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'en@@', '<unk>', '@', 'k', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 's', '.', '</s>']
2025-05-27 16:05:10,170 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 16:05:10,170 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 16:05:10,170 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sch<unk> @ en<unk> @ k auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise des glob<unk> @ alen Klima<unk> @ wandel<unk> @ s .
2025-05-27 16:05:10,170 - INFO - joeynmt.training - Example #3
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 16:05:10,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 16:05:10,171 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 16:05:10,171 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 16:05:10,171 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 16:05:10,171 - INFO - joeynmt.training - Example #4
2025-05-27 16:05:10,171 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 16:05:10,171 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 16:05:10,171 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 16:05:10,171 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 16:05:10,171 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 16:05:10,171 - INFO - joeynmt.training - 	Hypothesis: Und die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von was in den letzten 25 Jahren passiert ist .
2025-05-27 16:05:48,279 - INFO - joeynmt.training - Epoch   8, Step:    24600, Batch Loss:     1.144891, Batch Acc: 0.714296, Tokens per Sec:     4075, Lr: 0.000300
2025-05-27 16:06:22,497 - INFO - joeynmt.training - Epoch   8, Step:    24700, Batch Loss:     0.998748, Batch Acc: 0.716893, Tokens per Sec:     4383, Lr: 0.000300
2025-05-27 16:07:01,440 - INFO - joeynmt.training - Epoch   8, Step:    24800, Batch Loss:     1.045812, Batch Acc: 0.716811, Tokens per Sec:     4000, Lr: 0.000300
2025-05-27 16:07:37,147 - INFO - joeynmt.training - Epoch   8, Step:    24900, Batch Loss:     1.040509, Batch Acc: 0.715018, Tokens per Sec:     4313, Lr: 0.000300
2025-05-27 16:08:10,282 - INFO - joeynmt.training - Epoch   8, Step:    25000, Batch Loss:     0.912424, Batch Acc: 0.718843, Tokens per Sec:     4676, Lr: 0.000300
2025-05-27 16:08:10,282 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 16:08:10,282 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:10:44,951 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.18, ppl:   3.27, acc:   0.68, generation: 154.6594[sec], evaluation: 0.0000[sec]
2025-05-27 16:10:44,955 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 16:10:45,043 - INFO - joeynmt.helpers - delete models/bpe_4k_model/23500.ckpt
2025-05-27 16:10:45,044 - INFO - joeynmt.training - Example #0
2025-05-27 16:10:45,045 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 16:10:45,045 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 16:10:45,045 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'den', 'letzten', 'letzten', 'Jahr@@', '<unk>', '@', 'es', ',', 'als', 'ich', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'li@@', '<unk>', '@', 'ziert', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'ungefähr', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ers', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'Groß@@', '<unk>', '@', 'teil', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'äl@@', '<unk>', '@', 'le', 'des', 'V@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ers', 'war', '.', '</s>']
2025-05-27 16:10:45,045 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 16:10:45,045 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 16:10:45,045 - INFO - joeynmt.training - 	Hypothesis: In den letzten letzten Jahr<unk> @ es , als ich diese beiden F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ op<unk> @ ol<unk> @ li<unk> @ ziert , die die drei Millionen Jahre ungefähr die Größ<unk> @ e des V<unk> @ at<unk> @ ers der USA , mit 40 Prozent der Groß<unk> @ teil der USA , mit 40 Prozent der F<unk> @ äl<unk> @ le des V<unk> @ at<unk> @ ers war .
2025-05-27 16:10:45,045 - INFO - joeynmt.training - Example #1
2025-05-27 16:10:45,045 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 16:10:45,045 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 16:10:45,045 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'ern@@', '<unk>', '@', 'st', 'dieses', 'spezi@@', '<unk>', '@', 'ellen', 'Problem', ',', 'weil', 'es', 'nicht', 'den', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er', 'der', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 16:10:45,045 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 16:10:45,045 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 16:10:45,045 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die ern<unk> @ st dieses spezi<unk> @ ellen Problem , weil es nicht den Di<unk> @ ck<unk> @ er der E<unk> @ is zeigt .
2025-05-27 16:10:45,045 - INFO - joeynmt.training - Example #2
2025-05-27 16:10:45,045 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 16:10:45,045 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 16:10:45,045 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'im', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', ',', 'das', 'ist', 'ein', 'kl@@', '<unk>', '@', 'eines', 'Her@@', '<unk>', '@', 'z@@', '<unk>', '@', 'ens', 'glob@@', '<unk>', '@', 'al', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 16:10:45,046 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 16:10:45,046 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 16:10:45,046 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ sk<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist im Nor<unk> @ d<unk> @ po<unk> @ l , das ist ein kl<unk> @ eines Her<unk> @ z<unk> @ ens glob<unk> @ al Klima<unk> @ sy<unk> @ stem .
2025-05-27 16:10:45,046 - INFO - joeynmt.training - Example #3
2025-05-27 16:10:45,046 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 16:10:45,046 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 16:10:45,046 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'wird', 'in', 'den', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 16:10:45,046 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 16:10:45,046 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 16:10:45,046 - INFO - joeynmt.training - 	Hypothesis: Es wird in den Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 16:10:45,046 - INFO - joeynmt.training - Example #4
2025-05-27 16:10:45,046 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 16:10:45,046 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 16:10:45,046 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 16:10:45,046 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 16:10:45,046 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 16:10:45,046 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von was in den letzten 25 Jahren passiert ist .
2025-05-27 16:10:45,051 - INFO - joeynmt.training - Epoch   8: total training loss 3168.26
2025-05-27 16:10:45,051 - INFO - joeynmt.training - EPOCH 9
2025-05-27 16:11:18,142 - INFO - joeynmt.training - Epoch   9, Step:    25100, Batch Loss:     0.969989, Batch Acc: 0.729926, Tokens per Sec:     4591, Lr: 0.000300
2025-05-27 16:11:51,417 - INFO - joeynmt.training - Epoch   9, Step:    25200, Batch Loss:     0.953656, Batch Acc: 0.731878, Tokens per Sec:     4668, Lr: 0.000300
2025-05-27 16:12:24,549 - INFO - joeynmt.training - Epoch   9, Step:    25300, Batch Loss:     0.951278, Batch Acc: 0.729278, Tokens per Sec:     4579, Lr: 0.000300
2025-05-27 16:13:00,360 - INFO - joeynmt.training - Epoch   9, Step:    25400, Batch Loss:     0.932881, Batch Acc: 0.730262, Tokens per Sec:     4341, Lr: 0.000300
2025-05-27 16:13:35,298 - INFO - joeynmt.training - Epoch   9, Step:    25500, Batch Loss:     0.994803, Batch Acc: 0.729846, Tokens per Sec:     4349, Lr: 0.000300
2025-05-27 16:13:35,299 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 16:13:35,299 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:16:49,436 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.19, ppl:   3.29, acc:   0.68, generation: 194.1267[sec], evaluation: 0.0000[sec]
2025-05-27 16:16:49,534 - INFO - joeynmt.helpers - delete models/bpe_4k_model/21500.ckpt
2025-05-27 16:16:49,537 - INFO - joeynmt.training - Example #0
2025-05-27 16:16:49,537 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 16:16:49,537 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 16:16:49,537 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', '-@@', '<unk>', '@', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'drei', 'Millionen', 'Jahren', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'F@@', '<unk>', '@', 'est@@', '<unk>', '@', '-@@', '<unk>', '@', 'C@@', '<unk>', '@', 'han@@', '<unk>', '@', 'ce', 'hatte', ',', 'die', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'F@@', '<unk>', '@', 'el@@', '<unk>', '@', 'syste@@', '<unk>', '@', 'ms', 'war', '.', '</s>']
2025-05-27 16:16:49,537 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 16:16:49,537 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 16:16:49,537 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese beiden Di<unk> @ as zeigen , um die P<unk> @ op<unk> @ -<unk> @ F<unk> @ olie , die drei Millionen Jahren etwa die Größ<unk> @ e des F<unk> @ est<unk> @ -<unk> @ C<unk> @ han<unk> @ ce hatte , die drei Millionen Jahren die Größ<unk> @ e des F<unk> @ el<unk> @ syste<unk> @ ms war .
2025-05-27 16:16:49,537 - INFO - joeynmt.training - Example #1
2025-05-27 16:16:49,537 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 16:16:49,537 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 16:16:49,537 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'z@@', '<unk>', '@', 'ung', 'dieses', 'spezi@@', '<unk>', '@', 'ell@@', '<unk>', '@', 'es', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 16:16:49,537 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 16:16:49,537 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 16:16:49,538 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ z<unk> @ ung dieses spezi<unk> @ ell<unk> @ es Problem , weil es nicht die Di<unk> @ kte des E<unk> @ is zeigt .
2025-05-27 16:16:49,538 - INFO - joeynmt.training - Example #2
2025-05-27 16:16:49,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 16:16:49,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 16:16:49,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'en@@', '<unk>', '@', 'k', 'auf', 'dem', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'ßen', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 's', '.', '</s>']
2025-05-27 16:16:49,538 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 16:16:49,538 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 16:16:49,538 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ sch<unk> @ en<unk> @ k auf dem Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser<unk> @ ma<unk> @ ßen des glob<unk> @ alen Klima<unk> @ wandel<unk> @ s .
2025-05-27 16:16:49,538 - INFO - joeynmt.training - Example #3
2025-05-27 16:16:49,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 16:16:49,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 16:16:49,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'mer@@', '<unk>', '@', 'n', '.', '</s>']
2025-05-27 16:16:49,538 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 16:16:49,538 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 16:16:49,538 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Win<unk> @ ter und K<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und K<unk> @ ri<unk> @ m<unk> @ mer<unk> @ n .
2025-05-27 16:16:49,538 - INFO - joeynmt.training - Example #4
2025-05-27 16:16:49,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 16:16:49,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 16:16:49,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'möchte', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Geschichte', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 16:16:49,538 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 16:16:49,538 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 16:16:49,538 - INFO - joeynmt.training - 	Hypothesis: Und die nächste F<unk> @ olie , die ich Ihnen zeigen möchte , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Geschichte von dem , was in den letzten 25 Jahren passiert ist .
2025-05-27 16:17:23,918 - INFO - joeynmt.training - Epoch   9, Step:    25600, Batch Loss:     1.043571, Batch Acc: 0.727210, Tokens per Sec:     4399, Lr: 0.000300
2025-05-27 16:18:01,566 - INFO - joeynmt.training - Epoch   9, Step:    25700, Batch Loss:     0.943912, Batch Acc: 0.727476, Tokens per Sec:     4043, Lr: 0.000300
2025-05-27 16:18:37,855 - INFO - joeynmt.training - Epoch   9, Step:    25800, Batch Loss:     1.103613, Batch Acc: 0.726924, Tokens per Sec:     4263, Lr: 0.000300
2025-05-27 16:19:12,738 - INFO - joeynmt.training - Epoch   9, Step:    25900, Batch Loss:     1.088797, Batch Acc: 0.729205, Tokens per Sec:     4450, Lr: 0.000300
2025-05-27 16:19:46,807 - INFO - joeynmt.training - Epoch   9, Step:    26000, Batch Loss:     0.931649, Batch Acc: 0.725765, Tokens per Sec:     4566, Lr: 0.000300
2025-05-27 16:19:46,807 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 16:19:46,807 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:22:25,873 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.19, ppl:   3.28, acc:   0.68, generation: 159.0565[sec], evaluation: 0.0000[sec]
2025-05-27 16:22:25,977 - INFO - joeynmt.helpers - delete models/bpe_4k_model/25500.ckpt
2025-05-27 16:22:25,978 - INFO - joeynmt.helpers - delete /Users/merilinsilva/Documents/SemestersUZH/4thSem/MT/exercises/msousa_spareek_mt_exercise_04/mt-exercise-4/models/bpe_4k_model/25500.ckpt
2025-05-27 16:22:25,978 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/merilinsilva/Documents/SemestersUZH/4thSem/MT/exercises/msousa_spareek_mt_exercise_04/mt-exercise-4/models/bpe_4k_model/25500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/merilinsilva/Documents/SemestersUZH/4thSem/MT/exercises/msousa_spareek_mt_exercise_04/mt-exercise-4/models/bpe_4k_model/25500.ckpt')
2025-05-27 16:22:25,981 - INFO - joeynmt.training - Example #0
2025-05-27 16:22:25,981 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 16:22:25,981 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 16:22:25,981 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'F@@', '<unk>', '@', 'olie', 'ge@@', '<unk>', '@', 'zeigt', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ul@@', '<unk>', '@', 'ar@@', '<unk>', '@', '-@@', '<unk>', '@', 'Sk@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', ',', 'die', 'drei', 'Millionen', 'Jahre', 'alt', 'hatte', ',', 'die', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 'mor@@', '<unk>', '@', 'gens', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'ten', '.', '</s>']
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese beiden F<unk> @ olie ge<unk> @ zeigt , dass die P<unk> @ op<unk> @ ul<unk> @ ar<unk> @ -<unk> @ Sk<unk> @ a<unk> @ p , die drei Millionen Jahre alt hatte , die die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ mor<unk> @ gens der USA , mit 40 Prozent ver<unk> @ r<unk> @ ück<unk> @ ten .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - Example #1
2025-05-27 16:22:25,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 16:22:25,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 16:22:25,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'ern@@', '<unk>', '@', 'st', 'dieses', 'spezi@@', '<unk>', '@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'den', 'D@@', '<unk>', '@', 'ing', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die ern<unk> @ st dieses spezi<unk> @ elle Problem , weil es nicht den D<unk> @ ing des E<unk> @ is zeigt .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - Example #2
2025-05-27 16:22:25,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 16:22:25,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 16:22:25,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'im', 'Grunde', 'das', 'kl@@', '<unk>', '@', 'eines', 'Her@@', '<unk>', '@', 'zen', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ k<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist im Grunde das kl<unk> @ eines Her<unk> @ zen des glob<unk> @ alen Klima<unk> @ sy<unk> @ stem .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - Example #3
2025-05-27 16:22:25,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 16:22:25,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 16:22:25,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Sie', 'werden', 'in', 'den', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - 	Hypothesis: Sie werden in den Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 16:22:25,982 - INFO - joeynmt.training - Example #4
2025-05-27 16:22:25,983 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 16:22:25,983 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 16:22:25,983 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'gte', ',', 'dass', 'sie', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'die', 'ich', 'Ihnen', 'ein', 'sch@@', '<unk>', '@', 'la@@', '<unk>', '@', 'ss', 'ist', ',', 'dass', 'die', 'nächste', 'F@@', '<unk>', '@', 'olie', 'passiert', '.', '</s>']
2025-05-27 16:22:25,983 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 16:22:25,983 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 16:22:25,983 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion , die ich Ihnen zei<unk> @ gte , dass sie eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te , die ich Ihnen zei<unk> @ ge , die ich Ihnen zei<unk> @ ge , die ich Ihnen ein sch<unk> @ la<unk> @ ss ist , dass die nächste F<unk> @ olie passiert .
2025-05-27 16:22:59,685 - INFO - joeynmt.training - Epoch   9, Step:    26100, Batch Loss:     1.143439, Batch Acc: 0.726950, Tokens per Sec:     4471, Lr: 0.000300
2025-05-27 16:23:32,078 - INFO - joeynmt.training - Epoch   9, Step:    26200, Batch Loss:     0.998271, Batch Acc: 0.727173, Tokens per Sec:     4682, Lr: 0.000300
2025-05-27 16:24:05,018 - INFO - joeynmt.training - Epoch   9, Step:    26300, Batch Loss:     0.901790, Batch Acc: 0.724399, Tokens per Sec:     4603, Lr: 0.000300
2025-05-27 16:24:39,231 - INFO - joeynmt.training - Epoch   9, Step:    26400, Batch Loss:     0.962052, Batch Acc: 0.724643, Tokens per Sec:     4422, Lr: 0.000300
2025-05-27 16:25:12,000 - INFO - joeynmt.training - Epoch   9, Step:    26500, Batch Loss:     1.019578, Batch Acc: 0.725254, Tokens per Sec:     4682, Lr: 0.000300
2025-05-27 16:25:12,002 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 16:25:12,002 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:28:25,782 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.18, ppl:   3.27, acc:   0.68, generation: 193.7700[sec], evaluation: 0.0000[sec]
2025-05-27 16:28:25,871 - INFO - joeynmt.helpers - delete models/bpe_4k_model/24000.ckpt
2025-05-27 16:28:25,876 - INFO - joeynmt.training - Example #0
2025-05-27 16:28:25,876 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 16:28:25,876 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 16:28:25,876 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'den', 'letzten', 'letzten', 'Jahr@@', '<unk>', '@', 'zeh@@', '<unk>', '@', 'nte', 'ich', 'diese', 'zwei', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ul@@', '<unk>', '@', 'ar@@', '<unk>', '@', '-@@', '<unk>', '@', 'F@@', '<unk>', '@', 'e@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'lang', 'des', 'V@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ers', 'hatte', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'est@@', '<unk>', '@', 'land', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ück@@', '<unk>', '@', 'ten', '.', '</s>']
2025-05-27 16:28:25,876 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 16:28:25,876 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 16:28:25,876 - INFO - joeynmt.training - 	Hypothesis: In den letzten letzten Jahr<unk> @ zeh<unk> @ nte ich diese zwei Di<unk> @ as zeigen , um zu zeigen , dass die P<unk> @ op<unk> @ ul<unk> @ ar<unk> @ -<unk> @ F<unk> @ e<unk> @ sy<unk> @ stem , die die drei Millionen Jahre lang des V<unk> @ at<unk> @ ers hatte , die die drei Millionen Jahre der USA , mit 40 Prozent der USA , mit 40 Prozent der F<unk> @ est<unk> @ land , mit 40 Prozent der USA , mit 40 Prozent ver<unk> @ r<unk> @ ück<unk> @ ten .
2025-05-27 16:28:25,876 - INFO - joeynmt.training - Example #1
2025-05-27 16:28:25,876 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 16:28:25,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 16:28:25,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'Problem', 'der', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er@@', '<unk>', '@', '-@@', '<unk>', '@', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', '<unk>', '@', 'rit@@', '<unk>', '@', 'te', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 16:28:25,877 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 16:28:25,877 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 16:28:25,877 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das Problem der Di<unk> @ ck<unk> @ er<unk> @ -<unk> @ Problem , weil es nicht die D<unk> @ rit<unk> @ te des E<unk> @ is zeigt .
2025-05-27 16:28:25,877 - INFO - joeynmt.training - Example #2
2025-05-27 16:28:25,877 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 16:28:25,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 16:28:25,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'das', 'g@@', '<unk>', '@', 'il@@', '<unk>', '@', 't', 'unser', 'glob@@', '<unk>', '@', 'al@@', '<unk>', '@', 'es', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 16:28:25,877 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 16:28:25,877 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 16:28:25,877 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ k<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise das g<unk> @ il<unk> @ t unser glob<unk> @ al<unk> @ es Klima<unk> @ sy<unk> @ stem .
2025-05-27 16:28:25,877 - INFO - joeynmt.training - Example #3
2025-05-27 16:28:25,877 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 16:28:25,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 16:28:25,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'die', 'N@@', '<unk>', '@', 'amen', 'aus', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 16:28:25,877 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 16:28:25,877 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 16:28:25,877 - INFO - joeynmt.training - 	Hypothesis: Es ist die N<unk> @ amen aus der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 16:28:25,877 - INFO - joeynmt.training - Example #4
2025-05-27 16:28:25,877 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 16:28:25,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 16:28:25,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'der', 'ich', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 16:28:25,878 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 16:28:25,878 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 16:28:25,878 - INFO - joeynmt.training - 	Hypothesis: Das nächste F<unk> @ olie , der ich zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert ist .
2025-05-27 16:28:58,644 - INFO - joeynmt.training - Epoch   9, Step:    26600, Batch Loss:     1.030352, Batch Acc: 0.721842, Tokens per Sec:     4750, Lr: 0.000300
2025-05-27 16:29:32,569 - INFO - joeynmt.training - Epoch   9, Step:    26700, Batch Loss:     0.990356, Batch Acc: 0.723216, Tokens per Sec:     4510, Lr: 0.000300
2025-05-27 16:30:04,735 - INFO - joeynmt.training - Epoch   9, Step:    26800, Batch Loss:     1.001486, Batch Acc: 0.723593, Tokens per Sec:     4665, Lr: 0.000300
2025-05-27 16:30:38,309 - INFO - joeynmt.training - Epoch   9, Step:    26900, Batch Loss:     0.968639, Batch Acc: 0.722236, Tokens per Sec:     4569, Lr: 0.000300
2025-05-27 16:31:11,966 - INFO - joeynmt.training - Epoch   9, Step:    27000, Batch Loss:     0.975044, Batch Acc: 0.723227, Tokens per Sec:     4581, Lr: 0.000300
2025-05-27 16:31:11,966 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 16:31:11,967 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:34:06,065 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.18, ppl:   3.27, acc:   0.68, generation: 174.0897[sec], evaluation: 0.0000[sec]
2025-05-27 16:34:06,069 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 16:34:06,164 - INFO - joeynmt.helpers - delete models/bpe_4k_model/23000.ckpt
2025-05-27 16:34:06,167 - INFO - joeynmt.training - Example #0
2025-05-27 16:34:06,167 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 16:34:06,167 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 16:34:06,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', '<unk>', '@', 'as', ',', 'um', 'die', 'A@@', '<unk>', '@', 'pp@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'ten', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 'mor@@', '<unk>', '@', 'gens', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 's', 'war', '.', '</s>']
2025-05-27 16:34:06,167 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 16:34:06,167 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 16:34:06,167 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese beiden Di<unk> @ as , um die A<unk> @ pp<unk> @ ar<unk> @ ten , die die drei Millionen Jahre etwa die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ mor<unk> @ gens des V<unk> @ öl<unk> @ ker<unk> @ s war .
2025-05-27 16:34:06,167 - INFO - joeynmt.training - Example #1
2025-05-27 16:34:06,167 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 16:34:06,167 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 16:34:06,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'tatsächlich', 'die', 'ern@@', '<unk>', '@', 'st', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ische', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 16:34:06,167 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 16:34:06,167 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 16:34:06,167 - INFO - joeynmt.training - 	Hypothesis: Aber das ist tatsächlich die ern<unk> @ st dieses spezi<unk> @ f<unk> @ ische Problem , weil es nicht die Di<unk> @ ck<unk> @ er des E<unk> @ is zeigt .
2025-05-27 16:34:06,167 - INFO - joeynmt.training - Example #2
2025-05-27 16:34:06,167 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 16:34:06,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 16:34:06,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', 'us', 'des', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'ck@@', '<unk>', '@', 's', '.', '</s>']
2025-05-27 16:34:06,168 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 16:34:06,168 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 16:34:06,168 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ e auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise das G<unk> @ lo<unk> @ b<unk> @ us des G<unk> @ lo<unk> @ ck<unk> @ s .
2025-05-27 16:34:06,168 - INFO - joeynmt.training - Example #3
2025-05-27 16:34:06,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 16:34:06,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 16:34:06,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'ein', 'Win@@', '<unk>', '@', 'ter', ',', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 16:34:06,168 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 16:34:06,168 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 16:34:06,168 - INFO - joeynmt.training - 	Hypothesis: Es ist ein Win<unk> @ ter , und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ er .
2025-05-27 16:34:06,168 - INFO - joeynmt.training - Example #4
2025-05-27 16:34:06,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 16:34:06,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 16:34:06,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', ',', 'die', 'ich', 'Ihnen', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', '<unk>', '@', 'ehen', 'hat', '.', '</s>']
2025-05-27 16:34:06,168 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 16:34:06,168 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 16:34:06,168 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te , die ich Ihnen in den letzten 25 Jahren gesch<unk> @ ehen hat .
2025-05-27 16:34:38,937 - INFO - joeynmt.training - Epoch   9, Step:    27100, Batch Loss:     1.071269, Batch Acc: 0.722639, Tokens per Sec:     4670, Lr: 0.000300
2025-05-27 16:35:11,528 - INFO - joeynmt.training - Epoch   9, Step:    27200, Batch Loss:     1.003055, Batch Acc: 0.722056, Tokens per Sec:     4821, Lr: 0.000300
2025-05-27 16:35:46,860 - INFO - joeynmt.training - Epoch   9, Step:    27300, Batch Loss:     0.942692, Batch Acc: 0.722599, Tokens per Sec:     4383, Lr: 0.000300
2025-05-27 16:36:20,343 - INFO - joeynmt.training - Epoch   9, Step:    27400, Batch Loss:     1.015062, Batch Acc: 0.724929, Tokens per Sec:     4551, Lr: 0.000300
2025-05-27 16:36:56,342 - INFO - joeynmt.training - Epoch   9, Step:    27500, Batch Loss:     1.055831, Batch Acc: 0.721532, Tokens per Sec:     4239, Lr: 0.000300
2025-05-27 16:36:56,343 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 16:36:56,343 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:40:02,932 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.18, ppl:   3.25, acc:   0.68, generation: 186.5798[sec], evaluation: 0.0000[sec]
2025-05-27 16:40:02,937 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 16:40:03,030 - INFO - joeynmt.helpers - delete models/bpe_4k_model/26000.ckpt
2025-05-27 16:40:03,032 - INFO - joeynmt.training - Example #0
2025-05-27 16:40:03,032 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 16:40:03,032 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 16:40:03,032 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'ie', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'in', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ers', 'der', 'USA', 'hatte', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'est@@', '<unk>', '@', 'ung', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'Ver@@', '<unk>', '@', 'rü@@', '<unk>', '@', 'mm@@', '<unk>', '@', 'eln', 'war', '.', '</s>']
2025-05-27 16:40:03,032 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 16:40:03,032 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 16:40:03,032 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese zwei Di<unk> @ as zeigen , um zu zeigen , dass die die P<unk> @ oo<unk> @ l<unk> @ lin<unk> @ ie , die die drei Millionen Jahre in etwa die Größ<unk> @ e des V<unk> @ at<unk> @ ers der USA hatte , mit 40 Prozent der F<unk> @ est<unk> @ ung der USA , mit 40 Prozent der Ver<unk> @ rü<unk> @ mm<unk> @ eln war .
2025-05-27 16:40:03,032 - INFO - joeynmt.training - Example #1
2025-05-27 16:40:03,032 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 16:40:03,032 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 16:40:03,032 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'wirklich', 'wirklich', 'ern@@', '<unk>', '@', 'st', 'dieses', 'E@@', '<unk>', '@', 'is', 'des', 'E@@', '<unk>', '@', 'is', '.', 'Es', 'ist', 'nicht', 'die', 'D@@', '<unk>', '@', 'ing', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 16:40:03,032 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 16:40:03,032 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 16:40:03,032 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das wirklich wirklich ern<unk> @ st dieses E<unk> @ is des E<unk> @ is . Es ist nicht die D<unk> @ ing des E<unk> @ is zeigt .
2025-05-27 16:40:03,033 - INFO - joeynmt.training - Example #2
2025-05-27 16:40:03,033 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 16:40:03,033 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 16:40:03,033 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 's', '.', '</s>']
2025-05-27 16:40:03,033 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 16:40:03,033 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 16:40:03,033 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise des glob<unk> @ alen Klima<unk> @ wandel<unk> @ s .
2025-05-27 16:40:03,033 - INFO - joeynmt.training - Example #3
2025-05-27 16:40:03,033 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 16:40:03,033 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 16:40:03,033 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', '.', '</s>']
2025-05-27 16:40:03,033 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 16:40:03,033 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 16:40:03,033 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt .
2025-05-27 16:40:03,033 - INFO - joeynmt.training - Example #4
2025-05-27 16:40:03,033 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 16:40:03,033 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 16:40:03,033 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'zei@@', '<unk>', '@', 'ge', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'sch@@', '<unk>', '@', 're@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'des', ',', 'was', 'es', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 16:40:03,033 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 16:40:03,033 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 16:40:03,033 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich zei<unk> @ ge zei<unk> @ ge , ist eine sch<unk> @ re<unk> @ ck<unk> @ te Ver<unk> @ sion des , was es in den letzten 25 Jahren passiert ist .
2025-05-27 16:40:36,391 - INFO - joeynmt.training - Epoch   9, Step:    27600, Batch Loss:     1.050758, Batch Acc: 0.722696, Tokens per Sec:     4564, Lr: 0.000300
2025-05-27 16:41:10,190 - INFO - joeynmt.training - Epoch   9, Step:    27700, Batch Loss:     0.955710, Batch Acc: 0.724027, Tokens per Sec:     4529, Lr: 0.000300
2025-05-27 16:41:46,912 - INFO - joeynmt.training - Epoch   9, Step:    27800, Batch Loss:     0.863790, Batch Acc: 0.726093, Tokens per Sec:     4187, Lr: 0.000300
2025-05-27 16:42:20,435 - INFO - joeynmt.training - Epoch   9, Step:    27900, Batch Loss:     0.922624, Batch Acc: 0.720975, Tokens per Sec:     4548, Lr: 0.000300
2025-05-27 16:42:54,802 - INFO - joeynmt.training - Epoch   9, Step:    28000, Batch Loss:     0.999116, Batch Acc: 0.723357, Tokens per Sec:     4517, Lr: 0.000300
2025-05-27 16:42:54,803 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 16:42:54,803 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:45:46,306 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.17, ppl:   3.24, acc:   0.68, generation: 171.4937[sec], evaluation: 0.0000[sec]
2025-05-27 16:45:46,312 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 16:45:46,404 - INFO - joeynmt.helpers - delete models/bpe_4k_model/24500.ckpt
2025-05-27 16:45:46,406 - INFO - joeynmt.training - Example #0
2025-05-27 16:45:46,406 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 16:45:46,406 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 16:45:46,406 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', '<unk>', '@', 'as', 'gesehen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'ul@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'i@@', '<unk>', '@', 'onen', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'ungefähr', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 'mor@@', '<unk>', '@', 'gens', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'äl@@', '<unk>', '@', 'le', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'Ver@@', '<unk>', '@', 'k@@', '<unk>', '@', 'ehr@@', '<unk>', '@', 'ung', '.', '</s>']
2025-05-27 16:45:46,406 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 16:45:46,406 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 16:45:46,406 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese beiden Di<unk> @ as gesehen , um zu zeigen , dass die P<unk> @ op<unk> @ ul<unk> @ ar<unk> @ i<unk> @ onen , die die drei Millionen Jahre ungefähr die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ mor<unk> @ gens der Verein<unk> @ igten Staaten , mit 40 Prozent der F<unk> @ äl<unk> @ le der USA , mit 40 Prozent der Ver<unk> @ k<unk> @ ehr<unk> @ ung .
2025-05-27 16:45:46,406 - INFO - joeynmt.training - Example #1
2025-05-27 16:45:46,406 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 16:45:46,406 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 16:45:46,406 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'wirklich', 'wirklich', 'dieses', 'spezi@@', '<unk>', '@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 16:45:46,406 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 16:45:46,406 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 16:45:46,406 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das wirklich wirklich dieses spezi<unk> @ elle Problem , weil es nicht die Di<unk> @ kte des E<unk> @ is zeigt .
2025-05-27 16:45:46,406 - INFO - joeynmt.training - Example #2
2025-05-27 16:45:46,407 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 16:45:46,407 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 16:45:46,407 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 's', '.', '</s>']
2025-05-27 16:45:46,407 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 16:45:46,407 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 16:45:46,407 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ e auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise des glob<unk> @ alen Klima<unk> @ wandel<unk> @ s .
2025-05-27 16:45:46,407 - INFO - joeynmt.training - Example #3
2025-05-27 16:45:46,407 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 16:45:46,407 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 16:45:46,407 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 16:45:46,407 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 16:45:46,407 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 16:45:46,407 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 16:45:46,407 - INFO - joeynmt.training - Example #4
2025-05-27 16:45:46,407 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 16:45:46,407 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 16:45:46,407 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'sch@@', '<unk>', '@', 're@@', '<unk>', '@', 'chte', 'Ver@@', '<unk>', '@', 'sion', 'von', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 16:45:46,407 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 16:45:46,407 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 16:45:46,407 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine sch<unk> @ re<unk> @ chte Ver<unk> @ sion von was in den letzten 25 Jahren passiert ist .
2025-05-27 16:46:19,287 - INFO - joeynmt.training - Epoch   9, Step:    28100, Batch Loss:     0.910739, Batch Acc: 0.723394, Tokens per Sec:     4648, Lr: 0.000300
2025-05-27 16:46:27,881 - INFO - joeynmt.training - Epoch   9: total training loss 3103.75
2025-05-27 16:46:27,883 - INFO - joeynmt.training - EPOCH 10
2025-05-27 16:46:52,162 - INFO - joeynmt.training - Epoch  10, Step:    28200, Batch Loss:     0.986807, Batch Acc: 0.737284, Tokens per Sec:     4672, Lr: 0.000300
2025-05-27 16:47:24,975 - INFO - joeynmt.training - Epoch  10, Step:    28300, Batch Loss:     0.934365, Batch Acc: 0.735434, Tokens per Sec:     4672, Lr: 0.000300
2025-05-27 16:47:58,651 - INFO - joeynmt.training - Epoch  10, Step:    28400, Batch Loss:     0.900105, Batch Acc: 0.734777, Tokens per Sec:     4564, Lr: 0.000300
2025-05-27 16:48:33,516 - INFO - joeynmt.training - Epoch  10, Step:    28500, Batch Loss:     0.974916, Batch Acc: 0.733311, Tokens per Sec:     4400, Lr: 0.000300
2025-05-27 16:48:33,517 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 16:48:33,517 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:51:45,394 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.18, ppl:   3.27, acc:   0.68, generation: 191.8662[sec], evaluation: 0.0000[sec]
2025-05-27 16:51:45,500 - INFO - joeynmt.helpers - delete models/bpe_4k_model/26500.ckpt
2025-05-27 16:51:45,504 - INFO - joeynmt.training - Example #0
2025-05-27 16:51:45,504 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 16:51:45,504 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 16:51:45,504 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'zwei', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'die', 'Er@@', '<unk>', '@', 'kenn@@', '<unk>', '@', 't@@', '<unk>', '@', 'nis', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', 'it@@', '<unk>', '@', 'h@@', '<unk>', '@', '-@@', '<unk>', '@', 'U@@', '<unk>', '@', '.@@', '<unk>', '@', 'B@@', '<unk>', '@', '.', ',', 'die', 'drei', 'Millionen', 'Jahre', 'war', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'est@@', '<unk>', '@', 'pl@@', '<unk>', '@', 'än@@', '<unk>', '@', 'e', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'ver@@', '<unk>', '@', 'rü@@', '<unk>', '@', 'ckt', 'war', '.', '</s>']
2025-05-27 16:51:45,504 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 16:51:45,504 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 16:51:45,504 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese beiden zwei Di<unk> @ as zeigen , um die Er<unk> @ kenn<unk> @ t<unk> @ nis zu zeigen , dass die P<unk> @ oo<unk> @ l<unk> @ it<unk> @ h<unk> @ -<unk> @ U<unk> @ .<unk> @ B<unk> @ . , die drei Millionen Jahre war , mit 40 Prozent der F<unk> @ est<unk> @ pl<unk> @ än<unk> @ e der USA , mit 40 Prozent ver<unk> @ rü<unk> @ ckt war .
2025-05-27 16:51:45,504 - INFO - joeynmt.training - Example #1
2025-05-27 16:51:45,504 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 16:51:45,504 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 16:51:45,504 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'wirklich', 'wirklich', 'dieses', 'spezi@@', '<unk>', '@', 'elle', 'Problem', 'des', 'E@@', '<unk>', '@', 'is', ',', 'weil', 'es', 'nicht', 'den', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er@@', '<unk>', '@', 'b@@', '<unk>', '@', 'aut', '.', '</s>']
2025-05-27 16:51:45,504 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 16:51:45,504 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 16:51:45,504 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das wirklich wirklich dieses spezi<unk> @ elle Problem des E<unk> @ is , weil es nicht den Di<unk> @ ck<unk> @ er<unk> @ b<unk> @ aut .
2025-05-27 16:51:45,504 - INFO - joeynmt.training - Example #2
2025-05-27 16:51:45,504 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 16:51:45,504 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 16:51:45,504 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', 'us', 'des', 'Her@@', '<unk>', '@', 'z', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'sy@@', '<unk>', '@', 'stem', '.', '</s>']
2025-05-27 16:51:45,505 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 16:51:45,505 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 16:51:45,505 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise das G<unk> @ lo<unk> @ b<unk> @ us des Her<unk> @ z des glob<unk> @ alen Klima<unk> @ sy<unk> @ stem .
2025-05-27 16:51:45,505 - INFO - joeynmt.training - Example #3
2025-05-27 16:51:45,505 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 16:51:45,505 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 16:51:45,505 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'also', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 16:51:45,505 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 16:51:45,505 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 16:51:45,505 - INFO - joeynmt.training - 	Hypothesis: Es ist also im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und K<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 16:51:45,505 - INFO - joeynmt.training - Example #4
2025-05-27 16:51:45,505 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 16:51:45,505 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 16:51:45,505 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'zei@@', '<unk>', '@', 'ge', 'zei@@', '<unk>', '@', 'ge', 'ist', 'eine', 'sch@@', '<unk>', '@', 'l@@', '<unk>', '@', 'imm@@', '<unk>', '@', 'ste', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 16:51:45,505 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 16:51:45,505 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 16:51:45,505 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich zei<unk> @ ge zei<unk> @ ge ist eine sch<unk> @ l<unk> @ imm<unk> @ ste Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert ist .
2025-05-27 16:52:20,689 - INFO - joeynmt.training - Epoch  10, Step:    28600, Batch Loss:     1.051197, Batch Acc: 0.732517, Tokens per Sec:     4487, Lr: 0.000300
2025-05-27 16:52:54,286 - INFO - joeynmt.training - Epoch  10, Step:    28700, Batch Loss:     0.925551, Batch Acc: 0.733711, Tokens per Sec:     4596, Lr: 0.000300
2025-05-27 16:53:28,218 - INFO - joeynmt.training - Epoch  10, Step:    28800, Batch Loss:     1.014928, Batch Acc: 0.731695, Tokens per Sec:     4538, Lr: 0.000300
2025-05-27 16:54:01,686 - INFO - joeynmt.training - Epoch  10, Step:    28900, Batch Loss:     0.908481, Batch Acc: 0.736798, Tokens per Sec:     4623, Lr: 0.000300
2025-05-27 16:54:36,006 - INFO - joeynmt.training - Epoch  10, Step:    29000, Batch Loss:     0.868002, Batch Acc: 0.729244, Tokens per Sec:     4459, Lr: 0.000300
2025-05-27 16:54:36,008 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 16:54:36,008 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:57:41,972 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.18, ppl:   3.27, acc:   0.68, generation: 185.9545[sec], evaluation: 0.0000[sec]
2025-05-27 16:57:41,984 - INFO - joeynmt.training - Example #0
2025-05-27 16:57:41,985 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 16:57:41,985 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 16:57:41,985 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'die', 'Er@@', '<unk>', '@', 'ze@@', '<unk>', '@', 'ug@@', '<unk>', '@', 'ung', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'ungefähr', 'ungefähr', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'syste@@', '<unk>', '@', 'ms', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'die', 'etwa', '40', 'Prozent', 'der', 'Größ@@', '<unk>', '@', 'e', 'des', 'F@@', '<unk>', '@', 'amil@@', '<unk>', '@', 'i@@', '<unk>', '@', 'en@@', '<unk>', '@', 'ga@@', '<unk>', '@', 's', 'war', '.', '</s>']
2025-05-27 16:57:41,985 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 16:57:41,985 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 16:57:41,985 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese zwei Di<unk> @ as zeigen , um die Er<unk> @ ze<unk> @ ug<unk> @ ung , die die drei Millionen Jahre ungefähr ungefähr etwa die Größ<unk> @ e des V<unk> @ ar<unk> @ syste<unk> @ ms der Verein<unk> @ igten Staaten , die etwa 40 Prozent der Größ<unk> @ e des F<unk> @ amil<unk> @ i<unk> @ en<unk> @ ga<unk> @ s war .
2025-05-27 16:57:41,985 - INFO - joeynmt.training - Example #1
2025-05-27 16:57:41,985 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 16:57:41,985 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 16:57:41,985 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'hier', 'die', 'ern@@', '<unk>', '@', 'ste', 'dieser', 'spezi@@', '<unk>', '@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'den', 'Dur@@', '<unk>', '@', 'ch@@', '<unk>', '@', 'schn@@', '<unk>', '@', 'itt', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 16:57:41,985 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 16:57:41,985 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 16:57:41,985 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das hier die ern<unk> @ ste dieser spezi<unk> @ elle Problem , weil es nicht den Dur<unk> @ ch<unk> @ schn<unk> @ itt des E<unk> @ is zeigt .
2025-05-27 16:57:41,985 - INFO - joeynmt.training - Example #2
2025-05-27 16:57:41,985 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 16:57:41,985 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 16:57:41,985 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', 'us', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 's', '.', '</s>']
2025-05-27 16:57:41,985 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 16:57:41,985 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 16:57:41,985 - INFO - joeynmt.training - 	Hypothesis: Das E<unk> @ is<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise das G<unk> @ lo<unk> @ b<unk> @ us des glob<unk> @ alen Klima<unk> @ wandel<unk> @ s .
2025-05-27 16:57:41,985 - INFO - joeynmt.training - Example #3
2025-05-27 16:57:41,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 16:57:41,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 16:57:41,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'K@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'mer', '.', '</s>']
2025-05-27 16:57:41,986 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 16:57:41,986 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 16:57:41,986 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und K<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und K<unk> @ ri<unk> @ m<unk> @ mer .
2025-05-27 16:57:41,986 - INFO - joeynmt.training - Example #4
2025-05-27 16:57:41,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 16:57:41,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 16:57:41,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Und', 'die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'das', 'ich', 'Ihnen', 'zeigen', 'möchte', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', '.', '</s>']
2025-05-27 16:57:41,986 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 16:57:41,986 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 16:57:41,986 - INFO - joeynmt.training - 	Hypothesis: Und die nächste F<unk> @ olie , das ich Ihnen zeigen möchte , ist eine Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert .
2025-05-27 16:58:16,221 - INFO - joeynmt.training - Epoch  10, Step:    29100, Batch Loss:     0.949029, Batch Acc: 0.729913, Tokens per Sec:     4444, Lr: 0.000300
2025-05-27 16:58:48,852 - INFO - joeynmt.training - Epoch  10, Step:    29200, Batch Loss:     0.943222, Batch Acc: 0.730259, Tokens per Sec:     4730, Lr: 0.000300
2025-05-27 16:59:22,700 - INFO - joeynmt.training - Epoch  10, Step:    29300, Batch Loss:     0.971562, Batch Acc: 0.729959, Tokens per Sec:     4558, Lr: 0.000300
2025-05-27 16:59:57,766 - INFO - joeynmt.training - Epoch  10, Step:    29400, Batch Loss:     0.977361, Batch Acc: 0.728827, Tokens per Sec:     4318, Lr: 0.000300
2025-05-27 17:00:31,226 - INFO - joeynmt.training - Epoch  10, Step:    29500, Batch Loss:     0.963978, Batch Acc: 0.730153, Tokens per Sec:     4528, Lr: 0.000300
2025-05-27 17:00:31,227 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 17:00:31,227 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 17:03:38,811 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.18, ppl:   3.27, acc:   0.68, generation: 187.5751[sec], evaluation: 0.0000[sec]
2025-05-27 17:03:38,818 - INFO - joeynmt.training - Example #0
2025-05-27 17:03:38,818 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 17:03:38,818 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 17:03:38,818 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'op@@', '<unk>', '@', 'o@@', '<unk>', '@', 'se', ',', 'die', 'drei', 'Millionen', 'Jahre', 'ungefähr', 'ungefähr', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'öl@@', '<unk>', '@', 'ker@@', '<unk>', '@', 'mor@@', '<unk>', '@', 'gens', 'war', '.', '</s>']
2025-05-27 17:03:38,819 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 17:03:38,819 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 17:03:38,819 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese zwei Di<unk> @ as zeigen , um zu zeigen , dass die P<unk> @ op<unk> @ o<unk> @ se , die drei Millionen Jahre ungefähr ungefähr die Größ<unk> @ e des V<unk> @ öl<unk> @ ker<unk> @ mor<unk> @ gens war .
2025-05-27 17:03:38,819 - INFO - joeynmt.training - Example #1
2025-05-27 17:03:38,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 17:03:38,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 17:03:38,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'das', 'wirklich', 'wirklich', 'dieses', 'spezi@@', '<unk>', '@', 'elle', 'Problem', 'des', 'E@@', '<unk>', '@', 'is', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'te', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 17:03:38,819 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 17:03:38,819 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 17:03:38,819 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt das wirklich wirklich dieses spezi<unk> @ elle Problem des E<unk> @ is , weil es nicht die Di<unk> @ ck<unk> @ te des E<unk> @ is zeigt .
2025-05-27 17:03:38,819 - INFO - joeynmt.training - Example #2
2025-05-27 17:03:38,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 17:03:38,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 17:03:38,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'das', 'G@@', '<unk>', '@', 'lo@@', '<unk>', '@', 'b@@', '<unk>', '@', 'us', '.', '</s>']
2025-05-27 17:03:38,819 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 17:03:38,819 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 17:03:38,819 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise das G<unk> @ lo<unk> @ b<unk> @ us .
2025-05-27 17:03:38,819 - INFO - joeynmt.training - Example #3
2025-05-27 17:03:38,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 17:03:38,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 17:03:38,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 17:03:38,820 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 17:03:38,820 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 17:03:38,820 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 17:03:38,820 - INFO - joeynmt.training - Example #4
2025-05-27 17:03:38,820 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 17:03:38,820 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 17:03:38,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', '<unk>', '@', 'a@@', '<unk>', '@', 'gram@@', '<unk>', '@', 'm', ',', 'die', 'ich', 'Ihnen', 'zeigen', 'werde', ',', 'ist', 'eine', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 17:03:38,820 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 17:03:38,820 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 17:03:38,820 - INFO - joeynmt.training - 	Hypothesis: Die nächste Di<unk> @ a<unk> @ gram<unk> @ m , die ich Ihnen zeigen werde , ist eine Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert ist .
2025-05-27 17:04:12,513 - INFO - joeynmt.training - Epoch  10, Step:    29600, Batch Loss:     1.003881, Batch Acc: 0.729841, Tokens per Sec:     4521, Lr: 0.000300
2025-05-27 17:04:47,327 - INFO - joeynmt.training - Epoch  10, Step:    29700, Batch Loss:     1.016477, Batch Acc: 0.729681, Tokens per Sec:     4393, Lr: 0.000300
2025-05-27 17:05:21,056 - INFO - joeynmt.training - Epoch  10, Step:    29800, Batch Loss:     0.976429, Batch Acc: 0.728213, Tokens per Sec:     4537, Lr: 0.000300
2025-05-27 17:05:53,675 - INFO - joeynmt.training - Epoch  10, Step:    29900, Batch Loss:     1.003212, Batch Acc: 0.728463, Tokens per Sec:     4568, Lr: 0.000300
2025-05-27 17:06:27,154 - INFO - joeynmt.training - Epoch  10, Step:    30000, Batch Loss:     1.090182, Batch Acc: 0.728859, Tokens per Sec:     4508, Lr: 0.000300
2025-05-27 17:06:27,155 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 17:06:27,155 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 17:09:13,091 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.18, ppl:   3.26, acc:   0.68, generation: 165.9267[sec], evaluation: 0.0000[sec]
2025-05-27 17:09:13,192 - INFO - joeynmt.helpers - delete models/bpe_4k_model/25000.ckpt
2025-05-27 17:09:13,201 - INFO - joeynmt.training - Example #0
2025-05-27 17:09:13,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 17:09:13,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 17:09:13,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', '<unk>', '@', 'as', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'in', 'den', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'ie', ',', 'die', 'drei', 'Millionen', 'Jahre', ',', 'die', 'die', 'Größ@@', '<unk>', '@', 'e', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'F@@', '<unk>', '@', 'äl@@', '<unk>', '@', 'len', 'war', '.', '</s>']
2025-05-27 17:09:13,201 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 17:09:13,201 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 17:09:13,201 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese zwei Di<unk> @ as zeigen , um zu zeigen , dass die in den P<unk> @ oo<unk> @ l<unk> @ lin<unk> @ ie , die drei Millionen Jahre , die die Größ<unk> @ e der Verein<unk> @ igten Staaten , mit 40 Prozent der USA , mit 40 Prozent der F<unk> @ äl<unk> @ len war .
2025-05-27 17:09:13,201 - INFO - joeynmt.training - Example #1
2025-05-27 17:09:13,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 17:09:13,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 17:09:13,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'z@@', '<unk>', '@', 'te', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ischen', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'ck@@', '<unk>', '@', 'er@@', '<unk>', '@', 'e@@', '<unk>', '@', 'i', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die sch<unk> @ ät<unk> @ z<unk> @ te dieses spezi<unk> @ f<unk> @ ischen Problem , weil es nicht die Di<unk> @ ck<unk> @ er<unk> @ e<unk> @ i des E<unk> @ is zeigt .
2025-05-27 17:09:13,202 - INFO - joeynmt.training - Example #2
2025-05-27 17:09:13,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 17:09:13,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 17:09:13,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'am', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ser', 'Weise', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 's', '.', '</s>']
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ a<unk> @ p am Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ is<unk> @ ser Weise des glob<unk> @ alen Klima<unk> @ wandel<unk> @ s .
2025-05-27 17:09:13,202 - INFO - joeynmt.training - Example #3
2025-05-27 17:09:13,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 17:09:13,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 17:09:13,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'im', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Hypothesis: Es ist im Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 17:09:13,202 - INFO - joeynmt.training - Example #4
2025-05-27 17:09:13,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 17:09:13,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 17:09:13,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 17:09:13,202 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , die ich Ihnen zei<unk> @ ge , was in den letzten 25 Jahren passiert ist .
2025-05-27 17:09:46,139 - INFO - joeynmt.training - Epoch  10, Step:    30100, Batch Loss:     0.919298, Batch Acc: 0.724917, Tokens per Sec:     4669, Lr: 0.000300
2025-05-27 17:10:21,073 - INFO - joeynmt.training - Epoch  10, Step:    30200, Batch Loss:     0.948745, Batch Acc: 0.729521, Tokens per Sec:     4444, Lr: 0.000300
2025-05-27 17:10:53,774 - INFO - joeynmt.training - Epoch  10, Step:    30300, Batch Loss:     0.948356, Batch Acc: 0.728127, Tokens per Sec:     4697, Lr: 0.000300
2025-05-27 17:11:28,709 - INFO - joeynmt.training - Epoch  10, Step:    30400, Batch Loss:     1.041420, Batch Acc: 0.726330, Tokens per Sec:     4456, Lr: 0.000300
2025-05-27 17:12:00,674 - INFO - joeynmt.training - Epoch  10, Step:    30500, Batch Loss:     0.956953, Batch Acc: 0.730661, Tokens per Sec:     4760, Lr: 0.000300
2025-05-27 17:12:00,675 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 17:12:00,675 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 17:14:58,807 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.18, ppl:   3.24, acc:   0.68, generation: 178.0587[sec], evaluation: 0.0000[sec]
2025-05-27 17:14:58,909 - INFO - joeynmt.helpers - delete models/bpe_4k_model/28500.ckpt
2025-05-27 17:14:58,914 - INFO - joeynmt.training - Example #0
2025-05-27 17:14:58,914 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 17:14:58,914 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 17:14:58,914 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Di@@', '<unk>', '@', 'as', 'gesehen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'o@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ik', ',', 'die', 'drei', 'Millionen', 'Jahre', 'etwa', 'die', 'Größ@@', '<unk>', '@', 'e', 'des', 'V@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ers', 'der', 'Verein@@', '<unk>', '@', 'igten', 'Staaten', ',', 'mit', '40', 'Prozent', 'der', 'USA', ',', 'mit', '40', 'Prozent', 'der', 'U@@', '<unk>', '@', '.@@', '<unk>', '@', 'B@@', '<unk>', '@', '.', '.', '.', '</s>']
2025-05-27 17:14:58,914 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 17:14:58,914 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 17:14:58,914 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese beiden Di<unk> @ as gesehen , um zu zeigen , dass die P<unk> @ ol<unk> @ o<unk> @ st<unk> @ ik , die drei Millionen Jahre etwa die Größ<unk> @ e des V<unk> @ at<unk> @ ers der Verein<unk> @ igten Staaten , mit 40 Prozent der USA , mit 40 Prozent der U<unk> @ .<unk> @ B<unk> @ . . .
2025-05-27 17:14:58,914 - INFO - joeynmt.training - Example #1
2025-05-27 17:14:58,914 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 17:14:58,914 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 17:14:58,915 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'ern@@', '<unk>', '@', 'ste', 'dieses', 'spezi@@', '<unk>', '@', 'elle', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'Di@@', '<unk>', '@', 'kte', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 17:14:58,915 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 17:14:58,915 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 17:14:58,915 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die ern<unk> @ ste dieses spezi<unk> @ elle Problem , weil es nicht die Di<unk> @ kte des E<unk> @ is zeigt .
2025-05-27 17:14:58,915 - INFO - joeynmt.training - Example #2
2025-05-27 17:14:58,915 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 17:14:58,915 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 17:14:58,915 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'auf', 'der', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'im', 'gew@@', '<unk>', '@', 'issen', 'Sin@@', '<unk>', '@', 'ne', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'wandel@@', '<unk>', '@', 's', '.', '</s>']
2025-05-27 17:14:58,915 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 17:14:58,915 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 17:14:58,915 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ a<unk> @ p auf der Nor<unk> @ d<unk> @ po<unk> @ l im gew<unk> @ issen Sin<unk> @ ne des glob<unk> @ alen Klima<unk> @ wandel<unk> @ s .
2025-05-27 17:14:58,915 - INFO - joeynmt.training - Example #3
2025-05-27 17:14:58,915 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 17:14:58,915 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 17:14:58,915 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Sie', 'sch@@', '<unk>', '@', 'al@@', '<unk>', '@', 'tet', 'aus', 'den', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 17:14:58,915 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 17:14:58,915 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 17:14:58,915 - INFO - joeynmt.training - 	Hypothesis: Sie sch<unk> @ al<unk> @ tet aus den Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 17:14:58,915 - INFO - joeynmt.training - Example #4
2025-05-27 17:14:58,915 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 17:14:58,915 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 17:14:58,915 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.', '</s>']
2025-05-27 17:14:58,916 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 17:14:58,916 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 17:14:58,916 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von dem , was in den letzten 25 Jahren passiert ist .
2025-05-27 17:15:34,743 - INFO - joeynmt.training - Epoch  10, Step:    30600, Batch Loss:     0.959270, Batch Acc: 0.727819, Tokens per Sec:     4260, Lr: 0.000300
2025-05-27 17:16:17,082 - INFO - joeynmt.training - Epoch  10, Step:    30700, Batch Loss:     0.922630, Batch Acc: 0.727185, Tokens per Sec:     3656, Lr: 0.000300
2025-05-27 17:16:57,135 - INFO - joeynmt.training - Epoch  10, Step:    30800, Batch Loss:     0.948793, Batch Acc: 0.727508, Tokens per Sec:     3788, Lr: 0.000300
2025-05-27 17:17:40,701 - INFO - joeynmt.training - Epoch  10, Step:    30900, Batch Loss:     1.020307, Batch Acc: 0.727123, Tokens per Sec:     3479, Lr: 0.000300
2025-05-27 17:18:22,335 - INFO - joeynmt.training - Epoch  10, Step:    31000, Batch Loss:     0.962124, Batch Acc: 0.726764, Tokens per Sec:     3711, Lr: 0.000300
2025-05-27 17:18:22,336 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 17:18:22,336 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 17:21:58,835 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.18, ppl:   3.24, acc:   0.68, generation: 216.4853[sec], evaluation: 0.0000[sec]
2025-05-27 17:21:58,958 - INFO - joeynmt.helpers - delete models/bpe_4k_model/27000.ckpt
2025-05-27 17:21:58,963 - INFO - joeynmt.training - Example #0
2025-05-27 17:21:58,964 - DEBUG - joeynmt.training - 	Tokenized source:     ['V@@', '@@@', '@', 'or@@', '@@@', '@', 'ig', 'jaar', 'li@@', '@@@', '@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;s', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', '@@@', '@', 'oo@@', '@@@', '@', 'lij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', '@@@', '@', 'te', 'had', 'van', 'het', 'va@@', '@@@', '@', 'stel@@', '@@@', '@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'gek@@', '@@@', '@', 'ro@@', '@@@', '@', 'm@@', '@@@', '@', 'pen', 'was', '.']
2025-05-27 17:21:58,964 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', ',', 'die', 'für', 'an@@', '@@@', '@', 'nä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ö@@', '@@@', '@', 'sse', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-27 17:21:58,964 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'etz@@', '<unk>', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'zwei', 'F@@', '<unk>', '@', 'olie', 'zeigen', ',', 'um', 'zu', 'zeigen', ',', 'dass', 'die', 'P@@', '<unk>', '@', 'oo@@', '<unk>', '@', 'l@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'ie', ',', 'die', 'die', 'drei', 'Millionen', 'Jahre', 'ungefähr', 'der', 'Größ@@', '<unk>', '@', 'e', 'des', 'F@@', '<unk>', '@', 'eh@@', '<unk>', '@', 'l@@', '<unk>', '@', 'ers', 'hatte', ',', 'die', 'die', 'drei', 'Millionen', 'von', 'den', 'USA', ',', 'mit', '40', '%', 'der', 'USA', ',', 'mit', '40', 'Jahren', ',', 'mit', 'der', 'F@@', '<unk>', '@', 'eh@@', '<unk>', '@', 'ler', 'war', '.', '</s>']
2025-05-27 17:21:58,964 - INFO - joeynmt.training - 	Source:     V@@ or@@ ig jaar li@@ et ik deze twee dia &apos;s zien om aan te tonen dat de p@@ oo@@ lij@@ sk@@ a@@ p , die de afgelopen drie miljoen jaar ongeveer de groot@@ te had van het va@@ stel@@ and van de VS , met 40 % gek@@ ro@@ m@@ pen was .
2025-05-27 17:21:58,964 - INFO - joeynmt.training - 	Reference:  L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe , die für an@@ nä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ö@@ sse der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ ru@@ mp@@ ft ist .
2025-05-27 17:21:58,964 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ etz<unk> @ tes Jahr habe ich diese beiden zwei F<unk> @ olie zeigen , um zu zeigen , dass die P<unk> @ oo<unk> @ l<unk> @ lin<unk> @ ie , die die drei Millionen Jahre ungefähr der Größ<unk> @ e des F<unk> @ eh<unk> @ l<unk> @ ers hatte , die die drei Millionen von den USA , mit 40 % der USA , mit 40 Jahren , mit der F<unk> @ eh<unk> @ ler war .
2025-05-27 17:21:58,964 - INFO - joeynmt.training - Example #1
2025-05-27 17:21:58,965 - DEBUG - joeynmt.training - 	Tokenized source:     ['Maar', 'dit', 'onder@@', '@@@', '@', 'scha@@', '@@@', '@', 't', 'eigenlijk', 'de', 'ern@@', '@@@', '@', 'st', 'van', 'dit', 'speci@@', '@@@', '@', 'f@@', '@@@', '@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', '@@@', '@', 'kte', 'van', 'het', 'ij@@', '@@@', '@', 's', 'laat', 'zien', '.']
2025-05-27 17:21:58,965 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drü@@', '@@@', '@', 'ckt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'ft@@', '@@@', '@', 'igkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Proble@@', '@@@', '@', 'ms', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-27 17:21:58,965 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', '<unk>', '@', 'sch@@', '<unk>', '@', 'ät@@', '<unk>', '@', 'zt', 'die', 'ern@@', '<unk>', '@', 'st', 'dieses', 'spezi@@', '<unk>', '@', 'f@@', '<unk>', '@', 'isches', 'Problem', ',', 'weil', 'es', 'nicht', 'die', 'D@@', '<unk>', '@', 'ing', 'des', 'E@@', '<unk>', '@', 'is', 'zeigt', '.', '</s>']
2025-05-27 17:21:58,965 - INFO - joeynmt.training - 	Source:     Maar dit onder@@ scha@@ t eigenlijk de ern@@ st van dit speci@@ f@@ ieke probleem omdat het niet de di@@ kte van het ij@@ s laat zien .
2025-05-27 17:21:58,965 - INFO - joeynmt.training - 	Reference:  Aber dies drü@@ ckt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ ft@@ igkeit dieses spezi@@ ellen Proble@@ ms aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-27 17:21:58,965 - INFO - joeynmt.training - 	Hypothesis: Aber das unter<unk> @ sch<unk> @ ät<unk> @ zt die ern<unk> @ st dieses spezi<unk> @ f<unk> @ isches Problem , weil es nicht die D<unk> @ ing des E<unk> @ is zeigt .
2025-05-27 17:21:58,965 - INFO - joeynmt.training - Example #2
2025-05-27 17:21:58,965 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'ij@@', '@@@', '@', 'sk@@', '@@@', '@', 'a@@', '@@@', '@', 'p', 'op', 'de', 'N@@', '@@@', '@', 'oor@@', '@@@', '@', 'd@@', '@@@', '@', 'p@@', '@@@', '@', 'ool', 'is', 'in', 'ze@@', '@@@', '@', 'kere', 'zin', 'het', 'kl@@', '@@@', '@', 'opp@@', '@@@', '@', 'end', 'hart', 'van', 'ons', 'glob@@', '@@@', '@', 'aal', 'klim@@', '@@@', '@', 'aat@@', '@@@', '@', 'systeem', '.']
2025-05-27 17:21:58,965 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gew@@', '@@@', '@', 'iss@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kti@@', '@@@', '@', 'sche', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'ap@@', '@@@', '@', 'pe', 'das', 'sch@@', '@@@', '@', 'la@@', '@@@', '@', 'gende', 'Her@@', '@@@', '@', 'z', 'unser@@', '@@@', '@', 'es', 'glob@@', '@@@', '@', 'alen', 'Klima@@', '@@@', '@', 'syste@@', '@@@', '@', 'ms', '.']
2025-05-27 17:21:58,965 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'a@@', '<unk>', '@', 'p', 'am', 'Nor@@', '<unk>', '@', 'd@@', '<unk>', '@', 'po@@', '<unk>', '@', 'l', 'ist', 'in', 'gew@@', '<unk>', '@', 'iss@@', '<unk>', '@', 'em', 'Sin@@', '<unk>', '@', 'ne', 'des', 'glob@@', '<unk>', '@', 'alen', 'Klima@@', '<unk>', '@', 'syste@@', '<unk>', '@', 'ms', '.', '</s>']
2025-05-27 17:21:58,965 - INFO - joeynmt.training - 	Source:     De ij@@ sk@@ a@@ p op de N@@ oor@@ d@@ p@@ ool is in ze@@ kere zin het kl@@ opp@@ end hart van ons glob@@ aal klim@@ aat@@ systeem .
2025-05-27 17:21:58,965 - INFO - joeynmt.training - 	Reference:  In gew@@ iss@@ em Sin@@ ne ist die ar@@ kti@@ sche E@@ is@@ k@@ ap@@ pe das sch@@ la@@ gende Her@@ z unser@@ es glob@@ alen Klima@@ syste@@ ms .
2025-05-27 17:21:58,965 - INFO - joeynmt.training - 	Hypothesis: Der E<unk> @ is<unk> @ a<unk> @ p am Nor<unk> @ d<unk> @ po<unk> @ l ist in gew<unk> @ iss<unk> @ em Sin<unk> @ ne des glob<unk> @ alen Klima<unk> @ syste<unk> @ ms .
2025-05-27 17:21:58,965 - INFO - joeynmt.training - Example #3
2025-05-27 17:21:58,965 - DEBUG - joeynmt.training - 	Tokenized source:     ['Het', 'zet', 'uit', 'in', 'de', 'win@@', '@@@', '@', 'ter', 'en', 'k@@', '@@@', '@', 'ri@@', '@@@', '@', 'm@@', '@@@', '@', 'pt', 'in', 'de', 'z@@', '@@@', '@', 'om@@', '@@@', '@', 'er', '.']
2025-05-27 17:21:58,965 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', '@@@', '@', 'äch@@', '@@@', '@', 'st', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'ru@@', '@@@', '@', 'mp@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'omm@@', '@@@', '@', 'er', '.']
2025-05-27 17:21:58,966 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'der', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'Win@@', '<unk>', '@', 'ter', 'und', 'k@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'pt', 'in', 'den', 'S@@', '<unk>', '@', 'omm@@', '<unk>', '@', 'er', '.', '</s>']
2025-05-27 17:21:58,966 - INFO - joeynmt.training - 	Source:     Het zet uit in de win@@ ter en k@@ ri@@ m@@ pt in de z@@ om@@ er .
2025-05-27 17:21:58,966 - INFO - joeynmt.training - 	Reference:  Sie w@@ äch@@ st im Win@@ ter und sch@@ ru@@ mp@@ ft im S@@ omm@@ er .
2025-05-27 17:21:58,966 - INFO - joeynmt.training - 	Hypothesis: Es ist der Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den Win<unk> @ ter und k<unk> @ ri<unk> @ m<unk> @ pt in den S<unk> @ omm<unk> @ er .
2025-05-27 17:21:58,966 - INFO - joeynmt.training - Example #4
2025-05-27 17:21:58,966 - DEBUG - joeynmt.training - 	Tokenized source:     ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', '@@@', '@', 'n@@', '@@@', '@', 'elde', 'ver@@', '@@@', '@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeur@@', '@@@', '@', 'd', '.']
2025-05-27 17:21:58,966 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'F@@', '@@@', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '@@@', '@', 'ge', ',', 'ist', 'eine', 'Z@@', '@@@', '@', 'ei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-27 17:21:58,966 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'F@@', '<unk>', '@', 'olie', ',', 'die', 'ich', 'Ihnen', 'zei@@', '<unk>', '@', 'ge', ',', 'ist', 'eine', 'besch@@', '<unk>', '@', 'le@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ig@@', '<unk>', '@', 'te', 'Ver@@', '<unk>', '@', 'sion', 'von', 'dem', ',', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', '<unk>', '@', 'ehen', 'ist', '.', '</s>']
2025-05-27 17:21:58,966 - INFO - joeynmt.training - 	Source:     De volgende dia die ik laat zien is een vers@@ n@@ elde ver@@ sie van wat er de afgelopen 25 jaar is gebeur@@ d .
2025-05-27 17:21:58,966 - INFO - joeynmt.training - 	Reference:  Die nächste F@@ olie , die ich Ihnen zei@@ ge , ist eine Z@@ ei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-27 17:21:58,966 - INFO - joeynmt.training - 	Hypothesis: Die nächste F<unk> @ olie , die ich Ihnen zei<unk> @ ge , ist eine besch<unk> @ le<unk> @ un<unk> @ ig<unk> @ te Ver<unk> @ sion von dem , was in den letzten 25 Jahren gesch<unk> @ ehen ist .
2025-05-27 17:22:44,294 - INFO - joeynmt.training - Epoch  10, Step:    31100, Batch Loss:     0.978638, Batch Acc: 0.721691, Tokens per Sec:     3397, Lr: 0.000300
2025-05-27 17:23:24,059 - INFO - joeynmt.training - Epoch  10, Step:    31200, Batch Loss:     0.972223, Batch Acc: 0.727159, Tokens per Sec:     3824, Lr: 0.000300
2025-05-27 17:23:45,823 - INFO - joeynmt.training - Epoch  10: total training loss 3047.98
2025-05-27 17:23:45,823 - INFO - joeynmt.training - Training ended after  10 epochs.
2025-05-27 17:23:45,823 - INFO - joeynmt.training - Best validation result (greedy) at step    28000:   3.24 ppl.
2025-05-27 17:23:45,836 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-27 17:23:45,895 - INFO - joeynmt.model - Enc-dec model built.
2025-05-27 17:23:45,933 - INFO - joeynmt.helpers - Load model from /Users/merilinsilva/Documents/SemestersUZH/4thSem/MT/exercises/msousa_spareek_mt_exercise_04/mt-exercise-4/models/bpe_4k_model/28000.ckpt.
2025-05-27 17:23:45,936 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=3989),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=3989),
	loss_function=None)
2025-05-27 17:23:45,937 - INFO - joeynmt.prediction - Decoding on dev set...
2025-05-27 17:23:45,938 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 17:23:45,938 - INFO - joeynmt.prediction - Predicting 1001 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 17:26:03,591 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 137.6410[sec], evaluation: 0.0000[sec]
2025-05-27 17:26:03,596 - INFO - joeynmt.prediction - Translations saved to: /Users/merilinsilva/Documents/SemestersUZH/4thSem/MT/exercises/msousa_spareek_mt_exercise_04/mt-exercise-4/models/bpe_4k_model/00028000.hyps.dev.
2025-05-27 17:26:03,596 - INFO - joeynmt.prediction - Decoding on test set...
2025-05-27 17:26:03,596 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-27 17:26:03,596 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 17:29:11,719 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 188.1047[sec], evaluation: 0.0000[sec]
2025-05-27 17:29:11,724 - INFO - joeynmt.prediction - Translations saved to: /Users/merilinsilva/Documents/SemestersUZH/4thSem/MT/exercises/msousa_spareek_mt_exercise_04/mt-exercise-4/models/bpe_4k_model/00028000.hyps.test.
